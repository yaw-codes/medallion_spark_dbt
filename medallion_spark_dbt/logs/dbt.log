[0m04:50:17.691608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff23e3c24e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff23e52c050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff23c934e30>]}


============================== 04:50:17.696635 | 692494d0-e932-4573-8f76-eb787c5223d8 ==============================
[0m04:50:17.696635 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:50:17.697105 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:50:17.716801 [info ] [MainThread]: dbt version: 1.9.2
[0m04:50:17.717168 [info ] [MainThread]: python version: 3.12.3
[0m04:50:17.717442 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m04:50:17.717701 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m04:50:18.340761 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:50:18.341222 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:50:18.341526 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:50:19.633526 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m04:50:19.634022 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m04:50:19.634589 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m04:50:19.634988 [info ] [MainThread]: adapter type: databricks
[0m04:50:19.635317 [info ] [MainThread]: adapter version: 1.9.4
[0m04:50:19.720303 [info ] [MainThread]: Configuration:
[0m04:50:19.720712 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:50:19.720981 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:50:19.721259 [info ] [MainThread]: Required dependencies:
[0m04:50:19.721543 [debug] [MainThread]: Executing "git --help"
[0m04:50:19.740091 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:50:19.740514 [debug] [MainThread]: STDERR: "b''"
[0m04:50:19.740788 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:50:19.741106 [info ] [MainThread]: Connection:
[0m04:50:19.741429 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m04:50:19.741807 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m04:50:19.742107 [info ] [MainThread]:   catalog: hive_metastore
[0m04:50:19.742398 [info ] [MainThread]:   schema: saleslt
[0m04:50:19.742845 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m04:50:19.819276 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140677788063200, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(3868, 140678433865856), compute-name=) - Creating connection
[0m04:50:19.819647 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m04:50:19.819926 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140677788063200, session-id=None, name=debug, idle-time=3.5762786865234375e-06s, acquire-count=1, language=None, thread-identifier=(3868, 140678433865856), compute-name=) - Acquired connection on thread (3868, 140678433865856), using default compute resource
[0m04:50:19.820220 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140677788063200, session-id=None, name=debug, idle-time=0.00030517578125s, acquire-count=1, language=None, thread-identifier=(3868, 140678433865856), compute-name=) - Checking idleness
[0m04:50:19.820534 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140677788063200, session-id=None, name=debug, idle-time=0.000606536865234375s, acquire-count=1, language=None, thread-identifier=(3868, 140678433865856), compute-name=) - Retrieving connection
[0m04:50:19.820776 [debug] [MainThread]: Using databricks connection "debug"
[0m04:50:19.821030 [debug] [MainThread]: On debug: select 1 as id
[0m04:50:19.821367 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:50:21.193994 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140677788063200, session-id=4bf9f4f0-73fb-4a18-87b0-d08c1fa22e68, name=debug, idle-time=1.52587890625e-05s, acquire-count=1, language=None, thread-identifier=(3868, 140678433865856), compute-name=) - Connection created
[0m04:50:21.196292 [debug] [MainThread]: Databricks adapter: Cursor(session-id=4bf9f4f0-73fb-4a18-87b0-d08c1fa22e68, command-id=Unknown) - Created cursor
[0m04:50:21.731050 [debug] [MainThread]: SQL status: OK in 1.910 seconds
[0m04:50:21.733028 [debug] [MainThread]: Databricks adapter: Cursor(session-id=4bf9f4f0-73fb-4a18-87b0-d08c1fa22e68, command-id=adce6997-3273-47d6-a7f2-3a696f82fa7b) - Closing cursor
[0m04:50:21.737758 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140677788063200, session-id=4bf9f4f0-73fb-4a18-87b0-d08c1fa22e68, name=debug, idle-time=9.775161743164062e-06s, acquire-count=0, language=None, thread-identifier=(3868, 140678433865856), compute-name=) - Released connection
[0m04:50:21.738509 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:50:21.739165 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:50:21.742362 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.10811, "process_in_blocks": "250384", "process_kernel_time": 0.929295, "process_mem_max_rss": "240696", "process_out_blocks": "24", "process_user_time": 2.987022}
[0m04:50:21.743194 [debug] [MainThread]: Command `dbt debug` succeeded at 04:50:21.743002 after 4.11 seconds
[0m04:50:21.743765 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:50:21.744353 [debug] [MainThread]: On debug: Close
[0m04:50:21.744982 [debug] [MainThread]: Databricks adapter: Connection(session-id=4bf9f4f0-73fb-4a18-87b0-d08c1fa22e68) - Closing connection
[0m04:50:22.011614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff23c7aaae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff21a7594c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff219bae7b0>]}
[0m04:50:22.012895 [debug] [MainThread]: Flushing usage events
[0m04:50:26.520440 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:04:06.990427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d7de6ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d7eb83230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d7db931a0>]}


============================== 10:04:06.999211 | 36b71fca-0ad9-4407-8292-dbe95abb5b28 ==============================
[0m10:04:06.999211 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:04:06.999947 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m10:04:07.027952 [info ] [MainThread]: dbt version: 1.9.2
[0m10:04:07.028420 [info ] [MainThread]: python version: 3.12.3
[0m10:04:07.028784 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m10:04:07.029139 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m10:04:07.791271 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:04:07.791946 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:04:07.792594 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:04:09.274980 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m10:04:09.275514 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m10:04:09.275910 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m10:04:09.276307 [info ] [MainThread]: adapter type: databricks
[0m10:04:09.276680 [info ] [MainThread]: adapter version: 1.9.4
[0m10:04:09.368410 [info ] [MainThread]: Configuration:
[0m10:04:09.368875 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:04:09.369206 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:04:09.369574 [info ] [MainThread]: Required dependencies:
[0m10:04:09.369963 [debug] [MainThread]: Executing "git --help"
[0m10:04:09.374990 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:04:09.375437 [debug] [MainThread]: STDERR: "b''"
[0m10:04:09.375747 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:04:09.376082 [info ] [MainThread]: Connection:
[0m10:04:09.376489 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m10:04:09.376993 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m10:04:09.377325 [info ] [MainThread]:   catalog: hive_metastore
[0m10:04:09.377707 [info ] [MainThread]:   schema: saleslt
[0m10:04:09.378224 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m10:04:09.489845 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139970213524016, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(11736, 139970859958400), compute-name=) - Creating connection
[0m10:04:09.490332 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m10:04:09.490723 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139970213524016, session-id=None, name=debug, idle-time=6.198883056640625e-06s, acquire-count=1, language=None, thread-identifier=(11736, 139970859958400), compute-name=) - Acquired connection on thread (11736, 139970859958400), using default compute resource
[0m10:04:09.491120 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139970213524016, session-id=None, name=debug, idle-time=0.0004150867462158203s, acquire-count=1, language=None, thread-identifier=(11736, 139970859958400), compute-name=) - Checking idleness
[0m10:04:09.491479 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139970213524016, session-id=None, name=debug, idle-time=0.0007750988006591797s, acquire-count=1, language=None, thread-identifier=(11736, 139970859958400), compute-name=) - Retrieving connection
[0m10:04:09.491895 [debug] [MainThread]: Using databricks connection "debug"
[0m10:04:09.492232 [debug] [MainThread]: On debug: select 1 as id
[0m10:04:09.492555 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:04:11.159301 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139970213524016, session-id=28576baf-4242-4aa1-85db-94c838524b0d, name=debug, idle-time=1.6689300537109375e-05s, acquire-count=1, language=None, thread-identifier=(11736, 139970859958400), compute-name=) - Connection created
[0m10:04:11.161909 [debug] [MainThread]: Databricks adapter: Cursor(session-id=28576baf-4242-4aa1-85db-94c838524b0d, command-id=Unknown) - Created cursor
[0m10:04:18.368450 [debug] [MainThread]: SQL status: OK in 8.880 seconds
[0m10:04:18.370156 [debug] [MainThread]: Databricks adapter: Cursor(session-id=28576baf-4242-4aa1-85db-94c838524b0d, command-id=157cdd7f-b141-4989-90f5-8183a3cb7427) - Closing cursor
[0m10:04:18.682676 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139970213524016, session-id=28576baf-4242-4aa1-85db-94c838524b0d, name=debug, idle-time=1.1444091796875e-05s, acquire-count=0, language=None, thread-identifier=(11736, 139970859958400), compute-name=) - Released connection
[0m10:04:18.683853 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:04:18.685036 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:04:18.690104 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 11.767004, "process_in_blocks": "248976", "process_kernel_time": 1.425558, "process_mem_max_rss": "240492", "process_out_blocks": "24", "process_user_time": 3.085315}
[0m10:04:18.691591 [debug] [MainThread]: Command `dbt debug` succeeded at 10:04:18.691269 after 11.77 seconds
[0m10:04:18.692598 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:04:18.693679 [debug] [MainThread]: On debug: Close
[0m10:04:18.694569 [debug] [MainThread]: Databricks adapter: Connection(session-id=28576baf-4242-4aa1-85db-94c838524b0d) - Closing connection
[0m10:04:19.104202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d81583320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d5b0a81d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d7bd68bc0>]}
[0m10:04:19.106633 [debug] [MainThread]: Flushing usage events
[0m10:04:20.313055 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:30:17.587150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dba2ae300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dba54e030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6db9c434a0>]}


============================== 10:30:17.596272 | 12d636d9-484f-41b5-8ed5-737196dd6ac7 ==============================
[0m10:30:17.596272 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:30:17.596996 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/stanley_one/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:30:17.627161 [info ] [MainThread]: dbt version: 1.9.2
[0m10:30:17.627714 [info ] [MainThread]: python version: 3.12.3
[0m10:30:17.628127 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m10:30:17.628524 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m10:30:18.344036 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:30:18.344676 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:30:18.345154 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:30:19.772131 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m10:30:19.772631 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m10:30:19.772975 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m10:30:19.773309 [info ] [MainThread]: adapter type: databricks
[0m10:30:19.773648 [info ] [MainThread]: adapter version: 1.9.4
[0m10:30:19.871336 [info ] [MainThread]: Configuration:
[0m10:30:19.871811 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:30:19.872152 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:30:19.872486 [info ] [MainThread]: Required dependencies:
[0m10:30:19.872832 [debug] [MainThread]: Executing "git --help"
[0m10:30:19.876954 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:30:19.877805 [debug] [MainThread]: STDERR: "b''"
[0m10:30:19.878274 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:30:19.878793 [info ] [MainThread]: Connection:
[0m10:30:19.879348 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m10:30:19.879974 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m10:30:19.880525 [info ] [MainThread]:   catalog: hive_metastore
[0m10:30:19.881088 [info ] [MainThread]:   schema: saleslt
[0m10:30:19.881802 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m10:30:19.966408 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140108660395808, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(12153, 140109305507968), compute-name=) - Creating connection
[0m10:30:19.966836 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m10:30:19.967175 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140108660395808, session-id=None, name=debug, idle-time=4.0531158447265625e-06s, acquire-count=1, language=None, thread-identifier=(12153, 140109305507968), compute-name=) - Acquired connection on thread (12153, 140109305507968), using default compute resource
[0m10:30:19.967528 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140108660395808, session-id=None, name=debug, idle-time=0.0003654956817626953s, acquire-count=1, language=None, thread-identifier=(12153, 140109305507968), compute-name=) - Checking idleness
[0m10:30:19.967845 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140108660395808, session-id=None, name=debug, idle-time=0.0006847381591796875s, acquire-count=1, language=None, thread-identifier=(12153, 140109305507968), compute-name=) - Retrieving connection
[0m10:30:19.968128 [debug] [MainThread]: Using databricks connection "debug"
[0m10:30:19.968438 [debug] [MainThread]: On debug: select 1 as id
[0m10:30:19.968738 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:30:22.322547 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140108660395808, session-id=d2b11c49-687c-470c-b3ef-519f88dc00c2, name=debug, idle-time=9.298324584960938e-06s, acquire-count=1, language=None, thread-identifier=(12153, 140109305507968), compute-name=) - Connection created
[0m10:30:22.323751 [debug] [MainThread]: Databricks adapter: Cursor(session-id=d2b11c49-687c-470c-b3ef-519f88dc00c2, command-id=Unknown) - Created cursor
[0m10:30:22.913798 [debug] [MainThread]: SQL status: OK in 2.940 seconds
[0m10:30:22.915256 [debug] [MainThread]: Databricks adapter: Cursor(session-id=d2b11c49-687c-470c-b3ef-519f88dc00c2, command-id=60645ec0-4100-4857-a64d-791946446d92) - Closing cursor
[0m10:30:22.918407 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140108660395808, session-id=d2b11c49-687c-470c-b3ef-519f88dc00c2, name=debug, idle-time=7.62939453125e-06s, acquire-count=0, language=None, thread-identifier=(12153, 140109305507968), compute-name=) - Released connection
[0m10:30:22.918987 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:30:22.919575 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:30:22.922220 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 5.394119, "process_in_blocks": "249288", "process_kernel_time": 1.081628, "process_mem_max_rss": "237996", "process_out_blocks": "32", "process_user_time": 3.002278}
[0m10:30:22.922834 [debug] [MainThread]: Command `dbt debug` succeeded at 10:30:22.922696 after 5.39 seconds
[0m10:30:22.923259 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:30:22.923674 [debug] [MainThread]: On debug: Close
[0m10:30:22.924128 [debug] [MainThread]: Databricks adapter: Connection(session-id=d2b11c49-687c-470c-b3ef-519f88dc00c2) - Closing connection
[0m10:30:23.248680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dbb47c530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d97c314f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6db9f1ffb0>]}
[0m10:30:23.249520 [debug] [MainThread]: Flushing usage events
[0m10:30:25.309106 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:31:30.583748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40c666b110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40c67ca750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40c6d73d70>]}


============================== 10:31:30.592019 | 83cf9b02-1c0d-41a6-888f-1780a0752419 ==============================
[0m10:31:30.592019 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:31:30.592586 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:31:31.270197 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:31:31.270762 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:31:31.271172 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:31:32.863827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '83cf9b02-1c0d-41a6-888f-1780a0752419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40c7eb1940>]}
[0m10:31:32.918383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '83cf9b02-1c0d-41a6-888f-1780a0752419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40a898b140>]}
[0m10:31:32.919031 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m10:31:33.028152 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:31:33.028865 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:31:33.029263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '83cf9b02-1c0d-41a6-888f-1780a0752419', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40c6dac1d0>]}
[0m10:31:34.685475 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.medallion_spark_dbt.product_snapshot' (snapshots/product.sql) depends on a source named 'saleslt.product' which was not found
[0m10:31:34.688647 [debug] [MainThread]: Resource report: {"command_name": "snapshot", "command_success": false, "command_wall_clock_time": 4.1663537, "process_in_blocks": "250336", "process_kernel_time": 1.18567, "process_mem_max_rss": "248324", "process_out_blocks": "8", "process_user_time": 4.252734}
[0m10:31:34.689132 [debug] [MainThread]: Command `dbt snapshot` failed at 10:31:34.689036 after 4.17 seconds
[0m10:31:34.689541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40c6828ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40a20a67e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40a3454f20>]}
[0m10:31:34.689896 [debug] [MainThread]: Flushing usage events
[0m10:31:36.424942 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:36:44.908689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2efb97350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2f05b6a80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee7fc200>]}


============================== 10:36:44.914831 | e7e1b2a7-e3f1-449f-ae18-7f33480b5209 ==============================
[0m10:36:44.914831 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:36:44.915469 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m10:36:44.943187 [info ] [MainThread]: dbt version: 1.9.2
[0m10:36:44.943647 [info ] [MainThread]: python version: 3.12.3
[0m10:36:44.944020 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m10:36:44.944372 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m10:36:45.698198 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:36:45.698684 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:36:45.699046 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:36:47.108879 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m10:36:47.109438 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m10:36:47.109830 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m10:36:47.110202 [info ] [MainThread]: adapter type: databricks
[0m10:36:47.110560 [info ] [MainThread]: adapter version: 1.9.4
[0m10:36:47.212989 [info ] [MainThread]: Configuration:
[0m10:36:47.213514 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:36:47.213869 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:36:47.214223 [info ] [MainThread]: Required dependencies:
[0m10:36:47.214575 [debug] [MainThread]: Executing "git --help"
[0m10:36:47.218594 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:36:47.219351 [debug] [MainThread]: STDERR: "b''"
[0m10:36:47.219978 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:36:47.220610 [info ] [MainThread]: Connection:
[0m10:36:47.221166 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m10:36:47.221616 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m10:36:47.222002 [info ] [MainThread]:   catalog: hive_metastore
[0m10:36:47.222391 [info ] [MainThread]:   schema: saleslt
[0m10:36:47.222874 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m10:36:47.321137 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140474611715296, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(12389, 140475258658944), compute-name=) - Creating connection
[0m10:36:47.321635 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m10:36:47.322110 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140474611715296, session-id=None, name=debug, idle-time=4.76837158203125e-06s, acquire-count=1, language=None, thread-identifier=(12389, 140475258658944), compute-name=) - Acquired connection on thread (12389, 140475258658944), using default compute resource
[0m10:36:47.322460 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140474611715296, session-id=None, name=debug, idle-time=0.0004382133483886719s, acquire-count=1, language=None, thread-identifier=(12389, 140475258658944), compute-name=) - Checking idleness
[0m10:36:47.322810 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140474611715296, session-id=None, name=debug, idle-time=0.0007886886596679688s, acquire-count=1, language=None, thread-identifier=(12389, 140475258658944), compute-name=) - Retrieving connection
[0m10:36:47.323120 [debug] [MainThread]: Using databricks connection "debug"
[0m10:36:47.323426 [debug] [MainThread]: On debug: select 1 as id
[0m10:36:47.323716 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:36:48.587839 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140474611715296, session-id=3374f7a9-baf0-4dda-825e-6dab01332c40, name=debug, idle-time=1.0251998901367188e-05s, acquire-count=1, language=None, thread-identifier=(12389, 140475258658944), compute-name=) - Connection created
[0m10:36:48.589326 [debug] [MainThread]: Databricks adapter: Cursor(session-id=3374f7a9-baf0-4dda-825e-6dab01332c40, command-id=Unknown) - Created cursor
[0m10:36:49.025321 [debug] [MainThread]: SQL status: OK in 1.700 seconds
[0m10:36:49.026496 [debug] [MainThread]: Databricks adapter: Cursor(session-id=3374f7a9-baf0-4dda-825e-6dab01332c40, command-id=325b060a-9c91-4f04-95f8-07594d55055c) - Closing cursor
[0m10:36:49.028864 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140474611715296, session-id=3374f7a9-baf0-4dda-825e-6dab01332c40, name=debug, idle-time=5.7220458984375e-06s, acquire-count=0, language=None, thread-identifier=(12389, 140475258658944), compute-name=) - Released connection
[0m10:36:49.029322 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:36:49.029741 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:36:49.031508 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.1914434, "process_in_blocks": "249424", "process_kernel_time": 1.138601, "process_mem_max_rss": "237900", "process_out_blocks": "32", "process_user_time": 3.217786}
[0m10:36:49.032233 [debug] [MainThread]: Command `dbt debug` succeeded at 10:36:49.032072 after 4.19 seconds
[0m10:36:49.032829 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:36:49.033300 [debug] [MainThread]: On debug: Close
[0m10:36:49.033756 [debug] [MainThread]: Databricks adapter: Connection(session-id=3374f7a9-baf0-4dda-825e-6dab01332c40) - Closing connection
[0m10:36:49.262906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee588f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2d03f0f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2d0960290>]}
[0m10:36:49.263420 [debug] [MainThread]: Flushing usage events
[0m10:36:50.258833 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:37:47.392137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3902c95e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f390267dd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3902dc1e20>]}


============================== 10:37:47.400904 | db66b815-bf8b-4947-8d9c-9bbd4b423ea0 ==============================
[0m10:37:47.400904 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:37:47.401583 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:37:48.123520 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:37:48.124047 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:37:48.124441 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:37:49.655964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38dfeef3b0>]}
[0m10:37:49.709630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38dfe65a60>]}
[0m10:37:49.710254 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m10:37:49.808845 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:37:49.809439 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:37:49.809869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38eb7b1c70>]}
[0m10:37:51.725980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38dde4cda0>]}
[0m10:37:51.819677 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m10:37:51.821521 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m10:37:51.852884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38dde23c20>]}
[0m10:37:51.853335 [info ] [MainThread]: Found 2 models, 7 snapshots, 4 data tests, 9 sources, 605 macros
[0m10:37:51.853691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddc98590>]}
[0m10:37:51.855523 [info ] [MainThread]: 
[0m10:37:51.855938 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:37:51.856263 [info ] [MainThread]: 
[0m10:37:51.856821 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Creating connection
[0m10:37:51.857128 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:37:51.857438 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=4.0531158447265625e-06s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Acquired connection on thread (12464, 139882887078016), using default compute resource
[0m10:37:51.863096 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Creating connection
[0m10:37:51.863598 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m10:37:51.863941 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=None, name=list_hive_metastore, idle-time=5.245208740234375e-06s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource
[0m10:37:51.864296 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=None, name=list_hive_metastore, idle-time=0.0003714561462402344s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:51.864623 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=None, name=list_hive_metastore, idle-time=0.0007016658782958984s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:51.864896 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m10:37:51.865188 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m10:37:51.865478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:37:52.773759 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore, idle-time=1.5497207641601562e-05s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Connection created
[0m10:37:52.776070 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:53.638362 [debug] [ThreadPool]: SQL status: OK in 1.770 seconds
[0m10:37:53.642289 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=2a81f43e-3ddb-4646-8459-af7ff6643611) - Closing cursor
[0m10:37:53.643668 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore, idle-time=9.298324584960938e-06s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:37:53.646078 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore, idle-time=0.0024039745330810547s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:53.646994 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m10:37:53.647910 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.004324913024902344s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named list_hive_metastore
[0m10:37:53.648751 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.005166292190551758s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource
[0m10:37:53.649860 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.006247997283935547s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:53.650728 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.007144451141357422s, acquire-count=2, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource
[0m10:37:53.651619 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m10:37:53.667015 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.023446321487426758s, acquire-count=2, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:53.667722 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.024201631546020508s, acquire-count=2, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:53.668289 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.024778127670288086s, acquire-count=2, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:53.668831 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.025322675704956055s, acquire-count=2, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:53.669359 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:37:53.669775 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m10:37:53.670237 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m10:37:53.670718 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:54.658631 [debug] [ThreadPool]: SQL status: OK in 0.990 seconds
[0m10:37:54.662060 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=2bae29ee-67dc-45e7-aaca-e3d09ebb6858) - Closing cursor
[0m10:37:54.667611 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m10:37:54.669046 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=1.0253899097442627s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:37:54.670745 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=5.7220458984375e-06s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:37:54.676103 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=create_hive_metastore_snapshots, idle-time=0.005290031433105469s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:54.677785 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_snapshots, now list_hive_metastore_snapshots)
[0m10:37:54.679003 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_snapshots, idle-time=0.008252859115600586s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named create_hive_metastore_snapshots
[0m10:37:54.680283 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_snapshots, idle-time=0.009535074234008789s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource
[0m10:37:54.681462 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_snapshots, idle-time=0.010733366012573242s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:54.682686 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_snapshots, idle-time=0.011983156204223633s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:54.683409 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m10:37:54.684228 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m10:37:54.685037 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:55.122266 [debug] [ThreadPool]: SQL status: OK in 0.440 seconds
[0m10:37:55.126662 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=aa644a32-3766-4762-a45d-518c02aacdfd) - Closing cursor
[0m10:37:55.128061 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_snapshots, idle-time=7.62939453125e-06s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:37:55.129613 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_snapshots, idle-time=0.0015616416931152344s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:55.130678 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m10:37:55.131714 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.0036618709564208984s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m10:37:55.132669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.004617452621459961s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource
[0m10:37:55.133725 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.0056879520416259766s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:55.134659 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.006619930267333984s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:55.135553 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m10:37:55.136412 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m10:37:55.137159 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:55.448833 [debug] [ThreadPool]: SQL status: OK in 0.310 seconds
[0m10:37:55.451444 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=2ce9774c-ffea-4d2d-bef0-8fe9a9dcec95) - Closing cursor
[0m10:37:55.458986 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.3309750556945801s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:55.459677 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.33171963691711426s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:55.460149 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m10:37:55.460627 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m10:37:55.461107 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:55.883105 [debug] [ThreadPool]: SQL status: OK in 0.420 seconds
[0m10:37:55.898246 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=728ecfda-5954-4420-a2b0-75c13a23e1bc) - Closing cursor
[0m10:37:55.904346 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.7763683795928955s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:55.904895 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.7769639492034912s, acquire-count=1, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:55.905272 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m10:37:55.905670 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m10:37:55.906091 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:56.537576 [debug] [ThreadPool]: SQL status: OK in 0.630 seconds
[0m10:37:56.546730 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=f5aca37f-cd4c-4b8d-a99c-b7eaa04d13bc) - Closing cursor
[0m10:37:56.548417 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=9.775161743164062e-06s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:37:56.553797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f390057f470>]}
[0m10:37:56.554970 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=4.69741153717041s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Checking idleness
[0m10:37:56.555937 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=4.698355436325073s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Retrieving connection
[0m10:37:56.556811 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=4.699270009994507s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Checking idleness
[0m10:37:56.557763 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=4.700224161148071s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Retrieving connection
[0m10:37:56.558563 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:37:56.559395 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:37:56.560283 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=6.4373016357421875e-06s, acquire-count=0, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Released connection
[0m10:37:56.564739 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.address_snapshot
[0m10:37:56.565959 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m10:37:56.567334 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=list_hive_metastore_saleslt, idle-time=0.01877140998840332s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:56.568235 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now snapshot.medallion_spark_dbt.address_snapshot)
[0m10:37:56.569394 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.020614147186279297s, acquire-count=0, language=None, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m10:37:56.570385 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.0219118595123291s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m10:37:56.571320 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.address_snapshot
[0m10:37:56.584311 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.address_snapshot
[0m10:37:56.697679 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.149306058883667s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:56.698259 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.1499471664428711s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:56.698647 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.address_snapshot"
[0m10:37:56.699133 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:37:56.699588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:58.178694 [debug] [Thread-1 (]: SQL status: OK in 1.480 seconds
[0m10:37:58.180358 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=39fd11ed-0bd5-4796-b1a1-d36a36121604) - Closing cursor
[0m10:37:58.188748 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=1.6402630805969238s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:58.189607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=1.6411998271942139s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:58.190278 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.address_snapshot"
[0m10:37:58.191069 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:37:58.191819 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:37:58.441917 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m10:37:58.443516 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=66511196-7333-41c2-a3a1-d5d9e06651e7) - Closing cursor
[0m10:37:58.445528 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.address_snapshot"
[0m10:37:58.454607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=1.9053475856781006s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:37:58.456845 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=1.908046007156372s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:37:58.458532 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.address_snapshot"
[0m10:37:58.460913 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



  
  
[0m10:37:58.462424 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:22.278398 [debug] [Thread-1 (]: SQL status: OK in 23.820 seconds
[0m10:38:22.281209 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=750b3d23-b662-4a00-a599-33552ff90e8f) - Closing cursor
[0m10:38:22.542454 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:22.552574 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:22.553343 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:22.557345 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3902c0c980>]}
[0m10:38:22.558445 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 25.99s]
[0m10:38:22.559195 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.address_snapshot
[0m10:38:22.559720 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customer_snapshot
[0m10:38:22.560266 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m10:38:22.560982 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.00764918327331543s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:22.561571 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.address_snapshot, now snapshot.medallion_spark_dbt.customer_snapshot)
[0m10:38:22.562059 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.008736371994018555s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.address_snapshot
[0m10:38:22.562499 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.009180307388305664s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m10:38:22.562931 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customer_snapshot
[0m10:38:22.567648 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customer_snapshot
[0m10:38:22.573362 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.019972562789916992s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:22.573852 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.020557165145874023s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:22.574152 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customer_snapshot"
[0m10:38:22.574557 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:22.574947 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:23.131773 [debug] [Thread-1 (]: SQL status: OK in 0.560 seconds
[0m10:38:23.133363 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=7dd18098-537d-4b66-b4b6-0daae6a47ed1) - Closing cursor
[0m10:38:23.135731 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.5822587013244629s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:23.136579 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.5831270217895508s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:23.137189 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customer_snapshot"
[0m10:38:23.137886 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:23.138655 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:23.390902 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m10:38:23.391646 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=f08ec043-92b7-40c4-ac14-1ace76347025) - Closing cursor
[0m10:38:23.392489 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.customer_snapshot"
[0m10:38:23.393620 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.8402328491210938s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:23.394254 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.8408722877502441s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:23.394721 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customer_snapshot"
[0m10:38:23.395376 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



  
  
[0m10:38:23.396244 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:28.241560 [debug] [Thread-1 (]: SQL status: OK in 4.850 seconds
[0m10:38:28.243984 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=71002a1d-30d0-4692-93ea-95f7ec2c5dee) - Closing cursor
[0m10:38:28.246695 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:28.249122 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=6.4373016357421875e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:28.250416 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:28.251611 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddc06ae0>]}
[0m10:38:28.253240 [info ] [Thread-1 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 5.69s]
[0m10:38:28.254513 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customer_snapshot
[0m10:38:28.255361 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m10:38:28.256586 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m10:38:28.257722 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.007360935211181641s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:28.258431 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customer_snapshot, now snapshot.medallion_spark_dbt.customeraddress_snapshot)
[0m10:38:28.259215 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.008866071701049805s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customer_snapshot
[0m10:38:28.260043 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.009642839431762695s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m10:38:28.260773 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m10:38:28.266272 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m10:38:28.275690 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.025372982025146484s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:28.276309 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.02605462074279785s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:28.276683 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customeraddress_snapshot"
[0m10:38:28.277171 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:28.277665 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:28.769215 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m10:38:28.770031 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=e1e84444-ecd2-4ffe-89e3-08c7fb2c065d) - Closing cursor
[0m10:38:28.771828 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.5214464664459229s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:28.772576 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.5222659111022949s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:28.773180 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customeraddress_snapshot"
[0m10:38:28.773954 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:28.774556 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:29.018986 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:38:29.020369 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=e4e37a52-1032-454a-b63f-5fc380ff0058) - Closing cursor
[0m10:38:29.021991 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.customeraddress_snapshot"
[0m10:38:29.023927 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.7733993530273438s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:29.024851 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.7744796276092529s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:29.025570 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customeraddress_snapshot"
[0m10:38:29.026547 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



  
  
[0m10:38:29.027486 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:33.393735 [debug] [Thread-1 (]: SQL status: OK in 4.370 seconds
[0m10:38:33.395974 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=3c04c60a-dfe8-4c15-af4f-d59c4f2231d2) - Closing cursor
[0m10:38:33.399145 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:33.401616 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=6.67572021484375e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:33.402881 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:33.403917 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddc16210>]}
[0m10:38:33.405527 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.15s]
[0m10:38:33.407072 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m10:38:33.408055 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.product_snapshot
[0m10:38:33.409480 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m10:38:33.410829 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.007903814315795898s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:33.411692 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customeraddress_snapshot, now snapshot.medallion_spark_dbt.product_snapshot)
[0m10:38:33.412609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.00975346565246582s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m10:38:33.413647 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.010761499404907227s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m10:38:33.414534 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.product_snapshot
[0m10:38:33.421519 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.product_snapshot
[0m10:38:33.428222 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.025443315505981445s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:33.428793 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.026084184646606445s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:33.429148 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.product_snapshot"
[0m10:38:33.429626 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:33.430122 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:34.092368 [debug] [Thread-1 (]: SQL status: OK in 0.660 seconds
[0m10:38:34.093640 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=44125179-d647-4a47-ba8e-002aab1e8296) - Closing cursor
[0m10:38:34.096684 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.6938867568969727s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:34.097337 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.6946008205413818s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:34.097759 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.product_snapshot"
[0m10:38:34.098290 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:34.098810 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:34.356903 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m10:38:34.358563 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=4fed6a50-541a-4886-9b47-df14e6102363) - Closing cursor
[0m10:38:34.361455 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.product_snapshot"
[0m10:38:34.365358 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.9624636173248291s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:34.366271 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.9634706974029541s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:34.366854 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.product_snapshot"
[0m10:38:34.367698 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



  
  
[0m10:38:34.368373 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:38.639620 [debug] [Thread-1 (]: SQL status: OK in 4.270 seconds
[0m10:38:38.642467 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=23e6deb6-96b6-4e9c-a410-c46bf1a1ac81) - Closing cursor
[0m10:38:38.645986 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:38.648385 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=8.344650268554688e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:38.649557 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=4.291534423828125e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:38.650457 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddbecec0>]}
[0m10:38:38.651729 [info ] [Thread-1 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 5.24s]
[0m10:38:38.652803 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.product_snapshot
[0m10:38:38.653594 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m10:38:38.654873 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m10:38:38.656303 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.0068089962005615234s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:38.657112 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.product_snapshot, now snapshot.medallion_spark_dbt.productmodel_snapshot)
[0m10:38:38.657971 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.008502483367919922s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.product_snapshot
[0m10:38:38.658709 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.009209156036376953s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m10:38:38.659430 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m10:38:38.664315 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m10:38:38.670111 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.020659446716308594s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:38.670669 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.021283388137817383s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:38.671142 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.productmodel_snapshot"
[0m10:38:38.672024 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:38.672834 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:39.165911 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m10:38:39.166631 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=6e10646b-aa42-4247-968b-781c4337167b) - Closing cursor
[0m10:38:39.167718 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.5183477401733398s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:39.168237 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.5187826156616211s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:39.168547 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.productmodel_snapshot"
[0m10:38:39.169088 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:39.169745 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:39.414450 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:38:39.416974 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=5705fd65-2d0d-4cca-9ddd-6ea7da08579e) - Closing cursor
[0m10:38:39.419221 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.productmodel_snapshot"
[0m10:38:39.424494 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.7747387886047363s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:39.425977 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.7763075828552246s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:39.426994 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.productmodel_snapshot"
[0m10:38:39.428742 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



  
  
[0m10:38:39.430536 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:43.282507 [debug] [Thread-1 (]: SQL status: OK in 3.850 seconds
[0m10:38:43.283951 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=066b60cd-f641-4607-94f9-445419eb06e9) - Closing cursor
[0m10:38:43.285402 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:43.286527 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:43.287095 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:43.287609 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddc2e1e0>]}
[0m10:38:43.288416 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 4.63s]
[0m10:38:43.289182 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m10:38:43.289830 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m10:38:43.290699 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m10:38:43.291501 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.004351377487182617s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:43.291898 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.productmodel_snapshot, now snapshot.medallion_spark_dbt.salesorderdetail_snapshot)
[0m10:38:43.292360 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.005260944366455078s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.productmodel_snapshot
[0m10:38:43.292853 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.0057525634765625s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m10:38:43.293290 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m10:38:43.296895 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m10:38:43.303248 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.016103744506835938s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:43.303739 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.016664981842041016s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:43.304030 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"
[0m10:38:43.304410 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:43.304832 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:43.739732 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m10:38:43.740351 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=432957b6-0a8c-48b1-bfd0-5f5e26382edc) - Closing cursor
[0m10:38:43.741814 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.454639196395874s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:43.742312 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.45522475242614746s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:43.742605 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"
[0m10:38:43.743014 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:43.743428 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:44.040054 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m10:38:44.040611 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=0ff6ab97-9962-4f48-b90d-8f38659d4a86) - Closing cursor
[0m10:38:44.041351 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"
[0m10:38:44.042655 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.7554056644439697s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:44.043290 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.7561404705047607s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:44.043636 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"
[0m10:38:44.044095 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



  
  
[0m10:38:44.044552 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:48.164042 [debug] [Thread-1 (]: SQL status: OK in 4.120 seconds
[0m10:38:48.166330 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=7493307d-b05c-4302-abd3-74406b106caa) - Closing cursor
[0m10:38:48.168817 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:48.171064 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=5.245208740234375e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:48.171953 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:48.172708 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddc50380>]}
[0m10:38:48.174048 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 4.88s]
[0m10:38:48.175296 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m10:38:48.175972 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m10:38:48.176719 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m10:38:48.177598 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.005602359771728516s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:48.178152 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderdetail_snapshot, now snapshot.medallion_spark_dbt.salesorderheader_snapshot)
[0m10:38:48.178585 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.0067179203033447266s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m10:38:48.179014 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.00711822509765625s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Acquired connection on thread (12464, 139882214540992), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m10:38:48.179473 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m10:38:48.183411 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m10:38:48.189627 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.017618417739868164s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:48.190432 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.01849365234375s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:48.190840 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderheader_snapshot"
[0m10:38:48.191965 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:48.194739 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:48.658314 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m10:38:48.659169 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=d2472a31-f6a2-4b2d-8e9e-d6cbdb94b9eb) - Closing cursor
[0m10:38:48.661364 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.48925065994262695s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:48.662621 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.4905393123626709s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:48.663440 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderheader_snapshot"
[0m10:38:48.664153 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m10:38:48.664705 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:48.927489 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m10:38:48.928365 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=4d374184-057e-4cd1-a77b-a5656f6f968e) - Closing cursor
[0m10:38:48.929224 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_spark_dbt.salesorderheader_snapshot"
[0m10:38:48.930275 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.758312463760376s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Checking idleness
[0m10:38:48.930886 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.7589399814605713s, acquire-count=1, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Retrieving connection
[0m10:38:48.931363 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderheader_snapshot"
[0m10:38:48.931949 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



  
  
[0m10:38:48.932571 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=Unknown) - Created cursor
[0m10:38:53.008513 [debug] [Thread-1 (]: SQL status: OK in 4.080 seconds
[0m10:38:53.011600 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, command-id=c4d42ce4-5df5-4f27-8ebc-9e6f069e4f2c) - Closing cursor
[0m10:38:53.014545 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m10:38:53.016548 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=5.4836273193359375e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:53.017518 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139882215630432, session-id=e69ba714-ba78-40fe-923b-14879c5d6c94, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=3.5762786865234375e-06s, acquire-count=0, language=sql, thread-identifier=(12464, 139882214540992), compute-name=) - Released connection
[0m10:38:53.018408 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db66b815-bf8b-4947-8d9c-9bbd4b423ea0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f38ddc50e90>]}
[0m10:38:53.019945 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 4.84s]
[0m10:38:53.021251 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m10:38:53.024214 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=56.46388864517212s, acquire-count=0, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Checking idleness
[0m10:38:53.025005 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=56.464754819869995s, acquire-count=0, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Reusing connection previously named master
[0m10:38:53.025769 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=56.465516328811646s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Acquired connection on thread (12464, 139882887078016), using default compute resource
[0m10:38:53.026632 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=56.46638464927673s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Checking idleness
[0m10:38:53.027419 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=None, name=master, idle-time=56.46717715263367s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Retrieving connection
[0m10:38:53.028048 [debug] [MainThread]: On master: ROLLBACK
[0m10:38:53.028735 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:38:54.239150 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=2d1cc969-3ca3-408e-b6d8-ff03d8f6a3c3, name=master, idle-time=5.245208740234375e-06s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Connection created
[0m10:38:54.239671 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:38:54.240106 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=2d1cc969-3ca3-408e-b6d8-ff03d8f6a3c3, name=master, idle-time=0.001018524169921875s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Checking idleness
[0m10:38:54.240524 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=2d1cc969-3ca3-408e-b6d8-ff03d8f6a3c3, name=master, idle-time=0.0014243125915527344s, acquire-count=1, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Retrieving connection
[0m10:38:54.240997 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:38:54.241375 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:38:54.241800 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139882217484128, session-id=2d1cc969-3ca3-408e-b6d8-ff03d8f6a3c3, name=master, idle-time=3.0994415283203125e-06s, acquire-count=0, language=None, thread-identifier=(12464, 139882887078016), compute-name=) - Released connection
[0m10:38:54.242299 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:38:54.242678 [debug] [MainThread]: On master: ROLLBACK
[0m10:38:54.243038 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:38:54.243360 [debug] [MainThread]: On master: Close
[0m10:38:54.243742 [debug] [MainThread]: Databricks adapter: Connection(session-id=2d1cc969-3ca3-408e-b6d8-ff03d8f6a3c3) - Closing connection
[0m10:38:54.547932 [debug] [MainThread]: Connection 'snapshot.medallion_spark_dbt.salesorderheader_snapshot' was properly closed.
[0m10:38:54.548627 [debug] [MainThread]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: ROLLBACK
[0m10:38:54.549209 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:38:54.549735 [debug] [MainThread]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: Close
[0m10:38:54.550425 [debug] [MainThread]: Databricks adapter: Connection(session-id=e69ba714-ba78-40fe-923b-14879c5d6c94) - Closing connection
[0m10:38:54.797792 [info ] [MainThread]: 
[0m10:38:54.798769 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 1 minutes and 2.94 seconds (62.94s).
[0m10:38:54.801945 [debug] [MainThread]: Command end result
[0m10:38:54.952862 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m10:38:54.958133 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m10:38:54.963946 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m10:38:54.964338 [info ] [MainThread]: 
[0m10:38:54.964876 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:38:54.965448 [info ] [MainThread]: 
[0m10:38:54.966022 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m10:38:54.968520 [debug] [MainThread]: Resource report: {"command_name": "snapshot", "command_success": true, "command_wall_clock_time": 67.515785, "process_in_blocks": "259616", "process_kernel_time": 1.311879, "process_mem_max_rss": "257632", "process_out_blocks": "4448", "process_user_time": 5.75779}
[0m10:38:54.968988 [debug] [MainThread]: Command `dbt snapshot` succeeded at 10:38:54.968882 after 67.52 seconds
[0m10:38:54.969369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3902932480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3902c0e030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3902c0d0d0>]}
[0m10:38:54.969762 [debug] [MainThread]: Flushing usage events
[0m10:38:57.186474 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:25:23.444290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f571a851280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5717af8ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5717af9ca0>]}


============================== 11:25:23.452794 | f5768c2d-088d-43ea-b72c-f681753fa642 ==============================
[0m11:25:23.452794 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:25:23.453487 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/stanley_one/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:25:24.271785 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:25:24.272403 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:25:24.272828 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:25:25.865735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5768c2d-088d-43ea-b72c-f681753fa642', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57194c5940>]}
[0m11:25:25.928896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5768c2d-088d-43ea-b72c-f681753fa642', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f9b53fe0>]}
[0m11:25:25.929505 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:25:26.036546 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:25:26.325720 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 7 files added, 0 files changed.
[0m11:25:26.326374 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/marts/customer/dim_customer.yml
[0m11:25:26.326720 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/marts/sales/sales.sql
[0m11:25:26.327143 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/marts/sales/sales.yml
[0m11:25:26.327482 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/marts/customer/dim_customer.sql
[0m11:25:26.327843 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/marts/product/dim_product.yml
[0m11:25:26.328150 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/marts/product/dim_product.sql
[0m11:25:26.328488 [debug] [MainThread]: Partial parsing: added file: medallion_spark_dbt://models/staging/bronze.yml
[0m11:25:26.329206 [debug] [MainThread]: Partial parsing: deleted file: medallion_spark_dbt://models/example/my_second_dbt_model.sql
[0m11:25:26.329572 [debug] [MainThread]: Partial parsing: deleted file: medallion_spark_dbt://models/example/my_first_dbt_model.sql
[0m11:25:26.809597 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models/marts/customer/dim_customer.yml'
[0m11:25:26.923005 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models/marts/sales/sales.yml'
[0m11:25:26.965710 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models/marts/product/dim_product.yml'
[0m11:25:27.068010 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_customers_customer_sk.22a014df62' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:25:27.068607 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customer_sk.8ae5836863' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:25:27.069090 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customerid.209fbdda85' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:25:27.069528 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_AddressId.86b771f63e' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:25:27.069960 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderID.810c5f247c' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.070387 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderID.48ce11e7f3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.070805 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderDetailID.343b942405' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.071221 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.071613 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderQty.66af966596' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.072006 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productID.cbf6d34890' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.072441 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_unitPrice.3545b5473a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.072927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_lineTotal.d55bca27f8' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.073386 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_name.4c7b961f77' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.073779 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productNumber.3a23a94ddd' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.074286 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_standardCost.d3f58be9a3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.074695 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_listPrice.4ee58b9e3f' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.075071 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_sellStartDate.b44c8ea118' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.075465 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderDate.6f6f720ec3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.075892 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_customerID.60b0993af5' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.076432 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_subTotal.bfeb62a487' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.077104 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_taxAmt.94cff67d6a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.077884 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_freight.ca13e04131' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.078570 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_totalDue.920571e023' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:25:27.079305 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_products_product_sk.8f20ac7c5b' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:25:27.079980 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_sk.2a2df3e1b9' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:25:27.080709 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_name.991aec73f3' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:25:27.081401 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_sellstartdate.f97a265a0f' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:25:27.176500 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.medallion_spark_dbt.example
[0m11:25:27.189581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5768c2d-088d-43ea-b72c-f681753fa642', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eb6d6e40>]}
[0m11:25:27.298741 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:25:27.311271 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:25:27.361946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5768c2d-088d-43ea-b72c-f681753fa642', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eb3a1ee0>]}
[0m11:25:27.362453 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:25:27.362813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5768c2d-088d-43ea-b72c-f681753fa642', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f89f1b20>]}
[0m11:25:27.364409 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:25:27.364919 [debug] [MainThread]: Command end result
[0m11:25:27.394674 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:25:27.396054 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:25:27.398445 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:25:27.406665 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 4.0257406, "process_in_blocks": "252304", "process_kernel_time": 1.387838, "process_mem_max_rss": "256132", "process_out_blocks": "4520", "process_user_time": 4.214167}
[0m11:25:27.407235 [debug] [MainThread]: Command `dbt test` succeeded at 11:25:27.407131 after 4.03 seconds
[0m11:25:27.407658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5717daa960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eb7b86e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eb798f20>]}
[0m11:25:27.408038 [debug] [MainThread]: Flushing usage events
[0m11:25:29.427803 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:26:42.871708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3ae372540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3acc38380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3acbb3830>]}


============================== 11:26:42.877966 | 80540882-066c-40b8-9e7e-845d6aff4469 ==============================
[0m11:26:42.877966 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:26:42.878602 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:26:43.537551 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:26:43.538046 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:26:43.538420 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:26:45.068136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '80540882-066c-40b8-9e7e-845d6aff4469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb38a6543e0>]}
[0m11:26:45.130383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '80540882-066c-40b8-9e7e-845d6aff4469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb389d2a180>]}
[0m11:26:45.131040 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:26:45.242464 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:26:45.446044 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m11:26:45.446654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '80540882-066c-40b8-9e7e-845d6aff4469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3897d4080>]}
[0m11:26:46.923844 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models/marts/sales/sales.yml'
[0m11:26:47.044805 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models/marts/customer/dim_customer.yml'
[0m11:26:47.056072 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models/marts/product/dim_product.yml'
[0m11:26:47.148816 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderID.810c5f247c' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.149446 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderID.48ce11e7f3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.149924 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderDetailID.343b942405' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.150386 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.150830 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderQty.66af966596' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.151293 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productID.cbf6d34890' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.151739 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_unitPrice.3545b5473a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.152177 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_lineTotal.d55bca27f8' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.152609 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_name.4c7b961f77' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.153021 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productNumber.3a23a94ddd' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.153458 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_standardCost.d3f58be9a3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.153882 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_listPrice.4ee58b9e3f' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.154315 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_sellStartDate.b44c8ea118' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.154722 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderDate.6f6f720ec3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.155133 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_customerID.60b0993af5' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.155523 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_subTotal.bfeb62a487' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.155926 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_taxAmt.94cff67d6a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.156357 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_freight.ca13e04131' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.156757 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_totalDue.920571e023' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:26:47.157173 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_customers_customer_sk.22a014df62' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:26:47.157764 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customer_sk.8ae5836863' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:26:47.158182 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customerid.209fbdda85' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:26:47.158567 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_AddressId.86b771f63e' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:26:47.158995 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_products_product_sk.8f20ac7c5b' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:26:47.159432 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_sk.2a2df3e1b9' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:26:47.159954 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_name.991aec73f3' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:26:47.160694 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_sellstartdate.f97a265a0f' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:26:47.259491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80540882-066c-40b8-9e7e-845d6aff4469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3886a8050>]}
[0m11:26:47.349779 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:26:47.362913 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:26:47.411382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80540882-066c-40b8-9e7e-845d6aff4469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb388327a10>]}
[0m11:26:47.411911 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:26:47.412279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80540882-066c-40b8-9e7e-845d6aff4469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb38984cc20>]}
[0m11:26:47.413856 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:26:47.414366 [debug] [MainThread]: Command end result
[0m11:26:47.550114 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:26:47.551896 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:26:47.554782 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:26:47.557133 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 4.6706977, "process_in_blocks": "252200", "process_kernel_time": 1.200808, "process_mem_max_rss": "253676", "process_out_blocks": "4512", "process_user_time": 5.035322}
[0m11:26:47.557703 [debug] [MainThread]: Command `dbt test` succeeded at 11:26:47.557575 after 4.67 seconds
[0m11:26:47.558172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3ac917aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3ac99fbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3886d9fa0>]}
[0m11:26:47.558697 [debug] [MainThread]: Flushing usage events
[0m11:26:48.571518 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:28:06.960452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cb716300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cc2baf00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cc2b9ac0>]}


============================== 11:28:06.968780 | 7de91483-7ee5-4fff-9e1f-f768a3922f2e ==============================
[0m11:28:06.968780 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:28:06.969362 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:28:07.700923 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:28:07.701492 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:28:07.701896 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:28:09.283389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32acc4e5a0>]}
[0m11:28:09.337518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32ad56eb40>]}
[0m11:28:09.338122 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:28:09.440503 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:28:09.721097 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:28:09.721488 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:28:09.774678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32aca36db0>]}
[0m11:28:09.880269 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:28:09.883612 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:28:09.911229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32acbd2930>]}
[0m11:28:09.911692 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:28:09.912072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32acd1f5f0>]}
[0m11:28:09.913806 [info ] [MainThread]: 
[0m11:28:09.914196 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:28:09.914523 [info ] [MainThread]: 
[0m11:28:09.915076 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Creating connection
[0m11:28:09.915381 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:28:09.915706 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=4.5299530029296875e-06s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Acquired connection on thread (13314, 139856200298624), using default compute resource
[0m11:28:09.923035 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Creating connection
[0m11:28:09.923560 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:28:09.923904 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=None, name=list_hive_metastore, idle-time=3.5762786865234375e-06s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Acquired connection on thread (13314, 139855543662272), using default compute resource
[0m11:28:09.924266 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=None, name=list_hive_metastore, idle-time=0.000370025634765625s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:09.924577 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=None, name=list_hive_metastore, idle-time=0.0006864070892333984s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:09.924849 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:28:09.925173 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:28:09.925500 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:28:11.658351 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore, idle-time=8.344650268554688e-06s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Connection created
[0m11:28:11.659045 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:12.564236 [debug] [ThreadPool]: SQL status: OK in 2.640 seconds
[0m11:28:12.567198 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=0ef945bd-22c3-48cd-8c7b-524610311f6e) - Closing cursor
[0m11:28:12.568171 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore, idle-time=6.4373016357421875e-06s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:12.571796 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore, idle-time=0.003618478775024414s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:12.572879 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_snapshots)
[0m11:28:12.573652 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.005501985549926758s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Reusing connection previously named list_hive_metastore
[0m11:28:12.574360 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.006214618682861328s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Acquired connection on thread (13314, 139855543662272), using default compute resource
[0m11:28:12.575150 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.0070116519927978516s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:12.575920 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.007776737213134766s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:12.576577 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:28:12.577288 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:28:12.578005 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:12.959506 [debug] [ThreadPool]: SQL status: OK in 0.380 seconds
[0m11:28:12.963012 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=821bd552-0ad4-452e-bbb2-9c02b822ba96) - Closing cursor
[0m11:28:12.981137 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.41297340393066406s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:12.981827 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.4137299060821533s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:12.982335 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.4142446517944336s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:12.982811 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.41472339630126953s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:12.983264 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:28:12.983704 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:28:12.984169 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:28:12.984684 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:13.496597 [debug] [ThreadPool]: SQL status: OK in 0.510 seconds
[0m11:28:13.516707 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=ac3c8345-b818-43e0-8916-1ca285b90393) - Closing cursor
[0m11:28:13.527142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.9589786529541016s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:13.527813 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.9597082138061523s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:13.528251 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:28:13.528710 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:28:13.529167 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:14.308380 [debug] [ThreadPool]: SQL status: OK in 0.780 seconds
[0m11:28:14.310907 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=a271074f-157f-440b-a9d1-78c34aaaa4e7) - Closing cursor
[0m11:28:14.311670 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=3.814697265625e-06s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:14.312313 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_snapshots, idle-time=0.0006568431854248047s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:14.313952 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m11:28:14.314396 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.0027217864990234375s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m11:28:14.314806 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.0031664371490478516s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Acquired connection on thread (13314, 139855543662272), using default compute resource
[0m11:28:14.315314 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.0036780834197998047s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:14.315773 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.004135847091674805s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:14.316165 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:28:14.316837 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:28:14.317565 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:14.744782 [debug] [ThreadPool]: SQL status: OK in 0.430 seconds
[0m11:28:14.746194 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=7a3610eb-0a4b-44dd-84de-b3fc5d904812) - Closing cursor
[0m11:28:14.749148 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.43746209144592285s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:14.749628 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.437985897064209s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:14.749957 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:28:14.750248 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:28:14.750552 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:15.287920 [debug] [ThreadPool]: SQL status: OK in 0.540 seconds
[0m11:28:15.294384 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=7b83598f-7dfc-4d7c-bf4f-b1fb692b7686) - Closing cursor
[0m11:28:15.304110 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.9923095703125s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:15.305457 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.9936294555664062s, acquire-count=1, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:15.306491 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:28:15.307465 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:28:15.308447 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:15.749187 [debug] [ThreadPool]: SQL status: OK in 0.440 seconds
[0m11:28:15.751453 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=c90dd230-2896-4c28-9413-b02b1ae7a43c) - Closing cursor
[0m11:28:15.752273 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=4.0531158447265625e-06s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:15.754697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cd132690>]}
[0m11:28:15.755292 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=5.8395607471466064s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Checking idleness
[0m11:28:15.755669 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=5.839964866638184s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Retrieving connection
[0m11:28:15.756030 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=5.840330600738525s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Checking idleness
[0m11:28:15.756405 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=5.84070611000061s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Retrieving connection
[0m11:28:15.756775 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:28:15.757125 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:28:15.757565 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=4.76837158203125e-06s, acquire-count=0, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Released connection
[0m11:28:15.760699 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m11:28:15.761428 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m11:28:15.762275 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=list_hive_metastore_saleslt, idle-time=0.009974956512451172s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:15.762676 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now model.medallion_spark_dbt.dim_customer)
[0m11:28:15.763098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=0.01083993911743164s, acquire-count=0, language=None, thread-identifier=(13314, 139855543662272), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m11:28:15.763505 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=0.011257410049438477s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Acquired connection on thread (13314, 139855543662272), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m11:28:15.763872 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m11:28:15.772464 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m11:28:15.780872 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m11:28:15.791940 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:28:15.842509 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.dim_customer"
[0m11:28:15.846171 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=0.09376287460327148s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:15.846830 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=0.09451889991760254s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:15.847320 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m11:28:15.847968 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m11:28:15.848703 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:21.109314 [debug] [Thread-1 (]: SQL status: OK in 5.260 seconds
[0m11:28:21.111415 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=486e8dbd-938d-45d6-8975-d6baf6d7d44e) - Closing cursor
[0m11:28:21.152955 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=4.5299530029296875e-06s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:21.153575 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:21.156094 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32ac67ff20>]}
[0m11:28:21.156847 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 5.39s]
[0m11:28:21.157545 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m11:28:21.158062 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m11:28:21.158572 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m11:28:21.159429 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_customer, idle-time=0.005842924118041992s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:21.159832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m11:28:21.160234 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=0.006675243377685547s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m11:28:21.160650 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=0.007089376449584961s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Acquired connection on thread (13314, 139855543662272), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m11:28:21.161065 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m11:28:21.165417 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m11:28:21.166529 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m11:28:21.168684 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:28:21.170935 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.dim_product"
[0m11:28:21.172634 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=0.01905655860900879s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:21.173034 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=0.019487857818603516s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:21.173323 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m11:28:21.173732 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m11:28:21.174142 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:23.841128 [debug] [Thread-1 (]: SQL status: OK in 2.670 seconds
[0m11:28:23.843935 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=762cf0b3-57f8-4f8f-801e-d327e529f130) - Closing cursor
[0m11:28:23.847879 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=8.821487426757812e-06s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:23.848912 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=4.291534423828125e-06s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:23.849810 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32ac6df620>]}
[0m11:28:23.851232 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 2.69s]
[0m11:28:23.852352 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m11:28:23.853029 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m11:28:23.853966 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m11:28:23.854873 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.dim_product, idle-time=0.00597691535949707s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:23.855416 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m11:28:23.856026 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.sales, idle-time=0.00716710090637207s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m11:28:23.856634 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.sales, idle-time=0.007773399353027344s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Acquired connection on thread (13314, 139855543662272), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m11:28:23.857204 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m11:28:23.862390 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m11:28:23.864540 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m11:28:23.868208 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:28:23.870516 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.sales"
[0m11:28:23.872163 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.sales, idle-time=0.023285627365112305s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Checking idleness
[0m11:28:23.872668 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.sales, idle-time=0.023855209350585938s, acquire-count=1, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Retrieving connection
[0m11:28:23.873025 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m11:28:23.873598 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m11:28:23.874158 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=Unknown) - Created cursor
[0m11:28:27.417049 [debug] [Thread-1 (]: SQL status: OK in 3.540 seconds
[0m11:28:27.418873 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, command-id=0d705c03-32db-4efa-8bc1-2435482429d0) - Closing cursor
[0m11:28:27.422472 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.sales, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:27.423077 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139855619070848, session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d, name=model.medallion_spark_dbt.sales, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(13314, 139855543662272), compute-name=) - Released connection
[0m11:28:27.423581 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7de91483-7ee5-4fff-9e1f-f768a3922f2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32ac7230b0>]}
[0m11:28:27.424212 [info ] [Thread-1 (]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 3.57s]
[0m11:28:27.424910 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m11:28:27.426417 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=11.668838739395142s, acquire-count=0, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Checking idleness
[0m11:28:27.426858 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=11.669318914413452s, acquire-count=0, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Reusing connection previously named master
[0m11:28:27.427258 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=11.669694423675537s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Acquired connection on thread (13314, 139856200298624), using default compute resource
[0m11:28:27.427662 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=11.670128345489502s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Checking idleness
[0m11:28:27.428024 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=None, name=master, idle-time=11.67048954963684s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Retrieving connection
[0m11:28:27.428360 [debug] [MainThread]: On master: ROLLBACK
[0m11:28:27.428723 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:28:29.350193 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=435e25ea-53b8-4512-81e9-6bfef7cbe868, name=master, idle-time=4.76837158203125e-06s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Connection created
[0m11:28:29.350761 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:28:29.351228 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=435e25ea-53b8-4512-81e9-6bfef7cbe868, name=master, idle-time=0.0010998249053955078s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Checking idleness
[0m11:28:29.351675 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=435e25ea-53b8-4512-81e9-6bfef7cbe868, name=master, idle-time=0.0015499591827392578s, acquire-count=1, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Retrieving connection
[0m11:28:29.352150 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:28:29.352588 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:28:29.353052 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139855623167856, session-id=435e25ea-53b8-4512-81e9-6bfef7cbe868, name=master, idle-time=2.6226043701171875e-06s, acquire-count=0, language=None, thread-identifier=(13314, 139856200298624), compute-name=) - Released connection
[0m11:28:29.353577 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:28:29.353987 [debug] [MainThread]: On master: ROLLBACK
[0m11:28:29.354353 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:28:29.354726 [debug] [MainThread]: On master: Close
[0m11:28:29.355108 [debug] [MainThread]: Databricks adapter: Connection(session-id=435e25ea-53b8-4512-81e9-6bfef7cbe868) - Closing connection
[0m11:28:29.665123 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m11:28:29.665907 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m11:28:29.666539 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:28:29.667126 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m11:28:29.667795 [debug] [MainThread]: Databricks adapter: Connection(session-id=e85c5c25-de8f-4f61-a23c-6c23adc9a34d) - Closing connection
[0m11:28:30.076420 [info ] [MainThread]: 
[0m11:28:30.078376 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 20.16 seconds (20.16s).
[0m11:28:30.081346 [debug] [MainThread]: Command end result
[0m11:28:30.125001 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:28:30.129976 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:28:30.137494 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:28:30.138161 [info ] [MainThread]: 
[0m11:28:30.138678 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:28:30.139044 [info ] [MainThread]: 
[0m11:28:30.139466 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m11:28:30.141461 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 23.169851, "process_in_blocks": "257288", "process_kernel_time": 1.199355, "process_mem_max_rss": "248356", "process_out_blocks": "3232", "process_user_time": 3.845868}
[0m11:28:30.141944 [debug] [MainThread]: Command `dbt run` succeeded at 11:28:30.141838 after 23.17 seconds
[0m11:28:30.142337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cb716300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cb919df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32cf18b320>]}
[0m11:28:30.142705 [debug] [MainThread]: Flushing usage events
[0m11:28:31.346448 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:28:37.441582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa87aacdf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa87abf6e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa87abf4a70>]}


============================== 11:28:37.444566 | d06f972e-262f-4d50-9c15-e80edfb687e3 ==============================
[0m11:28:37.444566 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:28:37.445196 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/stanley_one/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt test', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:28:37.960955 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:28:37.961426 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:28:37.961755 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:28:38.646256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd06f972e-262f-4d50-9c15-e80edfb687e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa87aa81370>]}
[0m11:28:38.703032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd06f972e-262f-4d50-9c15-e80edfb687e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8635a19d0>]}
[0m11:28:38.703687 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:28:38.803542 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:28:39.010723 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:28:39.011233 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:28:39.069992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd06f972e-262f-4d50-9c15-e80edfb687e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8573d60c0>]}
[0m11:28:39.168257 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:28:39.169806 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:28:39.208485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd06f972e-262f-4d50-9c15-e80edfb687e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa856e6ca40>]}
[0m11:28:39.209025 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:28:39.209526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd06f972e-262f-4d50-9c15-e80edfb687e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa857047b60>]}
[0m11:28:39.211498 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:28:39.213223 [debug] [MainThread]: Command end result
[0m11:28:39.244295 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:28:39.245846 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:28:39.248693 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:28:39.249639 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.8615447, "process_in_blocks": "1968", "process_kernel_time": 0.850595, "process_mem_max_rss": "239460", "process_out_blocks": "3064", "process_user_time": 3.098598}
[0m11:28:39.250164 [debug] [MainThread]: Command `dbt test` succeeded at 11:28:39.250062 after 1.86 seconds
[0m11:28:39.250536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa879feb830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa87da73350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8573d6060>]}
[0m11:28:39.250902 [debug] [MainThread]: Flushing usage events
[0m11:28:40.223220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:31:26.771232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d39accd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d3983b8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d39afcbf0>]}


============================== 11:31:26.779901 | fc35bf7e-9c09-4336-8f2d-e579282bfb18 ==============================
[0m11:31:26.779901 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:31:26.780805 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:31:26.808391 [info ] [MainThread]: dbt version: 1.9.2
[0m11:31:26.808808 [info ] [MainThread]: python version: 3.12.3
[0m11:31:26.809125 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m11:31:26.809428 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m11:31:27.531357 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:31:27.531850 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:31:27.532232 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:31:28.967447 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m11:31:28.968002 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m11:31:28.968392 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m11:31:28.968773 [info ] [MainThread]: adapter type: databricks
[0m11:31:28.969148 [info ] [MainThread]: adapter version: 1.9.4
[0m11:31:29.056075 [info ] [MainThread]: Configuration:
[0m11:31:29.056550 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:31:29.056872 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:31:29.057181 [info ] [MainThread]: Required dependencies:
[0m11:31:29.057530 [debug] [MainThread]: Executing "git --help"
[0m11:31:29.077554 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:31:29.078031 [debug] [MainThread]: STDERR: "b''"
[0m11:31:29.078330 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:31:29.078675 [info ] [MainThread]: Connection:
[0m11:31:29.079003 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m11:31:29.079314 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m11:31:29.079619 [info ] [MainThread]:   catalog: hive_metastore
[0m11:31:29.079922 [info ] [MainThread]:   schema: saleslt
[0m11:31:29.080368 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:31:29.175655 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139694190147152, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13527, 139694835589248), compute-name=) - Creating connection
[0m11:31:29.176141 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m11:31:29.176481 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139694190147152, session-id=None, name=debug, idle-time=5.9604644775390625e-06s, acquire-count=1, language=None, thread-identifier=(13527, 139694835589248), compute-name=) - Acquired connection on thread (13527, 139694835589248), using default compute resource
[0m11:31:29.176822 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139694190147152, session-id=None, name=debug, idle-time=0.0003609657287597656s, acquire-count=1, language=None, thread-identifier=(13527, 139694835589248), compute-name=) - Checking idleness
[0m11:31:29.177135 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139694190147152, session-id=None, name=debug, idle-time=0.0006759166717529297s, acquire-count=1, language=None, thread-identifier=(13527, 139694835589248), compute-name=) - Retrieving connection
[0m11:31:29.177409 [debug] [MainThread]: Using databricks connection "debug"
[0m11:31:29.177694 [debug] [MainThread]: On debug: select 1 as id
[0m11:31:29.177987 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:31:30.403342 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139694190147152, session-id=ff5a59f1-bb92-4866-83b9-e9a5c823f157, name=debug, idle-time=9.5367431640625e-06s, acquire-count=1, language=None, thread-identifier=(13527, 139694835589248), compute-name=) - Connection created
[0m11:31:30.404648 [debug] [MainThread]: Databricks adapter: Cursor(session-id=ff5a59f1-bb92-4866-83b9-e9a5c823f157, command-id=Unknown) - Created cursor
[0m11:31:30.787760 [debug] [MainThread]: SQL status: OK in 1.610 seconds
[0m11:31:30.788971 [debug] [MainThread]: Databricks adapter: Cursor(session-id=ff5a59f1-bb92-4866-83b9-e9a5c823f157, command-id=bf22d1d3-1617-4386-bdd8-fa496ac33588) - Closing cursor
[0m11:31:30.791895 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139694190147152, session-id=ff5a59f1-bb92-4866-83b9-e9a5c823f157, name=debug, idle-time=5.0067901611328125e-06s, acquire-count=0, language=None, thread-identifier=(13527, 139694835589248), compute-name=) - Released connection
[0m11:31:30.792310 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:31:30.792799 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:31:30.795021 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.08764, "process_in_blocks": "249504", "process_kernel_time": 1.147054, "process_mem_max_rss": "239444", "process_out_blocks": "24", "process_user_time": 2.828753}
[0m11:31:30.795604 [debug] [MainThread]: Command `dbt debug` succeeded at 11:31:30.795483 after 4.09 seconds
[0m11:31:30.795990 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:31:30.796375 [debug] [MainThread]: On debug: Close
[0m11:31:30.796804 [debug] [MainThread]: Databricks adapter: Connection(session-id=ff5a59f1-bb92-4866-83b9-e9a5c823f157) - Closing connection
[0m11:31:31.086438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d39accd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d395b0620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d1736f830>]}
[0m11:31:31.089145 [debug] [MainThread]: Flushing usage events
[0m11:31:32.159553 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:31:37.409099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85380a6210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85380a4a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85380a4b90>]}


============================== 11:31:37.411697 | fae56ac4-6f8d-49d3-947c-0928889c239a ==============================
[0m11:31:37.411697 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:31:37.412223 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:31:37.903095 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:31:37.903655 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:31:37.904068 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:31:38.594745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8516737fb0>]}
[0m11:31:38.648137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8515545040>]}
[0m11:31:38.648746 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:31:38.734188 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:31:39.019979 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:31:39.020381 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:31:39.068829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85152b4440>]}
[0m11:31:39.166097 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:31:39.174451 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:31:39.192635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85150ab710>]}
[0m11:31:39.193224 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:31:39.193650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8515498e90>]}
[0m11:31:39.195534 [info ] [MainThread]: 
[0m11:31:39.195985 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:31:39.196362 [info ] [MainThread]: 
[0m11:31:39.197066 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Creating connection
[0m11:31:39.197543 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:31:39.197955 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=5.7220458984375e-06s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Acquired connection on thread (13610, 140210208686208), using default compute resource
[0m11:31:39.204332 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Creating connection
[0m11:31:39.204886 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:31:39.205236 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=None, name=list_hive_metastore, idle-time=5.0067901611328125e-06s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Acquired connection on thread (13610, 140209558603456), using default compute resource
[0m11:31:39.205740 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=None, name=list_hive_metastore, idle-time=0.00048279762268066406s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:39.206187 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=None, name=list_hive_metastore, idle-time=0.0009317398071289062s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:39.206691 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:31:39.207006 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:31:39.207302 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:31:40.466645 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore, idle-time=1.1444091796875e-05s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Connection created
[0m11:31:40.468137 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:41.106822 [debug] [ThreadPool]: SQL status: OK in 1.900 seconds
[0m11:31:41.112484 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=bc1c5122-98c0-4209-adfa-4de3840db4a6) - Closing cursor
[0m11:31:41.113955 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore, idle-time=1.0251998901367188e-05s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:41.119506 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore, idle-time=0.005523681640625s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:41.121026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_saleslt)
[0m11:31:41.121938 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.008067607879638672s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Reusing connection previously named list_hive_metastore
[0m11:31:41.122824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.008957147598266602s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Acquired connection on thread (13610, 140209558603456), using default compute resource
[0m11:31:41.123752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.009896278381347656s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:41.124637 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.010723352432250977s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:41.125374 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:31:41.126132 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:31:41.127092 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:41.514034 [debug] [ThreadPool]: SQL status: OK in 0.390 seconds
[0m11:31:41.518541 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=9f6d542f-485a-48fc-82d9-139522c89cb7) - Closing cursor
[0m11:31:41.540356 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.42647552490234375s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:41.541044 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.4272480010986328s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:41.541608 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.42780637741088867s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:41.542151 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.4283628463745117s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:41.542709 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:31:41.543241 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:31:41.543828 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:31:41.544421 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:41.902601 [debug] [ThreadPool]: SQL status: OK in 0.360 seconds
[0m11:31:41.924291 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=7c0f6b44-6535-4cb7-b60c-f3a8c1e64261) - Closing cursor
[0m11:31:41.933049 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.8191437721252441s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:41.933643 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.81986403465271s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:41.934060 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:31:41.934501 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:31:41.934998 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:42.344285 [debug] [ThreadPool]: SQL status: OK in 0.410 seconds
[0m11:31:42.350119 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=9f4c8615-ba85-4bac-8f85-ba4cdf85bbec) - Closing cursor
[0m11:31:42.351751 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=8.344650268554688e-06s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:42.353102 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_saleslt, idle-time=0.001378774642944336s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:42.354041 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m11:31:42.359648 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.0077893733978271484s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m11:31:42.360562 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.008827447891235352s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Acquired connection on thread (13610, 140209558603456), using default compute resource
[0m11:31:42.361497 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.00972437858581543s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:42.362319 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.010602712631225586s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:42.363039 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:31:42.363881 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:31:42.364618 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:42.716736 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m11:31:42.719180 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=d07e9398-fd02-4655-88d9-3578c8c90bf5) - Closing cursor
[0m11:31:42.722914 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.37120652198791504s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:42.723677 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.37201833724975586s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:42.724161 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:31:42.724695 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:31:42.725250 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:43.050934 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m11:31:43.059429 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=d308ab2e-939d-4bf7-bec1-4fe48a377e66) - Closing cursor
[0m11:31:43.071333 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.7194809913635254s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:43.072622 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.7208304405212402s, acquire-count=1, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:43.073845 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:31:43.074874 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:31:43.075684 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:43.472690 [debug] [ThreadPool]: SQL status: OK in 0.400 seconds
[0m11:31:43.480829 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=b69602dd-39b1-4e1c-ba98-baff62cc3917) - Closing cursor
[0m11:31:43.482797 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=1.0728836059570312e-05s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:43.487452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8536376a20>]}
[0m11:31:43.488619 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=4.290597915649414s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Checking idleness
[0m11:31:43.489468 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=4.291495084762573s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Retrieving connection
[0m11:31:43.490175 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=4.292172908782959s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Checking idleness
[0m11:31:43.490937 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=4.292923450469971s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Retrieving connection
[0m11:31:43.491569 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:31:43.492183 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:31:43.492874 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=5.245208740234375e-06s, acquire-count=0, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Released connection
[0m11:31:43.496743 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m11:31:43.498281 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m11:31:43.499582 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=list_hive_metastore_snapshots, idle-time=0.01685190200805664s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:43.500493 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now model.medallion_spark_dbt.dim_customer)
[0m11:31:43.501142 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=0.0184478759765625s, acquire-count=0, language=None, thread-identifier=(13610, 140209558603456), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m11:31:43.501798 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=0.019098281860351562s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Acquired connection on thread (13610, 140209558603456), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m11:31:43.502446 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m11:31:43.517101 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m11:31:43.519774 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m11:31:43.531897 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:31:43.537558 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=0.05486917495727539s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:43.538170 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=0.05553793907165527s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:43.538529 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m11:31:43.538949 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m11:31:43.539390 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:44.550377 [debug] [Thread-1 (]: SQL status: OK in 1.010 seconds
[0m11:31:44.557095 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=dbd492ef-9468-4faf-ab39-024026dce052) - Closing cursor
[0m11:31:44.619554 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.dim_customer"
[0m11:31:44.620227 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=1.137606143951416s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:44.620597 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=1.1379971504211426s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:44.620883 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m11:31:44.621280 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m11:31:44.621679 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:48.480395 [debug] [Thread-1 (]: SQL status: OK in 3.860 seconds
[0m11:31:48.481950 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=06f9e72e-c15a-4728-9573-a091adf8d561) - Closing cursor
[0m11:31:48.515777 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=4.5299530029296875e-06s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:48.516312 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:48.518889 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8514f49100>]}
[0m11:31:48.519716 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 5.02s]
[0m11:31:48.520312 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m11:31:48.520715 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m11:31:48.521165 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m11:31:48.521908 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_customer, idle-time=0.00558781623840332s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:48.522220 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m11:31:48.522580 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.00627446174621582s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m11:31:48.522950 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.0066394805908203125s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Acquired connection on thread (13610, 140209558603456), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m11:31:48.523380 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m11:31:48.527929 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m11:31:48.528628 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m11:31:48.530527 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:31:48.532433 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.01610255241394043s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:48.532824 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.016519784927368164s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:48.533125 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m11:31:48.533463 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m11:31:48.533846 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:49.088814 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m11:31:49.093149 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=0a70aa87-6059-4e3d-b05d-c0a6869af1bb) - Closing cursor
[0m11:31:49.097196 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.dim_product"
[0m11:31:49.098703 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.5821824073791504s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:49.099439 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.5830485820770264s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:49.099885 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m11:31:49.100637 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m11:31:49.101301 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:52.004568 [debug] [Thread-1 (]: SQL status: OK in 2.900 seconds
[0m11:31:52.005774 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=e2be8fad-c018-43b9-b31a-3eb4014e62d5) - Closing cursor
[0m11:31:52.007959 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=4.291534423828125e-06s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:52.008593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:52.009136 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8514fb0e00>]}
[0m11:31:52.009829 [info ] [Thread-1 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 3.49s]
[0m11:31:52.010643 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m11:31:52.011257 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m11:31:52.012171 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m11:31:52.012999 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.dim_product, idle-time=0.004414796829223633s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:52.013434 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m11:31:52.013882 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=0.0053348541259765625s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m11:31:52.014279 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=0.005737781524658203s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Acquired connection on thread (13610, 140209558603456), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m11:31:52.014701 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m11:31:52.018932 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m11:31:52.019635 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m11:31:52.021487 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:31:52.023460 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=0.01488351821899414s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:52.023912 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=0.015384435653686523s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:52.024205 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m11:31:52.024558 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
[0m11:31:52.024916 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:52.608477 [debug] [Thread-1 (]: SQL status: OK in 0.580 seconds
[0m11:31:52.615621 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=582f044e-5173-4190-a2f6-248d33fa4f40) - Closing cursor
[0m11:31:52.621923 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.sales"
[0m11:31:52.623071 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=0.614408016204834s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Checking idleness
[0m11:31:52.623844 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=0.6151959896087646s, acquire-count=1, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Retrieving connection
[0m11:31:52.624424 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m11:31:52.625465 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m11:31:52.626364 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=Unknown) - Created cursor
[0m11:31:55.763665 [debug] [Thread-1 (]: SQL status: OK in 3.140 seconds
[0m11:31:55.766411 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=c0f54331-60cf-4460-879b-89319f07ae55, command-id=f32e7770-63c7-49dc-b62d-c67f7db4be97) - Closing cursor
[0m11:31:55.770718 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:55.771633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140209560411056, session-id=c0f54331-60cf-4460-879b-89319f07ae55, name=model.medallion_spark_dbt.sales, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(13610, 140209558603456), compute-name=) - Released connection
[0m11:31:55.772454 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fae56ac4-6f8d-49d3-947c-0928889c239a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8514fb0e00>]}
[0m11:31:55.773526 [info ] [Thread-1 (]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 3.76s]
[0m11:31:55.774694 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m11:31:55.777530 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=12.284642219543457s, acquire-count=0, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Checking idleness
[0m11:31:55.778359 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=12.285511493682861s, acquire-count=0, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Reusing connection previously named master
[0m11:31:55.779163 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=12.286316871643066s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Acquired connection on thread (13610, 140210208686208), using default compute resource
[0m11:31:55.779895 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=12.287079095840454s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Checking idleness
[0m11:31:55.780448 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=None, name=master, idle-time=12.287636756896973s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Retrieving connection
[0m11:31:55.780947 [debug] [MainThread]: On master: ROLLBACK
[0m11:31:55.781477 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:31:56.949299 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=bd2c6a4c-95dd-41c3-b19c-b2667f9d575c, name=master, idle-time=2.3365020751953125e-05s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Connection created
[0m11:31:56.951702 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:31:56.954178 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=bd2c6a4c-95dd-41c3-b19c-b2667f9d575c, name=master, idle-time=0.004956722259521484s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Checking idleness
[0m11:31:56.956126 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=bd2c6a4c-95dd-41c3-b19c-b2667f9d575c, name=master, idle-time=0.007002115249633789s, acquire-count=1, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Retrieving connection
[0m11:31:56.957907 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:31:56.959155 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:31:56.960360 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140209560389248, session-id=bd2c6a4c-95dd-41c3-b19c-b2667f9d575c, name=master, idle-time=6.4373016357421875e-06s, acquire-count=0, language=None, thread-identifier=(13610, 140210208686208), compute-name=) - Released connection
[0m11:31:56.961786 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:31:56.962897 [debug] [MainThread]: On master: ROLLBACK
[0m11:31:56.963678 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:31:56.964444 [debug] [MainThread]: On master: Close
[0m11:31:56.965232 [debug] [MainThread]: Databricks adapter: Connection(session-id=bd2c6a4c-95dd-41c3-b19c-b2667f9d575c) - Closing connection
[0m11:31:57.245765 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m11:31:57.246591 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m11:31:57.247259 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:31:57.247885 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m11:31:57.248617 [debug] [MainThread]: Databricks adapter: Connection(session-id=c0f54331-60cf-4460-879b-89319f07ae55) - Closing connection
[0m11:31:57.527125 [info ] [MainThread]: 
[0m11:31:57.528314 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 18.33 seconds (18.33s).
[0m11:31:57.530889 [debug] [MainThread]: Command end result
[0m11:31:57.578056 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:31:57.579572 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:31:57.586741 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:31:57.587276 [info ] [MainThread]: 
[0m11:31:57.587739 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:31:57.588236 [info ] [MainThread]: 
[0m11:31:57.588842 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m11:31:57.590225 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 20.170862, "process_in_blocks": "8096", "process_kernel_time": 1.087625, "process_mem_max_rss": "246156", "process_out_blocks": "3240", "process_user_time": 3.462442}
[0m11:31:57.590681 [debug] [MainThread]: Command `dbt run` succeeded at 11:31:57.590575 after 20.17 seconds
[0m11:31:57.591110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8538510b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8538512cc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8538513f50>]}
[0m11:31:57.591516 [debug] [MainThread]: Flushing usage events
[0m11:31:58.800332 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:32:03.582808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdfef03cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdfef00c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdfef03350>]}


============================== 11:32:03.585390 | d21fabd6-1329-40a1-a516-db161e66cb3d ==============================
[0m11:32:03.585390 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:32:03.585844 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:32:04.072896 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:32:04.073371 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:32:04.073833 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:32:04.781609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd21fabd6-1329-40a1-a516-db161e66cb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdff46ee40>]}
[0m11:32:04.840333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd21fabd6-1329-40a1-a516-db161e66cb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcde0bffb30>]}
[0m11:32:04.840939 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:32:04.923902 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:32:05.146940 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:32:05.147601 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:32:05.202464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd21fabd6-1329-40a1-a516-db161e66cb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcddbca4770>]}
[0m11:32:05.297597 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:32:05.299779 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:32:05.328392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd21fabd6-1329-40a1-a516-db161e66cb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcddba97620>]}
[0m11:32:05.329097 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:32:05.329511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd21fabd6-1329-40a1-a516-db161e66cb3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcddbd06120>]}
[0m11:32:05.331150 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:32:05.332729 [debug] [MainThread]: Command end result
[0m11:32:05.361951 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:32:05.363342 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:32:05.365870 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:32:05.366651 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.8371363, "process_in_blocks": "1984", "process_kernel_time": 0.971634, "process_mem_max_rss": "237016", "process_out_blocks": "3064", "process_user_time": 2.874835}
[0m11:32:05.367039 [debug] [MainThread]: Command `dbt test` succeeded at 11:32:05.366947 after 1.84 seconds
[0m11:32:05.367382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcddbd44f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdfec66fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcddbe532f0>]}
[0m11:32:05.367717 [debug] [MainThread]: Flushing usage events
[0m11:32:06.261804 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:36:41.301462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe7a72bf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe79c0ebd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe79c0c350>]}


============================== 11:36:41.305251 | 75892037-a8b4-442f-958a-73a174de61ae ==============================
[0m11:36:41.305251 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:36:41.305958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m11:36:41.320692 [info ] [MainThread]: dbt version: 1.9.2
[0m11:36:41.321274 [info ] [MainThread]: python version: 3.12.3
[0m11:36:41.321699 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m11:36:41.322082 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m11:36:41.925758 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:36:41.926319 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:36:41.926726 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:36:42.536526 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m11:36:42.537104 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m11:36:42.537521 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m11:36:42.537922 [info ] [MainThread]: adapter type: databricks
[0m11:36:42.538316 [info ] [MainThread]: adapter version: 1.9.4
[0m11:36:42.657771 [info ] [MainThread]: Configuration:
[0m11:36:42.658351 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:36:42.658754 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:36:42.659155 [info ] [MainThread]: Required dependencies:
[0m11:36:42.659557 [debug] [MainThread]: Executing "git --help"
[0m11:36:42.661557 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:36:42.661965 [debug] [MainThread]: STDERR: "b''"
[0m11:36:42.662292 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:36:42.662656 [info ] [MainThread]: Connection:
[0m11:36:42.663044 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m11:36:42.663472 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m11:36:42.663856 [info ] [MainThread]:   catalog: hive_metastore
[0m11:36:42.664284 [info ] [MainThread]:   schema: saleslt
[0m11:36:42.664952 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:36:42.741662 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139630843819600, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13813, 139631491018880), compute-name=) - Creating connection
[0m11:36:42.742103 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m11:36:42.742507 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139630843819600, session-id=None, name=debug, idle-time=5.245208740234375e-06s, acquire-count=1, language=None, thread-identifier=(13813, 139631491018880), compute-name=) - Acquired connection on thread (13813, 139631491018880), using default compute resource
[0m11:36:42.742921 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139630843819600, session-id=None, name=debug, idle-time=0.0004055500030517578s, acquire-count=1, language=None, thread-identifier=(13813, 139631491018880), compute-name=) - Checking idleness
[0m11:36:42.743306 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139630843819600, session-id=None, name=debug, idle-time=0.0008111000061035156s, acquire-count=1, language=None, thread-identifier=(13813, 139631491018880), compute-name=) - Retrieving connection
[0m11:36:42.743630 [debug] [MainThread]: Using databricks connection "debug"
[0m11:36:42.743953 [debug] [MainThread]: On debug: select 1 as id
[0m11:36:42.744288 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:36:44.115361 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139630843819600, session-id=608746ee-8ece-4bc0-a520-34326b818042, name=debug, idle-time=4.5299530029296875e-06s, acquire-count=1, language=None, thread-identifier=(13813, 139631491018880), compute-name=) - Connection created
[0m11:36:44.115954 [debug] [MainThread]: Databricks adapter: Cursor(session-id=608746ee-8ece-4bc0-a520-34326b818042, command-id=Unknown) - Created cursor
[0m11:36:44.508909 [debug] [MainThread]: SQL status: OK in 1.760 seconds
[0m11:36:44.512299 [debug] [MainThread]: Databricks adapter: Cursor(session-id=608746ee-8ece-4bc0-a520-34326b818042, command-id=5411beb9-e01d-4b46-9fce-cc967472f5ed) - Closing cursor
[0m11:36:44.513480 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139630843819600, session-id=608746ee-8ece-4bc0-a520-34326b818042, name=debug, idle-time=8.344650268554688e-06s, acquire-count=0, language=None, thread-identifier=(13813, 139631491018880), compute-name=) - Released connection
[0m11:36:44.514317 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:36:44.515429 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:36:44.517397 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.2779288, "process_in_blocks": "0", "process_kernel_time": 0.844271, "process_mem_max_rss": "241432", "process_out_blocks": "24", "process_user_time": 3.145918}
[0m11:36:44.518264 [debug] [MainThread]: Command `dbt debug` succeeded at 11:36:44.518054 after 3.28 seconds
[0m11:36:44.518971 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:36:44.519673 [debug] [MainThread]: On debug: Close
[0m11:36:44.520363 [debug] [MainThread]: Databricks adapter: Connection(session-id=608746ee-8ece-4bc0-a520-34326b818042) - Closing connection
[0m11:36:44.857363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe7a0a27e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe5c3cfce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe5d85e870>]}
[0m11:36:44.858966 [debug] [MainThread]: Flushing usage events
[0m11:36:45.927050 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:36:50.541238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff1754e000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff16fdf920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff17795d90>]}


============================== 11:36:50.544359 | 7db99e25-9f79-4437-bacb-7816025142a2 ==============================
[0m11:36:50.544359 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:36:50.544922 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:36:51.074235 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:36:51.074802 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:36:51.075179 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:36:51.733954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff1774ae10>]}
[0m11:36:51.789746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef8a58aa0>]}
[0m11:36:51.790422 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:36:51.874975 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:36:52.073912 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:36:52.074396 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:36:52.133449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef3ffea80>]}
[0m11:36:52.230403 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:36:52.232038 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:36:52.244541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef3db8320>]}
[0m11:36:52.245003 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:36:52.245372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef8206120>]}
[0m11:36:52.247159 [info ] [MainThread]: 
[0m11:36:52.247620 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:36:52.247940 [info ] [MainThread]: 
[0m11:36:52.248571 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Creating connection
[0m11:36:52.248881 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:36:52.249210 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=3.5762786865234375e-06s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Acquired connection on thread (13851, 139634126336128), using default compute resource
[0m11:36:52.256029 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Creating connection
[0m11:36:52.256650 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:36:52.257021 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=None, name=list_hive_metastore, idle-time=4.291534423828125e-06s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Acquired connection on thread (13851, 139633476236992), using default compute resource
[0m11:36:52.257534 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=None, name=list_hive_metastore, idle-time=0.0003991127014160156s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:52.257912 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=None, name=list_hive_metastore, idle-time=0.0008716583251953125s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:52.258288 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:36:52.258837 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:36:52.259237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:53.056498 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore, idle-time=5.0067901611328125e-06s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Connection created
[0m11:36:53.057115 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:53.623852 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m11:36:53.628720 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=b36db26f-c879-4a1a-ba2f-5e52fb67afc5) - Closing cursor
[0m11:36:53.630344 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore, idle-time=1.2874603271484375e-05s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:53.636330 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore, idle-time=0.006052255630493164s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:53.637820 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_snapshots)
[0m11:36:53.638917 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.008712530136108398s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Reusing connection previously named list_hive_metastore
[0m11:36:53.639752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.009580850601196289s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Acquired connection on thread (13851, 139633476236992), using default compute resource
[0m11:36:53.640538 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.01037907600402832s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:53.641321 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.011159181594848633s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:53.642069 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:36:53.642753 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:36:53.643557 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:53.925150 [debug] [ThreadPool]: SQL status: OK in 0.280 seconds
[0m11:36:53.929421 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=3492687c-c175-4da1-b05a-85669e05e120) - Closing cursor
[0m11:36:53.952029 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.32177162170410156s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:53.952733 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.3226051330566406s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:53.953345 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.3232285976409912s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:53.953888 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.3237729072570801s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:53.954402 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:36:53.954867 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:36:53.955421 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:36:53.955973 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:54.245385 [debug] [ThreadPool]: SQL status: OK in 0.290 seconds
[0m11:36:54.255411 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=fa46adc2-3a78-4ddb-8e8a-a2272c6b3cf4) - Closing cursor
[0m11:36:54.267112 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.6369121074676514s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:54.267944 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.6377911567687988s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:54.268561 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:36:54.269279 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:36:54.269952 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:54.604079 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m11:36:54.611200 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=bddbb886-cd8a-49d7-8244-73ecccae5114) - Closing cursor
[0m11:36:54.613095 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=7.62939453125e-06s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:54.614561 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_snapshots, idle-time=0.00150299072265625s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:54.615506 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m11:36:54.619823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.0067310333251953125s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m11:36:54.620824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.0077631473541259766s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Acquired connection on thread (13851, 139633476236992), using default compute resource
[0m11:36:54.621623 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.00861501693725586s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:54.622282 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.009273290634155273s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:54.622914 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:36:54.623518 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:36:54.624187 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:54.908832 [debug] [ThreadPool]: SQL status: OK in 0.280 seconds
[0m11:36:54.913087 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=dcb6e620-f4db-4e6c-a89a-d1402dd9f644) - Closing cursor
[0m11:36:54.917765 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.3046748638153076s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:54.918851 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.30579471588134766s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:54.919605 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:36:54.920459 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:36:54.921287 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:55.170027 [debug] [ThreadPool]: SQL status: OK in 0.250 seconds
[0m11:36:55.178259 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=8cd99ed5-598c-4402-86d1-3c9d5143a646) - Closing cursor
[0m11:36:55.190475 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.5772290229797363s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:55.191874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.5787553787231445s, acquire-count=1, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:55.192807 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:36:55.193666 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:36:55.194507 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:55.525614 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m11:36:55.530958 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=e07cf158-c052-400a-bb04-2a419476fd9e) - Closing cursor
[0m11:36:55.532820 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=8.344650268554688e-06s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:55.538948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef8252bd0>]}
[0m11:36:55.540018 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=3.2907042503356934s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Checking idleness
[0m11:36:55.540773 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=3.2915027141571045s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Retrieving connection
[0m11:36:55.541388 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=3.2921228408813477s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Checking idleness
[0m11:36:55.541997 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=3.2927327156066895s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Retrieving connection
[0m11:36:55.542714 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:36:55.543276 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:36:55.543879 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=4.291534423828125e-06s, acquire-count=0, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Released connection
[0m11:36:55.548517 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m11:36:55.549361 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m11:36:55.550604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=list_hive_metastore_saleslt, idle-time=0.017592430114746094s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:55.551141 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now model.medallion_spark_dbt.dim_customer)
[0m11:36:55.551778 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=0.019106388092041016s, acquire-count=0, language=None, thread-identifier=(13851, 139633476236992), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m11:36:55.552443 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=0.01973271369934082s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Acquired connection on thread (13851, 139633476236992), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m11:36:55.553121 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m11:36:55.563590 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m11:36:55.564468 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m11:36:55.577084 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:36:55.582779 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=0.050071001052856445s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:55.583498 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=0.05088996887207031s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:55.583889 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m11:36:55.584387 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m11:36:55.584840 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:56.021985 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Closing cursor
[0m11:36:56.025442 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=7a9b5e48-de88-42b4-ad4c-5ee3a37f4ebc
[0m11:36:56.028309 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:36:56.030203 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=9.298324584960938e-06s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:56.059510 [debug] [Thread-1 (]: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:36:56.060187 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=5.245208740234375e-06s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:56.062005 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef3c51a90>]}
[0m11:36:56.062790 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.51s]
[0m11:36:56.063593 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m11:36:56.064112 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m11:36:56.064817 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m11:36:56.065579 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table..
[0m11:36:56.066301 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_customer, idle-time=0.00614619255065918s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:56.067738 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m11:36:56.068252 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=0.008108377456665039s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m11:36:56.068718 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=0.008572816848754883s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Acquired connection on thread (13851, 139633476236992), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m11:36:56.069180 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m11:36:56.074376 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m11:36:56.075453 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m11:36:56.078914 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:36:56.083037 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=0.022665023803710938s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:56.083904 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=0.02372002601623535s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:56.084408 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m11:36:56.084894 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m11:36:56.085362 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:56.402046 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Closing cursor
[0m11:36:56.405339 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=3df8779e-8804-49dc-abd5-093d99d3ebd9
[0m11:36:56.409070 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:36:56.410890 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=9.775161743164062e-06s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:56.422062 [debug] [Thread-1 (]: Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:36:56.423344 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:56.424388 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef0333380>]}
[0m11:36:56.425815 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.36s]
[0m11:36:56.427416 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m11:36:56.428446 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m11:36:56.429734 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_product' to be skipped because of status 'error'.  Reason: Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table..
[0m11:36:56.430904 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m11:36:56.432922 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.dim_product, idle-time=0.009520292282104492s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:56.433698 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m11:36:56.434537 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.sales, idle-time=0.011275529861450195s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m11:36:56.435544 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.sales, idle-time=0.012272834777832031s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Acquired connection on thread (13851, 139633476236992), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m11:36:56.436312 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m11:36:56.442704 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m11:36:56.443627 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m11:36:56.446246 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:36:56.451518 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.sales, idle-time=0.02834630012512207s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Checking idleness
[0m11:36:56.452129 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.sales, idle-time=0.0290224552154541s, acquire-count=1, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Retrieving connection
[0m11:36:56.452529 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m11:36:56.453010 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
[0m11:36:56.453485 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Created cursor
[0m11:36:56.812942 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=507987fa-2762-41f0-90cc-af503bab89bf, command-id=Unknown) - Closing cursor
[0m11:36:56.814982 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=c23503cc-193b-44bc-ba54-4b013778df07
[0m11:36:56.817262 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:36:56.819090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.sales, idle-time=1.0728836059570312e-05s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:56.827775 [debug] [Thread-1 (]: Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:36:56.828920 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139633478048880, session-id=507987fa-2762-41f0-90cc-af503bab89bf, name=model.medallion_spark_dbt.sales, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(13851, 139633476236992), compute-name=) - Released connection
[0m11:36:56.829964 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7db99e25-9f79-4437-bacb-7816025142a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efef03481a0>]}
[0m11:36:56.831411 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.40s]
[0m11:36:56.832618 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m11:36:56.833798 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.sales' to be skipped because of status 'error'.  Reason: Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table..
[0m11:36:56.836469 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=1.2925708293914795s, acquire-count=0, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Checking idleness
[0m11:36:56.837204 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=1.2932395935058594s, acquire-count=0, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Reusing connection previously named master
[0m11:36:56.838006 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=1.294093370437622s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Acquired connection on thread (13851, 139634126336128), using default compute resource
[0m11:36:56.838631 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=1.2947633266448975s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Checking idleness
[0m11:36:56.839195 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=None, name=master, idle-time=1.2953293323516846s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Retrieving connection
[0m11:36:56.839780 [debug] [MainThread]: On master: ROLLBACK
[0m11:36:56.840335 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:36:57.938510 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=9abd925f-6faa-48fd-8dae-0435e9d0c468, name=master, idle-time=9.5367431640625e-06s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Connection created
[0m11:36:57.939626 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:36:57.940689 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=9abd925f-6faa-48fd-8dae-0435e9d0c468, name=master, idle-time=0.0022478103637695312s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Checking idleness
[0m11:36:57.941684 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=9abd925f-6faa-48fd-8dae-0435e9d0c468, name=master, idle-time=0.0032532215118408203s, acquire-count=1, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Retrieving connection
[0m11:36:57.942628 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:36:57.943468 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:36:57.944440 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139633479527568, session-id=9abd925f-6faa-48fd-8dae-0435e9d0c468, name=master, idle-time=6.198883056640625e-06s, acquire-count=0, language=None, thread-identifier=(13851, 139634126336128), compute-name=) - Released connection
[0m11:36:57.945360 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:36:57.946136 [debug] [MainThread]: On master: ROLLBACK
[0m11:36:57.954639 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:36:57.955284 [debug] [MainThread]: On master: Close
[0m11:36:57.955799 [debug] [MainThread]: Databricks adapter: Connection(session-id=9abd925f-6faa-48fd-8dae-0435e9d0c468) - Closing connection
[0m11:36:58.258238 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m11:36:58.260557 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m11:36:58.262389 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:36:58.264090 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m11:36:58.266012 [debug] [MainThread]: Databricks adapter: Connection(session-id=507987fa-2762-41f0-90cc-af503bab89bf) - Closing connection
[0m11:36:58.464904 [info ] [MainThread]: 
[0m11:36:58.465860 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 6.22 seconds (6.22s).
[0m11:36:58.467974 [debug] [MainThread]: Command end result
[0m11:36:58.505788 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:36:58.512869 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:36:58.518194 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:36:58.518550 [info ] [MainThread]: 
[0m11:36:58.518969 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:36:58.519339 [info ] [MainThread]: 
[0m11:36:58.519769 [error] [MainThread]:   Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:36:58.520120 [info ] [MainThread]: 
[0m11:36:58.520518 [error] [MainThread]:   Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:36:58.520874 [info ] [MainThread]: 
[0m11:36:58.521249 [error] [MainThread]:   Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:36:58.521576 [info ] [MainThread]: 
[0m11:36:58.521956 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
[0m11:36:58.522752 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.036619, "process_in_blocks": "1160", "process_kernel_time": 1.037903, "process_mem_max_rss": "250056", "process_out_blocks": "3224", "process_user_time": 3.472984}
[0m11:36:58.523143 [debug] [MainThread]: Command `dbt run` failed at 11:36:58.523051 after 8.04 seconds
[0m11:36:58.523492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff1740a4b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff17909f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eff179095b0>]}
[0m11:36:58.523862 [debug] [MainThread]: Flushing usage events
[0m11:36:59.650374 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:39:21.167008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1410238f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa14050eb40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa140a6ab40>]}


============================== 11:39:21.176483 | d474b649-6148-4987-9734-cbfca703e42a ==============================
[0m11:39:21.176483 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:39:21.177215 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:39:21.923745 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:39:21.924256 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:39:21.924585 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:39:23.456941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11fdf6b40>]}
[0m11:39:23.507909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11e11c3b0>]}
[0m11:39:23.508546 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:39:23.610858 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:39:23.906792 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:39:23.907178 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:39:23.955995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa13ffdb710>]}
[0m11:39:24.053829 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:39:24.057001 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:39:24.082840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11cf98410>]}
[0m11:39:24.083345 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:39:24.083719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11d153e90>]}
[0m11:39:24.085787 [info ] [MainThread]: 
[0m11:39:24.086258 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:39:24.086591 [info ] [MainThread]: 
[0m11:39:24.087143 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Creating connection
[0m11:39:24.087467 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:39:24.087871 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=4.76837158203125e-06s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Acquired connection on thread (14004, 140330600935552), using default compute resource
[0m11:39:24.094155 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Creating connection
[0m11:39:24.094906 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:39:24.095570 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=None, name=list_hive_metastore, idle-time=6.9141387939453125e-06s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Acquired connection on thread (14004, 140329950889664), using default compute resource
[0m11:39:24.096046 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=None, name=list_hive_metastore, idle-time=0.0005269050598144531s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:24.096413 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=None, name=list_hive_metastore, idle-time=0.0008881092071533203s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:24.096726 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:39:24.097066 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:39:24.097420 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:39:25.353128 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore, idle-time=6.198883056640625e-06s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Connection created
[0m11:39:25.353743 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:26.045935 [debug] [ThreadPool]: SQL status: OK in 1.950 seconds
[0m11:39:26.048469 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=36e776fb-71a9-4ee5-9387-6a2ee4eb1ef3) - Closing cursor
[0m11:39:26.049134 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore, idle-time=5.7220458984375e-06s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:26.051920 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore, idle-time=0.0028014183044433594s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:26.052553 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_saleslt)
[0m11:39:26.053026 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.003924131393432617s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Reusing connection previously named list_hive_metastore
[0m11:39:26.053492 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.004373788833618164s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Acquired connection on thread (14004, 140329950889664), using default compute resource
[0m11:39:26.053963 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.004869937896728516s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:26.054537 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.005400657653808594s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:26.055155 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:39:26.055752 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:39:26.056335 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:26.407907 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m11:39:26.409897 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=81ecaf14-c4ea-43a6-adef-029f134c312a) - Closing cursor
[0m11:39:26.423194 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.37406253814697266s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:26.423722 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.37464165687561035s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:26.424142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.37507128715515137s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:26.424543 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.37545084953308105s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:26.424917 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:39:26.425238 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:39:26.425592 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:39:26.426084 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:26.862293 [debug] [ThreadPool]: SQL status: OK in 0.440 seconds
[0m11:39:26.885974 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=d3023b4b-2d49-47ab-9255-85b4c438e4fe) - Closing cursor
[0m11:39:26.895766 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.8466076850891113s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:26.896451 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.8473460674285889s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:26.896921 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:39:26.897412 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:39:26.897886 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:27.387821 [debug] [ThreadPool]: SQL status: OK in 0.490 seconds
[0m11:39:27.395471 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=037de25e-63f8-406b-9eec-9a0ab18b4afa) - Closing cursor
[0m11:39:27.397738 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=9.059906005859375e-06s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:27.399517 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_saleslt, idle-time=0.001916646957397461s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:27.405655 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m11:39:27.406723 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.009043455123901367s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m11:39:27.407890 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.010257244110107422s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Acquired connection on thread (14004, 140329950889664), using default compute resource
[0m11:39:27.409075 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.011450529098510742s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:27.409843 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.012296915054321289s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:27.410511 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:39:27.411212 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:39:27.412049 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:27.833417 [debug] [ThreadPool]: SQL status: OK in 0.420 seconds
[0m11:39:27.835110 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=0270c42b-9f9e-461d-9898-25f915869b6d) - Closing cursor
[0m11:39:27.837716 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.44020795822143555s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:27.838181 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.4406929016113281s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:27.838596 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:39:27.839013 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:39:27.839458 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:28.185243 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m11:39:28.187754 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=76e90eb8-faaa-4f51-819e-632249429034) - Closing cursor
[0m11:39:28.192245 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.79463791847229s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:28.192857 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.7953226566314697s, acquire-count=1, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:28.193411 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:39:28.193985 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:39:28.194632 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:28.588911 [debug] [ThreadPool]: SQL status: OK in 0.390 seconds
[0m11:39:28.594657 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=46391beb-ae18-4c77-a231-e53627c43422) - Closing cursor
[0m11:39:28.596427 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=7.62939453125e-06s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:28.601676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1418ea420>]}
[0m11:39:28.603330 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=4.515295505523682s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Checking idleness
[0m11:39:28.604369 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=4.516390800476074s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Retrieving connection
[0m11:39:28.605279 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=4.517324686050415s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Checking idleness
[0m11:39:28.606054 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=4.5181050300598145s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Retrieving connection
[0m11:39:28.606792 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:39:28.607628 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:39:28.608406 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=5.4836273193359375e-06s, acquire-count=0, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Released connection
[0m11:39:28.612559 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m11:39:28.613649 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m11:39:28.614684 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=list_hive_metastore_snapshots, idle-time=0.018290996551513672s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:28.615648 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now model.medallion_spark_dbt.dim_customer)
[0m11:39:28.616376 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=0.01998138427734375s, acquire-count=0, language=None, thread-identifier=(14004, 140329950889664), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m11:39:28.617105 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=0.020707368850708008s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Acquired connection on thread (14004, 140329950889664), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m11:39:28.617839 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m11:39:28.633900 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m11:39:28.636041 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m11:39:28.649304 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:39:28.655162 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=0.05871891975402832s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:28.655753 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=0.059445858001708984s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:28.656101 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m11:39:28.656525 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m11:39:28.656969 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:29.180407 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Closing cursor
[0m11:39:29.182505 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=6e488488-93a9-45f1-9dc7-367dcd43e89a
[0m11:39:29.184726 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:39:29.186962 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=8.58306884765625e-06s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:29.212702 [debug] [Thread-1 (]: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:39:29.213476 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=4.5299530029296875e-06s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:29.215898 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11ce42d80>]}
[0m11:39:29.216670 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.60s]
[0m11:39:29.217403 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m11:39:29.218040 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m11:39:29.218775 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table..
[0m11:39:29.219436 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m11:39:29.221066 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_customer, idle-time=0.007634401321411133s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:29.221464 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m11:39:29.221914 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=0.008498907089233398s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m11:39:29.222436 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=0.009021759033203125s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Acquired connection on thread (14004, 140329950889664), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m11:39:29.222880 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m11:39:29.226781 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m11:39:29.227496 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m11:39:29.229706 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:39:29.231891 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=0.018461227416992188s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:29.232395 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=0.018980026245117188s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:29.232746 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m11:39:29.233287 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m11:39:29.233725 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:29.628642 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Closing cursor
[0m11:39:29.629934 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=73d1791c-6b4f-440a-8bb9-c6f4de99c248
[0m11:39:29.631200 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:39:29.632233 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=5.7220458984375e-06s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:29.637708 [debug] [Thread-1 (]: Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:39:29.638989 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=7.3909759521484375e-06s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:29.639909 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11c50c110>]}
[0m11:39:29.641082 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.42s]
[0m11:39:29.643221 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m11:39:29.644509 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m11:39:29.645275 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_product' to be skipped because of status 'error'.  Reason: Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table..
[0m11:39:29.646057 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m11:39:29.647179 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.dim_product, idle-time=0.008292436599731445s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:29.647704 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m11:39:29.648297 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.sales, idle-time=0.009458065032958984s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m11:39:29.648979 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.sales, idle-time=0.010074377059936523s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Acquired connection on thread (14004, 140329950889664), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m11:39:29.649725 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m11:39:29.653875 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m11:39:29.654529 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m11:39:29.656790 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:39:29.661593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.sales, idle-time=0.022669315338134766s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Checking idleness
[0m11:39:29.662482 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.sales, idle-time=0.023588180541992188s, acquire-count=1, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Retrieving connection
[0m11:39:29.662993 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m11:39:29.663506 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
[0m11:39:29.663995 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Created cursor
[0m11:39:30.040085 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, command-id=Unknown) - Closing cursor
[0m11:39:30.043583 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=b52cc3ea-4dd0-4810-b6a2-4006c554025f
[0m11:39:30.047222 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:39:30.049215 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.sales, idle-time=7.152557373046875e-06s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:30.059238 [debug] [Thread-1 (]: Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:39:30.060309 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140329952582912, session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e, name=model.medallion_spark_dbt.sales, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(14004, 140329950889664), compute-name=) - Released connection
[0m11:39:30.061315 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd474b649-6148-4987-9734-cbfca703e42a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa11c52be90>]}
[0m11:39:30.062891 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.41s]
[0m11:39:30.064368 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m11:39:30.065717 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.sales' to be skipped because of status 'error'.  Reason: Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table..
[0m11:39:30.069189 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=1.4607441425323486s, acquire-count=0, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Checking idleness
[0m11:39:30.070292 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=1.4618279933929443s, acquire-count=0, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Reusing connection previously named master
[0m11:39:30.071150 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=1.4627304077148438s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Acquired connection on thread (14004, 140330600935552), using default compute resource
[0m11:39:30.072382 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=1.4637868404388428s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Checking idleness
[0m11:39:30.073330 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=None, name=master, idle-time=1.464855670928955s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Retrieving connection
[0m11:39:30.074270 [debug] [MainThread]: On master: ROLLBACK
[0m11:39:30.075127 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:39:31.239841 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=bda61968-80de-49ba-af76-7f78f0f73c26, name=master, idle-time=9.5367431640625e-06s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Connection created
[0m11:39:31.241423 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:39:31.243213 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=bda61968-80de-49ba-af76-7f78f0f73c26, name=master, idle-time=0.0032913684844970703s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Checking idleness
[0m11:39:31.244792 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=bda61968-80de-49ba-af76-7f78f0f73c26, name=master, idle-time=0.005045652389526367s, acquire-count=1, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Retrieving connection
[0m11:39:31.245621 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:39:31.246399 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:39:31.247298 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140329956757280, session-id=bda61968-80de-49ba-af76-7f78f0f73c26, name=master, idle-time=5.4836273193359375e-06s, acquire-count=0, language=None, thread-identifier=(14004, 140330600935552), compute-name=) - Released connection
[0m11:39:31.248258 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:39:31.249101 [debug] [MainThread]: On master: ROLLBACK
[0m11:39:31.249875 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:39:31.250613 [debug] [MainThread]: On master: Close
[0m11:39:31.251473 [debug] [MainThread]: Databricks adapter: Connection(session-id=bda61968-80de-49ba-af76-7f78f0f73c26) - Closing connection
[0m11:39:31.551228 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m11:39:31.553650 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m11:39:31.555647 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:39:31.557022 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m11:39:31.558261 [debug] [MainThread]: Databricks adapter: Connection(session-id=0edc77d3-37eb-46ec-aa99-b837a5c2eb2e) - Closing connection
[0m11:39:31.854973 [info ] [MainThread]: 
[0m11:39:31.857579 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 7.76 seconds (7.76s).
[0m11:39:31.861800 [debug] [MainThread]: Command end result
[0m11:39:31.906838 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:39:31.908661 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:39:31.916206 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:39:31.916622 [info ] [MainThread]: 
[0m11:39:31.917071 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:39:31.917443 [info ] [MainThread]: 
[0m11:39:31.917841 [error] [MainThread]:   Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:39:31.918172 [info ] [MainThread]: 
[0m11:39:31.918549 [error] [MainThread]:   Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:39:31.918889 [info ] [MainThread]: 
[0m11:39:31.919254 [error] [MainThread]:   Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:39:31.919692 [info ] [MainThread]: 
[0m11:39:31.920433 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
[0m11:39:31.923035 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 10.825098, "process_in_blocks": "258040", "process_kernel_time": 1.109692, "process_mem_max_rss": "249728", "process_out_blocks": "3232", "process_user_time": 3.878835}
[0m11:39:31.923599 [debug] [MainThread]: Command `dbt run` failed at 11:39:31.923460 after 10.83 seconds
[0m11:39:31.924076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1403419d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1403432f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa13fe014c0>]}
[0m11:39:31.924581 [debug] [MainThread]: Flushing usage events
[0m11:39:33.316751 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:47:00.993605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa45491ade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa453086120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa453084290>]}


============================== 11:47:01.003048 | 9feb72f2-384e-43a5-a4a1-4c60a534122f ==============================
[0m11:47:01.003048 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:47:01.003772 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt clean', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:47:01.161478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9feb72f2-384e-43a5-a4a1-4c60a534122f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa452b4a4b0>]}
[0m11:47:01.198841 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.26921776, "process_in_blocks": "4408", "process_kernel_time": 0.069678, "process_mem_max_rss": "96356", "process_out_blocks": "8", "process_user_time": 1.44334}
[0m11:47:01.199345 [debug] [MainThread]: Command `dbt clean` succeeded at 11:47:01.199242 after 0.27 seconds
[0m11:47:01.199695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4537716d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa453084290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa453dd6630>]}
[0m11:47:01.200135 [debug] [MainThread]: Flushing usage events
[0m11:47:02.201398 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:47:09.301067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e4bf44a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e4eb4fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e6462f60>]}


============================== 11:47:09.304083 | b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178 ==============================
[0m11:47:09.304083 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:47:09.304596 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'True'}
[0m11:47:10.024039 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:47:10.024541 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:47:10.025165 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:47:11.461089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e4d1dd30>]}
[0m11:47:11.516597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3c1fe4e60>]}
[0m11:47:11.517304 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:47:11.623244 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:47:11.624004 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:47:11.624496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3c2a32b40>]}
[0m11:47:13.283613 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models/marts/sales/sales.yml'
[0m11:47:13.405758 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models/marts/customer/dim_customer.yml'
[0m11:47:13.416653 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models/marts/product/dim_product.yml'
[0m11:47:13.509285 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderID.810c5f247c' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.509946 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderID.48ce11e7f3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.510633 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderDetailID.343b942405' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.511115 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.511640 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderQty.66af966596' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.512084 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productID.cbf6d34890' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.512507 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_unitPrice.3545b5473a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.512925 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_lineTotal.d55bca27f8' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.513327 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_name.4c7b961f77' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.513789 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productNumber.3a23a94ddd' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.514220 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_standardCost.d3f58be9a3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.514686 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_listPrice.4ee58b9e3f' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.515110 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_sellStartDate.b44c8ea118' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.515511 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderDate.6f6f720ec3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.515956 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_customerID.60b0993af5' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.516363 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_subTotal.bfeb62a487' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.516867 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_taxAmt.94cff67d6a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.517444 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_freight.ca13e04131' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.517912 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_totalDue.920571e023' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m11:47:13.518300 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_customers_customer_sk.22a014df62' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:47:13.518741 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customer_sk.8ae5836863' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:47:13.519164 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customerid.209fbdda85' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:47:13.519571 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_AddressId.86b771f63e' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m11:47:13.519960 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_products_product_sk.8f20ac7c5b' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:47:13.520329 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_sk.2a2df3e1b9' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:47:13.520684 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_name.991aec73f3' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:47:13.521035 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_sellstartdate.f97a265a0f' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m11:47:13.620408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3c14821b0>]}
[0m11:47:13.716818 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:47:13.718661 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:47:13.734535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3c01617f0>]}
[0m11:47:13.735029 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:47:13.735395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e4eb6990>]}
[0m11:47:13.737299 [info ] [MainThread]: 
[0m11:47:13.737707 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:47:13.738039 [info ] [MainThread]: 
[0m11:47:13.738662 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140478379577696, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14269, 140479385194624), compute-name=) - Creating connection
[0m11:47:13.739010 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:47:13.739335 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140478379577696, session-id=None, name=master, idle-time=4.76837158203125e-06s, acquire-count=1, language=None, thread-identifier=(14269, 140479385194624), compute-name=) - Acquired connection on thread (14269, 140479385194624), using default compute resource
[0m11:47:13.745558 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Creating connection
[0m11:47:13.746219 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m11:47:13.746692 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=None, name=list_hive_metastore_saleslt, idle-time=5.0067901611328125e-06s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource
[0m11:47:13.747093 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.00044226646423339844s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:13.747445 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0007996559143066406s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:13.747774 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:47:13.748060 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:47:13.748390 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:14.890831 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=1.5974044799804688e-05s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Connection created
[0m11:47:14.892989 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=Unknown) - Created cursor
[0m11:47:15.442003 [debug] [ThreadPool]: SQL status: OK in 1.690 seconds
[0m11:47:15.444237 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=d3ee19c1-9949-41c8-bd12-bd2194215969) - Closing cursor
[0m11:47:15.456240 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=0.5659229755401611s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:15.456721 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=0.5664422512054443s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:15.457169 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=0.5668697357177734s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:15.457568 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=0.5672965049743652s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:15.457943 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:47:15.458285 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:47:15.458665 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:47:15.459098 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=Unknown) - Created cursor
[0m11:47:15.848130 [debug] [ThreadPool]: SQL status: OK in 0.390 seconds
[0m11:47:15.883060 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=f095eff3-f5a7-4d1f-9e9b-4a26fac2efb7) - Closing cursor
[0m11:47:15.895368 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=1.0049798488616943s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:15.896125 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=1.00579833984375s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:15.896688 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:47:15.897292 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:47:15.897913 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=Unknown) - Created cursor
[0m11:47:16.247745 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m11:47:16.252098 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=f4a9015e-baae-457f-b58d-0f6ff9126109) - Closing cursor
[0m11:47:16.253398 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=5.7220458984375e-06s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:16.254531 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_saleslt, idle-time=0.0011591911315917969s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:16.258778 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m11:47:16.259486 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.006112575531005859s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m11:47:16.260183 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.006808757781982422s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource
[0m11:47:16.260921 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.007558107376098633s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:16.261561 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.008195877075195312s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:16.262129 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:47:16.262760 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:47:16.263385 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=Unknown) - Created cursor
[0m11:47:16.581154 [debug] [ThreadPool]: SQL status: OK in 0.320 seconds
[0m11:47:16.584494 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=c8418caf-6d4c-4aab-8f8c-75f7d94310a9) - Closing cursor
[0m11:47:16.589079 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.33568668365478516s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:16.589865 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.3364732265472412s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:16.590458 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:47:16.591076 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:47:16.591774 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=Unknown) - Created cursor
[0m11:47:16.885248 [debug] [ThreadPool]: SQL status: OK in 0.290 seconds
[0m11:47:16.893338 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=241a9568-04b4-452a-83d1-26f8d7ad5ab3) - Closing cursor
[0m11:47:16.901626 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.6481211185455322s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:16.902910 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.6494441032409668s, acquire-count=1, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Retrieving connection
[0m11:47:16.903814 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:47:16.904685 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:47:16.905518 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=Unknown) - Created cursor
[0m11:47:17.325934 [debug] [ThreadPool]: SQL status: OK in 0.420 seconds
[0m11:47:17.330764 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, command-id=560e166a-caad-4e56-8be3-40ae83e2cbdf) - Closing cursor
[0m11:47:17.332262 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=7.3909759521484375e-06s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.335675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4c41dfb-a91a-4f77-9e7b-4a2a59dc9178', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3c01e2600>]}
[0m11:47:17.336564 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140478379577696, session-id=None, name=master, idle-time=5.7220458984375e-06s, acquire-count=0, language=None, thread-identifier=(14269, 140479385194624), compute-name=) - Released connection
[0m11:47:17.346306 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.address_snapshot
[0m11:47:17.347254 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=list_hive_metastore_snapshots, idle-time=0.01502847671508789s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.347719 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now snapshot.medallion_spark_dbt.address_snapshot)
[0m11:47:17.348253 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.016043901443481445s, acquire-count=0, language=None, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m11:47:17.348822 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.016607046127319336s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m11:47:17.349333 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.address_snapshot
[0m11:47:17.359303 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.address_snapshot
[0m11:47:17.359986 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.360610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.361285 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.address_snapshot
[0m11:47:17.361789 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customer_snapshot
[0m11:47:17.362441 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.0018031597137451172s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.363020 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.address_snapshot, now snapshot.medallion_spark_dbt.customer_snapshot)
[0m11:47:17.363513 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.002905607223510742s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.address_snapshot
[0m11:47:17.363984 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.003376483917236328s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m11:47:17.364438 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customer_snapshot
[0m11:47:17.368045 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customer_snapshot
[0m11:47:17.368572 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.369157 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.369832 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customer_snapshot
[0m11:47:17.370305 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m11:47:17.371031 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.001873016357421875s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.371392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customer_snapshot, now snapshot.medallion_spark_dbt.customeraddress_snapshot)
[0m11:47:17.371815 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.0026655197143554688s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customer_snapshot
[0m11:47:17.372236 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.003086566925048828s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m11:47:17.372650 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m11:47:17.376158 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m11:47:17.376740 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.377277 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=1.9073486328125e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.377859 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m11:47:17.378287 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.product_snapshot
[0m11:47:17.379315 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.0020303726196289062s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.379739 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customeraddress_snapshot, now snapshot.medallion_spark_dbt.product_snapshot)
[0m11:47:17.380235 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.0029621124267578125s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m11:47:17.380661 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.0033915042877197266s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m11:47:17.381039 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.product_snapshot
[0m11:47:17.384572 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.product_snapshot
[0m11:47:17.385401 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=6.198883056640625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.386310 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.386928 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.product_snapshot
[0m11:47:17.387380 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m11:47:17.388082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.0018541812896728516s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.388461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.product_snapshot, now snapshot.medallion_spark_dbt.productmodel_snapshot)
[0m11:47:17.388899 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0026733875274658203s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.product_snapshot
[0m11:47:17.389338 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0031232833862304688s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m11:47:17.389786 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m11:47:17.393104 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m11:47:17.393622 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.394140 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.394718 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m11:47:17.395147 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m11:47:17.395695 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0015566349029541016s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.396201 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.productmodel_snapshot, now snapshot.medallion_spark_dbt.salesorderdetail_snapshot)
[0m11:47:17.396820 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.0026721954345703125s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.productmodel_snapshot
[0m11:47:17.397261 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.0031213760375976562s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m11:47:17.397666 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m11:47:17.400899 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m11:47:17.401391 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.401919 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.402507 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m11:47:17.402949 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m11:47:17.403775 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.0018668174743652344s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.404139 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderdetail_snapshot, now snapshot.medallion_spark_dbt.salesorderheader_snapshot)
[0m11:47:17.404566 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.0026750564575195312s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m11:47:17.404985 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.003091573715209961s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m11:47:17.405372 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m11:47:17.408631 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m11:47:17.409107 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.409635 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.410246 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m11:47:17.410705 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m11:47:17.411214 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.0015900135040283203s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.411698 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderheader_snapshot, now model.medallion_spark_dbt.dim_customer)
[0m11:47:17.412110 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.0024852752685546875s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m11:47:17.412510 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.0028896331787109375s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m11:47:17.412948 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m11:47:17.416113 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m11:47:17.418860 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m11:47:17.419363 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_customer, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.419921 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_customer, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.420507 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m11:47:17.421074 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m11:47:17.421766 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.0018219947814941406s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.422168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m11:47:17.422777 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_product, idle-time=0.0028972625732421875s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m11:47:17.423297 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_product, idle-time=0.0033812522888183594s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m11:47:17.423792 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m11:47:17.427330 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m11:47:17.428204 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m11:47:17.428712 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_product, idle-time=3.814697265625e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.429294 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_product, idle-time=3.5762786865234375e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.430012 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m11:47:17.430593 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m11:47:17.431369 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.dim_product, idle-time=0.0020689964294433594s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Checking idleness
[0m11:47:17.431820 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m11:47:17.432238 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.sales, idle-time=0.0029823780059814453s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m11:47:17.432643 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.sales, idle-time=0.0033941268920898438s, acquire-count=1, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Acquired connection on thread (14269, 140478377883328), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m11:47:17.433027 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m11:47:17.438633 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m11:47:17.439826 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m11:47:17.440302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.sales, idle-time=4.291534423828125e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.440823 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140479341364912, session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f, name=model.medallion_spark_dbt.sales, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(14269, 140478377883328), compute-name=) - Released connection
[0m11:47:17.441375 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m11:47:17.442890 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:47:17.443406 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m11:47:17.443837 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m11:47:17.444356 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:47:17.444821 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m11:47:17.445231 [debug] [MainThread]: Databricks adapter: Connection(session-id=8ccf5e3b-d45a-4dd2-9e4e-3b4c9260d28f) - Closing connection
[0m11:47:17.635893 [debug] [MainThread]: Command end result
[0m11:47:17.862440 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:47:17.864416 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:47:17.871228 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:47:17.872164 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 8.550381, "process_in_blocks": "192800", "process_kernel_time": 1.032551, "process_mem_max_rss": "256952", "process_out_blocks": "4656", "process_user_time": 5.293078}
[0m11:47:17.872648 [debug] [MainThread]: Command `dbt compile` succeeded at 11:47:17.872535 after 8.55 seconds
[0m11:47:17.873052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e4b69f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e50fb050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3e4ce2060>]}
[0m11:47:17.873462 [debug] [MainThread]: Flushing usage events
[0m11:47:18.731137 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:47:26.470183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7b55dd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7b0f3a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7b75d100>]}


============================== 11:47:26.472859 | 89535f4a-9a47-4d9c-a7d1-8a449d254308 ==============================
[0m11:47:26.472859 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:47:26.473385 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:47:26.972236 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:47:26.972688 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:47:26.973030 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:47:27.652645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7cc1e270>]}
[0m11:47:27.712165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf5c5761e0>]}
[0m11:47:27.712922 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m11:47:27.797729 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:47:28.016690 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:47:28.017154 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:47:28.076111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf5c5594c0>]}
[0m11:47:28.171085 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:47:28.172611 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:47:28.186474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf5db429c0>]}
[0m11:47:28.187005 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m11:47:28.187394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf5cec6630>]}
[0m11:47:28.189299 [info ] [MainThread]: 
[0m11:47:28.189707 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:47:28.190047 [info ] [MainThread]: 
[0m11:47:28.190620 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Creating connection
[0m11:47:28.190972 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:47:28.191294 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=4.0531158447265625e-06s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Acquired connection on thread (14357, 140529161035904), using default compute resource
[0m11:47:28.198271 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Creating connection
[0m11:47:28.198916 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:47:28.199338 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=None, name=list_hive_metastore, idle-time=5.9604644775390625e-06s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Acquired connection on thread (14357, 140528509253312), using default compute resource
[0m11:47:28.199804 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=None, name=list_hive_metastore, idle-time=0.0004813671112060547s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:28.200200 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=None, name=list_hive_metastore, idle-time=0.000911712646484375s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:28.200516 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:47:28.200838 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:47:28.201140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:29.397604 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore, idle-time=1.3113021850585938e-05s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Connection created
[0m11:47:29.399776 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:29.728610 [debug] [ThreadPool]: SQL status: OK in 1.530 seconds
[0m11:47:29.731745 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=be74efd5-710d-4595-a850-30ff15969bd7) - Closing cursor
[0m11:47:29.732826 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore, idle-time=7.3909759521484375e-06s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:29.738038 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore, idle-time=0.005154609680175781s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:29.739057 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_saleslt)
[0m11:47:29.739812 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.007041215896606445s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Reusing connection previously named list_hive_metastore
[0m11:47:29.740585 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.007817983627319336s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Acquired connection on thread (14357, 140528509253312), using default compute resource
[0m11:47:29.741304 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.008571863174438477s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:29.741875 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.00914144515991211s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:29.742375 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:47:29.742944 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:47:29.743549 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:30.116070 [debug] [ThreadPool]: SQL status: OK in 0.370 seconds
[0m11:47:30.119438 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=514665b3-fd8d-40bc-8626-f0f1cf724bf3) - Closing cursor
[0m11:47:30.137775 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.4049561023712158s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:30.138453 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.40574049949645996s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:30.138950 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.40624380111694336s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:30.139394 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.4066910743713379s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:30.139830 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:47:30.140214 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:47:30.140652 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:47:30.141140 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:30.475059 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m11:47:30.484439 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=fb6548b5-d48c-49cb-9010-b471da7b247d) - Closing cursor
[0m11:47:30.495578 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.7627947330474854s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:30.496318 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.7635636329650879s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:30.497048 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:47:30.498061 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:47:30.499072 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:30.933725 [debug] [ThreadPool]: SQL status: OK in 0.430 seconds
[0m11:47:30.942780 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=c75c44b5-1702-4094-888c-eccbb4abbb97) - Closing cursor
[0m11:47:30.944979 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=9.775161743164062e-06s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:30.946892 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_saleslt, idle-time=0.0019521713256835938s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:30.953294 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m11:47:30.956520 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.011581182479858398s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m11:47:30.957516 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.01263570785522461s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Acquired connection on thread (14357, 140528509253312), using default compute resource
[0m11:47:30.958438 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.013568639755249023s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:30.959336 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.014461755752563477s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:30.960187 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:47:30.960964 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:47:30.961853 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:31.319597 [debug] [ThreadPool]: SQL status: OK in 0.360 seconds
[0m11:47:31.321686 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=83d17c09-c83b-4604-8b45-aaa8e86ae183) - Closing cursor
[0m11:47:31.324864 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.38004207611083984s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:31.325504 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.3806734085083008s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:31.326036 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:47:31.326490 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:47:31.326967 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:31.671832 [debug] [ThreadPool]: SQL status: OK in 0.340 seconds
[0m11:47:31.679343 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=26d4e939-a611-453d-8d00-c1d14e46c4e1) - Closing cursor
[0m11:47:31.687080 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.7421824932098389s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:31.688046 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.7431752681732178s, acquire-count=1, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:31.688827 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:47:31.689643 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:47:31.690431 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:32.854476 [debug] [ThreadPool]: SQL status: OK in 1.160 seconds
[0m11:47:32.860902 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=a869511b-244f-4ab3-b0ca-721670ded49d) - Closing cursor
[0m11:47:32.863632 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=1.5020370483398438e-05s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:32.868851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7ca9ee70>]}
[0m11:47:32.869997 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=4.678625106811523s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Checking idleness
[0m11:47:32.870747 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=4.679382562637329s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Retrieving connection
[0m11:47:32.871445 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=4.680087089538574s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Checking idleness
[0m11:47:32.872108 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=4.680751800537109s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Retrieving connection
[0m11:47:32.872785 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:47:32.873403 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:47:32.874133 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=5.0067901611328125e-06s, acquire-count=0, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Released connection
[0m11:47:32.879677 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m11:47:32.880751 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m11:47:32.881921 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=list_hive_metastore_snapshots, idle-time=0.018436193466186523s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:32.882923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now model.medallion_spark_dbt.dim_customer)
[0m11:47:32.883728 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=0.020298480987548828s, acquire-count=0, language=None, thread-identifier=(14357, 140528509253312), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m11:47:32.884616 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=0.02118206024169922s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Acquired connection on thread (14357, 140528509253312), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m11:47:32.885333 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m11:47:32.897176 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m11:47:32.897956 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m11:47:32.911055 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:47:32.916690 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=0.05333518981933594s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:32.917186 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=0.05386543273925781s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:32.917541 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m11:47:32.917992 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m11:47:32.918518 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:33.371999 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Closing cursor
[0m11:47:33.374693 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=4e5aa90a-50d0-4c75-a7e8-42e9b4c17f52
[0m11:47:33.377749 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:47:33.380195 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=9.775161743164062e-06s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:33.415938 [debug] [Thread-1 (]: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:47:33.416914 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:33.420641 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7baec5f0>]}
[0m11:47:33.421770 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.54s]
[0m11:47:33.422768 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m11:47:33.423353 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m11:47:33.424054 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table..
[0m11:47:33.424789 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m11:47:33.426689 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_customer, idle-time=0.009819269180297852s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:33.427148 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m11:47:33.427671 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=0.010848760604858398s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m11:47:33.428130 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=0.011323690414428711s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Acquired connection on thread (14357, 140528509253312), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m11:47:33.428599 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m11:47:33.432903 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m11:47:33.433606 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m11:47:33.435770 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:47:33.438017 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=0.02119731903076172s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:33.438517 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=0.02168416976928711s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:33.438873 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m11:47:33.439298 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m11:47:33.439757 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:34.076611 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Closing cursor
[0m11:47:34.079825 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=cb008229-9b9f-4cb1-86ee-23b1ae39e606
[0m11:47:34.081962 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:47:34.083615 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=9.5367431640625e-06s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:34.091735 [debug] [Thread-1 (]: Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:47:34.092593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=5.245208740234375e-06s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:34.093472 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf5c3beab0>]}
[0m11:47:34.094647 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.67s]
[0m11:47:34.095659 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m11:47:34.096347 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m11:47:34.097222 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_product' to be skipped because of status 'error'.  Reason: Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table..
[0m11:47:34.098142 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m11:47:34.099525 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.dim_product, idle-time=0.0069446563720703125s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:34.099984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m11:47:34.100515 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.sales, idle-time=0.00800466537475586s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m11:47:34.101090 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.sales, idle-time=0.008579015731811523s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Acquired connection on thread (14357, 140528509253312), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m11:47:34.101632 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m11:47:34.106961 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m11:47:34.107727 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m11:47:34.110105 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m11:47:34.114601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.sales, idle-time=0.022079944610595703s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Checking idleness
[0m11:47:34.115080 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.sales, idle-time=0.022617101669311523s, acquire-count=1, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Retrieving connection
[0m11:47:34.115402 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m11:47:34.115780 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
[0m11:47:34.116178 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Created cursor
[0m11:47:34.527229 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, command-id=Unknown) - Closing cursor
[0m11:47:34.530179 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=0cfd34fd-021d-4339-8c0d-c23412f76997
[0m11:47:34.533765 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:47:34.536115 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.sales, idle-time=9.5367431640625e-06s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:34.545398 [debug] [Thread-1 (]: Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:47:34.546282 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140528511246192, session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd, name=model.medallion_spark_dbt.sales, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(14357, 140528509253312), compute-name=) - Released connection
[0m11:47:34.547121 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89535f4a-9a47-4d9c-a7d1-8a449d254308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf54573fb0>]}
[0m11:47:34.548213 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.45s]
[0m11:47:34.549400 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m11:47:34.550534 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.sales' to be skipped because of status 'error'.  Reason: Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table..
[0m11:47:34.553024 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=1.678870439529419s, acquire-count=0, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Checking idleness
[0m11:47:34.553800 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=1.6796371936798096s, acquire-count=0, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Reusing connection previously named master
[0m11:47:34.554696 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=1.6805598735809326s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Acquired connection on thread (14357, 140529161035904), using default compute resource
[0m11:47:34.555414 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=1.6812903881072998s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Checking idleness
[0m11:47:34.556117 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=None, name=master, idle-time=1.681992530822754s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Retrieving connection
[0m11:47:34.556801 [debug] [MainThread]: On master: ROLLBACK
[0m11:47:34.557541 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:47:35.703792 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=948ce351-48e3-454d-a512-de947b876fc4, name=master, idle-time=7.152557373046875e-06s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Connection created
[0m11:47:35.704580 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:47:35.705186 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=948ce351-48e3-454d-a512-de947b876fc4, name=master, idle-time=0.0014729499816894531s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Checking idleness
[0m11:47:35.705763 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=948ce351-48e3-454d-a512-de947b876fc4, name=master, idle-time=0.0020487308502197266s, acquire-count=1, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Retrieving connection
[0m11:47:35.706254 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:47:35.706696 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:47:35.707180 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140528582545184, session-id=948ce351-48e3-454d-a512-de947b876fc4, name=master, idle-time=2.86102294921875e-06s, acquire-count=0, language=None, thread-identifier=(14357, 140529161035904), compute-name=) - Released connection
[0m11:47:35.707764 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:47:35.708258 [debug] [MainThread]: On master: ROLLBACK
[0m11:47:35.708890 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:47:35.709403 [debug] [MainThread]: On master: Close
[0m11:47:35.710080 [debug] [MainThread]: Databricks adapter: Connection(session-id=948ce351-48e3-454d-a512-de947b876fc4) - Closing connection
[0m11:47:36.008788 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m11:47:36.010784 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m11:47:36.012590 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:47:36.014500 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m11:47:36.016371 [debug] [MainThread]: Databricks adapter: Connection(session-id=51f99932-54c3-4cf5-92e9-d38796ff55cd) - Closing connection
[0m11:47:36.338360 [info ] [MainThread]: 
[0m11:47:36.339329 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 8.15 seconds (8.15s).
[0m11:47:36.341266 [debug] [MainThread]: Command end result
[0m11:47:36.380489 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m11:47:36.381958 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m11:47:36.386664 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m11:47:36.387011 [info ] [MainThread]: 
[0m11:47:36.387372 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:47:36.387699 [info ] [MainThread]: 
[0m11:47:36.388089 [error] [MainThread]:   Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/customers/dim_customer doesn't exist, or is not a Delta table.
[0m11:47:36.388407 [info ] [MainThread]: 
[0m11:47:36.388763 [error] [MainThread]:   Database Error in model dim_product (models/marts/product/dim_product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/products/dim_product doesn't exist, or is not a Delta table.
[0m11:47:36.389076 [info ] [MainThread]: 
[0m11:47:36.389429 [error] [MainThread]:   Database Error in model sales (models/marts/sales/sales.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/gold/sales/sales doesn't exist, or is not a Delta table.
[0m11:47:36.389753 [info ] [MainThread]: 
[0m11:47:36.390090 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
[0m11:47:36.390853 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.97902, "process_in_blocks": "728", "process_kernel_time": 0.867574, "process_mem_max_rss": "245340", "process_out_blocks": "3232", "process_user_time": 3.715704}
[0m11:47:36.391256 [debug] [MainThread]: Command `dbt run` failed at 11:47:36.391160 after 9.98 seconds
[0m11:47:36.391609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7b7b9580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7b5c3aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf7c9ce6c0>]}
[0m11:47:36.391973 [debug] [MainThread]: Flushing usage events
[0m11:47:37.689766 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:07:22.223046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17e43683b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17e3fd0770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17e3d1ccb0>]}


============================== 14:07:22.230718 | e6606105-3ebb-4e20-84a7-5209cba5817c ==============================
[0m14:07:22.230718 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:07:22.231541 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:07:22.261327 [info ] [MainThread]: dbt version: 1.9.2
[0m14:07:22.261835 [info ] [MainThread]: python version: 3.12.3
[0m14:07:22.262237 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m14:07:22.262599 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m14:07:22.999282 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:07:22.999745 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:07:23.000070 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:07:24.367434 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m14:07:24.367959 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m14:07:24.368356 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m14:07:24.368728 [info ] [MainThread]: adapter type: databricks
[0m14:07:24.369073 [info ] [MainThread]: adapter version: 1.9.4
[0m14:07:24.457237 [info ] [MainThread]: Configuration:
[0m14:07:24.457739 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:07:24.458092 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:07:24.458428 [info ] [MainThread]: Required dependencies:
[0m14:07:24.458815 [debug] [MainThread]: Executing "git --help"
[0m14:07:24.462468 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:07:24.462927 [debug] [MainThread]: STDERR: "b''"
[0m14:07:24.463255 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:07:24.463594 [info ] [MainThread]: Connection:
[0m14:07:24.463929 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m14:07:24.464267 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m14:07:24.464558 [info ] [MainThread]:   catalog: hive_metastore
[0m14:07:24.464903 [info ] [MainThread]:   schema: saleslt
[0m14:07:24.465432 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m14:07:24.567988 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139740065412688, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14814, 139740644536448), compute-name=) - Creating connection
[0m14:07:24.568534 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:07:24.568952 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139740065412688, session-id=None, name=debug, idle-time=5.7220458984375e-06s, acquire-count=1, language=None, thread-identifier=(14814, 139740644536448), compute-name=) - Acquired connection on thread (14814, 139740644536448), using default compute resource
[0m14:07:24.569411 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139740065412688, session-id=None, name=debug, idle-time=0.00047516822814941406s, acquire-count=1, language=None, thread-identifier=(14814, 139740644536448), compute-name=) - Checking idleness
[0m14:07:24.569812 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139740065412688, session-id=None, name=debug, idle-time=0.0008809566497802734s, acquire-count=1, language=None, thread-identifier=(14814, 139740644536448), compute-name=) - Retrieving connection
[0m14:07:24.570168 [debug] [MainThread]: Using databricks connection "debug"
[0m14:07:24.570565 [debug] [MainThread]: On debug: select 1 as id
[0m14:07:24.570922 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:07:26.313445 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139740065412688, session-id=1556b59a-6dba-412f-879a-d63746352e74, name=debug, idle-time=7.3909759521484375e-06s, acquire-count=1, language=None, thread-identifier=(14814, 139740644536448), compute-name=) - Connection created
[0m14:07:26.314366 [debug] [MainThread]: Databricks adapter: Cursor(session-id=1556b59a-6dba-412f-879a-d63746352e74, command-id=Unknown) - Created cursor
[0m14:07:28.864125 [debug] [MainThread]: SQL status: OK in 4.290 seconds
[0m14:07:28.869317 [debug] [MainThread]: Databricks adapter: Cursor(session-id=1556b59a-6dba-412f-879a-d63746352e74, command-id=a71fc3d0-b3b8-43c3-90f4-4fffffc27807) - Closing cursor
[0m14:07:28.874468 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139740065412688, session-id=1556b59a-6dba-412f-879a-d63746352e74, name=debug, idle-time=1.1682510375976562e-05s, acquire-count=0, language=None, thread-identifier=(14814, 139740644536448), compute-name=) - Released connection
[0m14:07:28.875478 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:07:28.876440 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:07:28.880234 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 6.7201138, "process_in_blocks": "234896", "process_kernel_time": 1.100771, "process_mem_max_rss": "240028", "process_out_blocks": "24", "process_user_time": 2.969053}
[0m14:07:28.881215 [debug] [MainThread]: Command `dbt debug` succeeded at 14:07:28.881006 after 6.72 seconds
[0m14:07:28.881934 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:07:28.882592 [debug] [MainThread]: On debug: Close
[0m14:07:28.883434 [debug] [MainThread]: Databricks adapter: Connection(session-id=1556b59a-6dba-412f-879a-d63746352e74) - Closing connection
[0m14:07:29.346070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17e434c440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17e3c17860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17e4b4a990>]}
[0m14:07:29.347376 [debug] [MainThread]: Flushing usage events
[0m14:07:30.449861 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:07:40.005752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ee038920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ef8c2570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ee715ac0>]}


============================== 14:07:40.008954 | b2e6a57e-425c-4112-a2ca-2f44d2d8d53c ==============================
[0m14:07:40.008954 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:07:40.009517 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:07:40.543578 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:07:40.544202 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:07:40.544638 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:07:41.264751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ee5a1b20>]}
[0m14:07:41.320193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d0e70500>]}
[0m14:07:41.320845 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m14:07:41.411952 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m14:07:41.687973 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:07:41.688348 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:07:41.741593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ef58efc0>]}
[0m14:07:41.840239 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:07:41.845040 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:07:41.863789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10cac9fb30>]}
[0m14:07:41.864330 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m14:07:41.864701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10cac6cbf0>]}
[0m14:07:41.866480 [info ] [MainThread]: 
[0m14:07:41.866850 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:07:41.867193 [info ] [MainThread]: 
[0m14:07:41.867787 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Creating connection
[0m14:07:41.868155 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:07:41.868481 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=3.814697265625e-06s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Acquired connection on thread (14897, 139710746067072), using default compute resource
[0m14:07:41.874124 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Creating connection
[0m14:07:41.874692 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m14:07:41.875222 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=None, name=list_hive_metastore, idle-time=5.245208740234375e-06s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource
[0m14:07:41.875746 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=None, name=list_hive_metastore, idle-time=0.000576019287109375s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:41.876279 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=None, name=list_hive_metastore, idle-time=0.001087188720703125s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:41.876840 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:07:41.877579 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m14:07:41.878168 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:07:43.617838 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore, idle-time=1.2636184692382812e-05s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Connection created
[0m14:07:43.620247 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:44.110364 [debug] [ThreadPool]: SQL status: OK in 2.230 seconds
[0m14:07:44.112312 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=fb81201a-85fa-4c09-aba9-5affa2765df6) - Closing cursor
[0m14:07:44.112985 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore, idle-time=4.5299530029296875e-06s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:44.114063 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore, idle-time=0.001092672348022461s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:44.114550 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_saleslt)
[0m14:07:44.115038 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.0020842552185058594s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Reusing connection previously named list_hive_metastore
[0m14:07:44.115516 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.002564668655395508s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource
[0m14:07:44.116130 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.0031805038452148438s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:44.116621 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.0036706924438476562s, acquire-count=2, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource
[0m14:07:44.117126 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "saleslt"
"
[0m14:07:44.130410 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.017423391342163086s, acquire-count=2, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:44.130980 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.018044233322143555s, acquire-count=2, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:44.131382 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.01845574378967285s, acquire-count=2, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:44.131785 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.018861770629882812s, acquire-count=2, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:44.132139 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:44.132466 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_saleslt"
[0m14:07:44.132874 [debug] [ThreadPool]: On create_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "create_hive_metastore_saleslt"} */
create schema if not exists `hive_metastore`.`saleslt`
  
[0m14:07:44.133279 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:45.052643 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m14:07:45.053701 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=9fbb9b35-e136-40cc-9913-00bf631a8025) - Closing cursor
[0m14:07:45.054169 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:07:45.054541 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.941617488861084s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:45.055047 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=2.6226043701171875e-06s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:45.056927 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=create_hive_metastore_saleslt, idle-time=0.0018520355224609375s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:45.057505 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_saleslt, now list_hive_metastore_saleslt)
[0m14:07:45.057905 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_saleslt, idle-time=0.0028548240661621094s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Reusing connection previously named create_hive_metastore_saleslt
[0m14:07:45.058268 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_saleslt, idle-time=0.003220796585083008s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource
[0m14:07:45.058736 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_saleslt, idle-time=0.003691434860229492s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:45.059094 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_saleslt, idle-time=0.004050731658935547s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:45.059424 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m14:07:45.059767 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m14:07:45.060109 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:45.520493 [debug] [ThreadPool]: SQL status: OK in 0.460 seconds
[0m14:07:45.522474 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=59090195-ab3d-441f-baec-423de7c371cf) - Closing cursor
[0m14:07:45.523103 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_saleslt, idle-time=4.76837158203125e-06s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:45.523914 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_saleslt, idle-time=0.0008180141448974609s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:45.524572 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m14:07:45.525200 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_snapshots, idle-time=0.0020720958709716797s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m14:07:45.525764 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_snapshots, idle-time=0.002651691436767578s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource
[0m14:07:45.526379 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_snapshots, idle-time=0.0032491683959960938s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:45.526903 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_snapshots, idle-time=0.0038001537322998047s, acquire-count=1, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:45.527379 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m14:07:45.527910 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m14:07:45.528536 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:45.861706 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m14:07:45.864663 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=01420242-13a5-4383-a7c7-59ec6b8e8c00) - Closing cursor
[0m14:07:45.866536 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_snapshots, idle-time=8.106231689453125e-06s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:45.868214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10caf84710>]}
[0m14:07:45.869135 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=4.0005903244018555s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Checking idleness
[0m14:07:45.869737 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=4.001196384429932s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Retrieving connection
[0m14:07:45.870216 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=4.001712083816528s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Checking idleness
[0m14:07:45.870722 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=4.002216815948486s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Retrieving connection
[0m14:07:45.871187 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:45.871684 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:07:45.872197 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=5.0067901611328125e-06s, acquire-count=0, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Released connection
[0m14:07:45.875165 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m14:07:45.875916 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m14:07:45.876757 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=list_hive_metastore_snapshots, idle-time=0.010586023330688477s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:45.877219 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now model.medallion_spark_dbt.dim_customer)
[0m14:07:45.877752 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.011598348617553711s, acquire-count=0, language=None, thread-identifier=(14897, 139710095947456), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m14:07:45.878330 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.012131214141845703s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m14:07:45.878849 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m14:07:45.888835 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m14:07:45.889886 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m14:07:45.903225 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m14:07:45.961183 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.dim_customer"
[0m14:07:45.968129 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.1019284725189209s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:45.968697 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.10257887840270996s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:45.969083 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_customer"
[0m14:07:45.969595 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m14:07:45.970119 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:47.091311 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Closing cursor
[0m14:07:47.092066 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`customer_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 43 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`customer_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 43 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`customer_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 43 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=2f5d56a9-448a-4f65-9e13-48a6e6e6d396
[0m14:07:47.092899 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:47.104556 [debug] [Thread-1 (]: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`customer_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 43 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/customer/dim_customer.sql
[0m14:07:47.105207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=4.5299530029296875e-06s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:47.107175 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ee5649e0>]}
[0m14:07:47.107917 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 1.23s]
[0m14:07:47.108728 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m14:07:47.109221 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m14:07:47.109812 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`customer_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 43 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/customer/dim_customer.sql.
[0m14:07:47.110491 [info ] [Thread-1 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m14:07:47.112046 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_customer, idle-time=0.006810188293457031s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:47.112461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m14:07:47.112938 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=0.007736682891845703s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m14:07:47.113367 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=0.008179664611816406s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m14:07:47.113836 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m14:07:47.117590 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m14:07:47.119628 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m14:07:47.122103 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m14:07:47.124125 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.dim_product"
[0m14:07:47.124962 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=0.019774675369262695s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:47.125326 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=0.020168781280517578s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:47.125628 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.dim_product"
[0m14:07:47.126015 [debug] [Thread-1 (]: On model.medallion_spark_dbt.dim_product: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m14:07:47.126625 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:47.921733 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Closing cursor
[0m14:07:47.922492 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`product_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 32 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`product_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 32 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`product_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 32 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=28242f9d-943e-4c43-a739-22c2eedaba6d
[0m14:07:47.923210 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:47.925530 [debug] [Thread-1 (]: Database Error in model dim_product (models/marts/product/dim_product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`product_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 32 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/product/dim_product.sql
[0m14:07:47.926015 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:47.926483 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10caa57530>]}
[0m14:07:47.927078 [error] [Thread-1 (]: 2 of 3 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.81s]
[0m14:07:47.927767 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m14:07:47.928389 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m14:07:47.929003 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.dim_product' to be skipped because of status 'error'.  Reason: Database Error in model dim_product (models/marts/product/dim_product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`product_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 32 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/product/dim_product.sql.
[0m14:07:47.929579 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m14:07:47.930517 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.dim_product, idle-time=0.004492044448852539s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:47.930844 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m14:07:47.931256 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.sales, idle-time=0.005251646041870117s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m14:07:47.931633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.sales, idle-time=0.005638599395751953s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Acquired connection on thread (14897, 139710095947456), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m14:07:47.932004 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m14:07:47.935460 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m14:07:47.936232 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m14:07:47.938911 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m14:07:47.941306 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_spark_dbt.sales"
[0m14:07:47.943106 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.sales, idle-time=0.01706671714782715s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Checking idleness
[0m14:07:47.943520 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.sales, idle-time=0.01752448081970215s, acquire-count=1, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Retrieving connection
[0m14:07:47.943797 [debug] [Thread-1 (]: Using databricks connection "model.medallion_spark_dbt.sales"
[0m14:07:47.944238 [debug] [Thread-1 (]: On model.medallion_spark_dbt.sales: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m14:07:47.944681 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Created cursor
[0m14:07:48.738833 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b70db027-8d02-4326-8a03-02a22ca0116f, command-id=Unknown) - Closing cursor
[0m14:07:48.743053 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "model.medallion_spark_dbt.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 28 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 28 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 28 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=8ea00417-54d8-43a2-8782-abeb14832402
[0m14:07:48.747126 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.sales, idle-time=1.1682510375976562e-05s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:48.756563 [debug] [Thread-1 (]: Database Error in model sales (models/marts/sales/sales.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 28 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/sales/sales.sql
[0m14:07:48.757946 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=139710097777264, session-id=b70db027-8d02-4326-8a03-02a22ca0116f, name=model.medallion_spark_dbt.sales, idle-time=9.5367431640625e-06s, acquire-count=0, language=sql, thread-identifier=(14897, 139710095947456), compute-name=) - Released connection
[0m14:07:48.759136 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2e6a57e-425c-4112-a2ca-2f44d2d8d53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10c8953f80>]}
[0m14:07:48.760491 [error] [Thread-1 (]: 3 of 3 ERROR creating sql table model saleslt.sales ............................ [[31mERROR[0m in 0.83s]
[0m14:07:48.762007 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m14:07:48.763109 [debug] [Thread-4 (]: Marking all children of 'model.medallion_spark_dbt.sales' to be skipped because of status 'error'.  Reason: Database Error in model sales (models/marts/sales/sales.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 28 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/sales/sales.sql.
[0m14:07:48.765577 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=2.8933229446411133s, acquire-count=0, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Checking idleness
[0m14:07:48.766365 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=2.8941333293914795s, acquire-count=0, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Reusing connection previously named master
[0m14:07:48.767093 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=2.894855260848999s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Acquired connection on thread (14897, 139710746067072), using default compute resource
[0m14:07:48.767683 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=2.8954808712005615s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Checking idleness
[0m14:07:48.768265 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=None, name=master, idle-time=2.8960633277893066s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Retrieving connection
[0m14:07:48.768857 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:48.769405 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:07:49.685314 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=6fa3f039-3434-481a-935a-c26d0014b671, name=master, idle-time=7.152557373046875e-06s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Connection created
[0m14:07:49.686127 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:49.686809 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=6fa3f039-3434-481a-935a-c26d0014b671, name=master, idle-time=0.0016658306121826172s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Checking idleness
[0m14:07:49.687415 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=6fa3f039-3434-481a-935a-c26d0014b671, name=master, idle-time=0.002296924591064453s, acquire-count=1, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Retrieving connection
[0m14:07:49.687946 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:07:49.688510 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:07:49.689078 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=139710101233952, session-id=6fa3f039-3434-481a-935a-c26d0014b671, name=master, idle-time=3.814697265625e-06s, acquire-count=0, language=None, thread-identifier=(14897, 139710746067072), compute-name=) - Released connection
[0m14:07:49.689777 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:07:49.690278 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:49.690782 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:49.691313 [debug] [MainThread]: On master: Close
[0m14:07:49.691946 [debug] [MainThread]: Databricks adapter: Connection(session-id=6fa3f039-3434-481a-935a-c26d0014b671) - Closing connection
[0m14:07:49.903075 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m14:07:49.905649 [debug] [MainThread]: On model.medallion_spark_dbt.sales: ROLLBACK
[0m14:07:49.907964 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:07:49.910006 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m14:07:49.911660 [debug] [MainThread]: Databricks adapter: Connection(session-id=b70db027-8d02-4326-8a03-02a22ca0116f) - Closing connection
[0m14:07:50.257191 [info ] [MainThread]: 
[0m14:07:50.258159 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 8.39 seconds (8.39s).
[0m14:07:50.260259 [debug] [MainThread]: Command end result
[0m14:07:50.301436 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:07:50.303905 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:07:50.309283 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m14:07:50.309720 [info ] [MainThread]: 
[0m14:07:50.310152 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:07:50.310563 [info ] [MainThread]: 
[0m14:07:50.311011 [error] [MainThread]:   Database Error in model dim_customer (models/marts/customer/dim_customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`customer_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 43 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/customer/dim_customer.sql
[0m14:07:50.311408 [info ] [MainThread]: 
[0m14:07:50.311831 [error] [MainThread]:   Database Error in model dim_product (models/marts/product/dim_product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`product_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 32 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/product/dim_product.sql
[0m14:07:50.312237 [info ] [MainThread]: 
[0m14:07:50.312649 [error] [MainThread]:   Database Error in model sales (models/marts/sales/sales.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 28 pos 9
  compiled code at target/run/medallion_spark_dbt/models/marts/sales/sales.sql
[0m14:07:50.313011 [info ] [MainThread]: 
[0m14:07:50.313389 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
[0m14:07:50.314210 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 10.3644085, "process_in_blocks": "6376", "process_kernel_time": 0.934983, "process_mem_max_rss": "247392", "process_out_blocks": "3280", "process_user_time": 3.451481}
[0m14:07:50.314632 [debug] [MainThread]: Command `dbt run` failed at 14:07:50.314535 after 10.36 seconds
[0m14:07:50.314985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ee715ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ee717740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10ec1b4620>]}
[0m14:07:50.315347 [debug] [MainThread]: Flushing usage events
[0m14:07:51.496160 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:08:21.101897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7a4b18d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7a6812810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7a697eae0>]}


============================== 14:08:21.104761 | 417f5cf9-2950-46d3-9b39-ef0ed19b6821 ==============================
[0m14:08:21.104761 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:08:21.105296 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m14:08:21.610055 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:08:21.610493 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:08:21.610807 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:08:22.310797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7a551ae70>]}
[0m14:08:22.377992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7821fe1e0>]}
[0m14:08:22.378753 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m14:08:22.487634 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m14:08:22.721430 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:08:22.721955 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:08:22.817161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff781ebd490>]}
[0m14:08:22.979063 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:08:22.982392 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:08:23.005613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff781bbbc20>]}
[0m14:08:23.006575 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m14:08:23.007428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff781d94890>]}
[0m14:08:23.010264 [info ] [MainThread]: 
[0m14:08:23.010832 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:08:23.011342 [info ] [MainThread]: 
[0m14:08:23.012161 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Creating connection
[0m14:08:23.012640 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:08:23.013289 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=9.298324584960938e-06s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Acquired connection on thread (14997, 140701658042496), using default compute resource
[0m14:08:23.025316 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Creating connection
[0m14:08:23.026195 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m14:08:23.026730 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=None, name=list_hive_metastore, idle-time=6.9141387939453125e-06s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource
[0m14:08:23.027240 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=None, name=list_hive_metastore, idle-time=0.0005369186401367188s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:23.027716 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=None, name=list_hive_metastore, idle-time=0.0009851455688476562s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:23.028115 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:08:23.028528 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m14:08:23.028924 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:24.206643 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore, idle-time=4.5299530029296875e-06s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Connection created
[0m14:08:24.207276 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:24.590596 [debug] [ThreadPool]: SQL status: OK in 1.560 seconds
[0m14:08:24.592007 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=35929b3d-0be5-48ec-9229-ae34bd457e84) - Closing cursor
[0m14:08:24.592469 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore, idle-time=3.5762786865234375e-06s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:24.593323 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore, idle-time=0.0008628368377685547s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:24.593735 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m14:08:24.594127 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.0016677379608154297s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named list_hive_metastore
[0m14:08:24.594514 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.0020554065704345703s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource
[0m14:08:24.594986 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.0025336742401123047s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:24.595369 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.002913236618041992s, acquire-count=2, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource
[0m14:08:24.595799 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m14:08:24.606872 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.014386177062988281s, acquire-count=2, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:24.607331 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.014872550964355469s, acquire-count=2, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:24.607748 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.015293359756469727s, acquire-count=2, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:24.608173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.015719175338745117s, acquire-count=2, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:24.608534 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:24.608959 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m14:08:24.609352 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m14:08:24.609772 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:25.091551 [debug] [ThreadPool]: SQL status: OK in 0.480 seconds
[0m14:08:25.092808 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=5d184806-12f5-42fe-bfe8-5f37b3c2b949) - Closing cursor
[0m14:08:25.093401 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:08:25.093926 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.5014393329620361s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:25.094599 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=2.6226043701171875e-06s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:25.097738 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=create_hive_metastore_snapshots, idle-time=0.0030324459075927734s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:25.098657 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m14:08:25.099369 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_saleslt, idle-time=0.004767656326293945s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named create_hive_metastore_snapshots
[0m14:08:25.099968 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_saleslt, idle-time=0.0053882598876953125s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource
[0m14:08:25.100583 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_saleslt, idle-time=0.006005048751831055s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:25.101187 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_saleslt, idle-time=0.006611824035644531s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:25.101663 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m14:08:25.102185 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m14:08:25.102733 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:25.538566 [debug] [ThreadPool]: SQL status: OK in 0.440 seconds
[0m14:08:25.539845 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=9e08e911-6f21-4e4f-a5dd-3f72091f9722) - Closing cursor
[0m14:08:25.540288 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_saleslt, idle-time=3.337860107421875e-06s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:25.540888 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_saleslt, idle-time=0.0006043910980224609s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:25.541353 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m14:08:25.541740 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_snapshots, idle-time=0.0014541149139404297s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m14:08:25.542110 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_snapshots, idle-time=0.0018253326416015625s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource
[0m14:08:25.542503 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_snapshots, idle-time=0.0022230148315429688s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:25.542866 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_snapshots, idle-time=0.0025873184204101562s, acquire-count=1, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:25.543218 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m14:08:25.543555 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m14:08:25.543925 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:25.970071 [debug] [ThreadPool]: SQL status: OK in 0.430 seconds
[0m14:08:25.971427 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=ec800614-a10f-459e-a55e-ddc7d095d5cf) - Closing cursor
[0m14:08:25.971970 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_snapshots, idle-time=3.5762786865234375e-06s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:25.972839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff781d97470>]}
[0m14:08:25.973494 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=2.9602668285369873s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Checking idleness
[0m14:08:25.973966 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=2.9607808589935303s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Retrieving connection
[0m14:08:25.974379 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=2.9612066745758057s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Checking idleness
[0m14:08:25.974751 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=2.961578130722046s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Retrieving connection
[0m14:08:25.975113 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:25.975468 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:08:25.975856 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=3.814697265625e-06s, acquire-count=0, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Released connection
[0m14:08:25.979124 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.address_snapshot
[0m14:08:25.980187 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m14:08:25.980861 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=list_hive_metastore_snapshots, idle-time=0.008835792541503906s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:25.981203 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now snapshot.medallion_spark_dbt.address_snapshot)
[0m14:08:25.981600 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.009605169296264648s, acquire-count=0, language=None, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m14:08:25.981997 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.01000356674194336s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m14:08:25.982387 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.address_snapshot
[0m14:08:25.991092 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.address_snapshot
[0m14:08:26.176086 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.20399761199951172s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:26.176728 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.20470309257507324s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:26.177162 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.address_snapshot"
[0m14:08:26.177670 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:26.178188 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:26.857416 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:26.858328 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=77e4adc8-d7cd-41ab-9f24-a507c37124ed
[0m14:08:26.859238 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:26.866008 [debug] [Thread-1 (]: Database Error in snapshot address_snapshot (snapshots/address.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:08:26.866767 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=4.76837158203125e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:26.868555 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7819aa270>]}
[0m14:08:26.869761 [error] [Thread-1 (]: 1 of 7 ERROR snapshotting snapshots.address_snapshot ........................... [[31mERROR[0m in 0.89s]
[0m14:08:26.870696 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.address_snapshot
[0m14:08:26.871276 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:08:26.871904 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.address_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot address_snapshot (snapshots/address.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9.
[0m14:08:26.872462 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m14:08:26.873997 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.007210969924926758s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:26.874401 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.address_snapshot, now snapshot.medallion_spark_dbt.customer_snapshot)
[0m14:08:26.874827 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.008093118667602539s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.address_snapshot
[0m14:08:26.875308 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.00857090950012207s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m14:08:26.875735 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:08:26.880884 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:08:26.886346 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.01950216293334961s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:26.886922 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.02017664909362793s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:26.887267 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customer_snapshot"
[0m14:08:26.887750 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:26.888218 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:27.258579 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:27.259775 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=ae01f892-a177-4fc7-a081-9c25d9ef6fbb
[0m14:08:27.261347 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:27.266983 [debug] [Thread-1 (]: Database Error in snapshot customer_snapshot (snapshots/customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
[0m14:08:27.267908 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=5.245208740234375e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:27.268984 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7800cf620>]}
[0m14:08:27.270496 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.39s]
[0m14:08:27.271644 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:08:27.272296 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:08:27.273140 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.customer_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customer_snapshot (snapshots/customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9.
[0m14:08:27.273937 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m14:08:27.275482 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.007571697235107422s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:27.276237 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customer_snapshot, now snapshot.medallion_spark_dbt.customeraddress_snapshot)
[0m14:08:27.277055 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.009164810180664062s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customer_snapshot
[0m14:08:27.277865 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.00994873046875s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m14:08:27.278739 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:08:27.284822 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:08:27.293267 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.025371551513671875s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:27.293869 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.026044130325317383s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:27.294385 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customeraddress_snapshot"
[0m14:08:27.294992 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:27.295880 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:27.659589 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:27.660365 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=384012c8-af61-4a64-ae32-f433b21cabd2
[0m14:08:27.661265 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:27.665014 [debug] [Thread-1 (]: Database Error in snapshot customeraddress_snapshot (snapshots/customeraddress.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:08:27.665733 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:27.666407 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7819b5400>]}
[0m14:08:27.667334 [error] [Thread-1 (]: 3 of 7 ERROR snapshotting snapshots.customeraddress_snapshot ................... [[31mERROR[0m in 0.39s]
[0m14:08:27.668067 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:08:27.668578 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.product_snapshot
[0m14:08:27.669268 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.customeraddress_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customeraddress_snapshot (snapshots/customeraddress.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9.
[0m14:08:27.669889 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m14:08:27.670984 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.0052337646484375s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:27.671525 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customeraddress_snapshot, now snapshot.medallion_spark_dbt.product_snapshot)
[0m14:08:27.672006 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.006354570388793945s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:08:27.672396 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.006752729415893555s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m14:08:27.672805 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.product_snapshot
[0m14:08:27.681367 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.product_snapshot
[0m14:08:27.688325 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.02261495590209961s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:27.688862 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.02320408821105957s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:27.689210 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.product_snapshot"
[0m14:08:27.689654 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:27.690191 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:28.057861 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:28.058894 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=5346e148-8ed8-43c4-ad1a-4168fc11f3ba
[0m14:08:28.059957 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=3.814697265625e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:28.064277 [debug] [Thread-1 (]: Database Error in snapshot product_snapshot (snapshots/product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
[0m14:08:28.064960 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:28.065502 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7800c3560>]}
[0m14:08:28.066438 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.39s]
[0m14:08:28.067389 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.product_snapshot
[0m14:08:28.068223 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:08:28.069118 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.product_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot product_snapshot (snapshots/product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9.
[0m14:08:28.069808 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m14:08:28.070968 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.005999326705932617s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:28.071396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.product_snapshot, now snapshot.medallion_spark_dbt.productmodel_snapshot)
[0m14:08:28.071953 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.007002592086791992s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.product_snapshot
[0m14:08:28.072412 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.007487058639526367s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m14:08:28.072861 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:08:28.077092 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:08:28.082353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.01738286018371582s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:28.082845 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.017937660217285156s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:28.083144 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.productmodel_snapshot"
[0m14:08:28.083531 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:28.083970 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:28.460483 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:28.461210 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=abc7ef6a-d9a6-4c0d-bc12-ee40a41e4e01
[0m14:08:28.462002 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:28.465407 [debug] [Thread-1 (]: Database Error in snapshot productmodel_snapshot (snapshots/productmodel.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:08:28.466082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=4.5299530029296875e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:28.466542 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff781b69490>]}
[0m14:08:28.467255 [error] [Thread-1 (]: 5 of 7 ERROR snapshotting snapshots.productmodel_snapshot ...................... [[31mERROR[0m in 0.40s]
[0m14:08:28.467854 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:08:28.468251 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:08:28.468924 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.productmodel_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot productmodel_snapshot (snapshots/productmodel.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9.
[0m14:08:28.469588 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m14:08:28.470415 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.004366397857666016s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:28.470735 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.productmodel_snapshot, now snapshot.medallion_spark_dbt.salesorderdetail_snapshot)
[0m14:08:28.471110 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.005084514617919922s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:08:28.471470 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.005445003509521484s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m14:08:28.471818 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:08:28.475261 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:08:28.481374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.015270471572875977s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:28.481864 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.0158383846282959s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:28.482174 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"
[0m14:08:28.482566 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:28.482975 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:28.858024 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:28.858744 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=9a7034d9-1d99-4c86-9e35-42ee686cbddd
[0m14:08:28.859438 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:28.862825 [debug] [Thread-1 (]: Database Error in snapshot salesorderdetail_snapshot (snapshots/salesorderdetail.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:08:28.863768 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=6.198883056640625e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:28.864595 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff780113da0>]}
[0m14:08:28.865686 [error] [Thread-1 (]: 6 of 7 ERROR snapshotting snapshots.salesorderdetail_snapshot .................. [[31mERROR[0m in 0.39s]
[0m14:08:28.866920 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:08:28.867761 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:08:28.868616 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.salesorderdetail_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderdetail_snapshot (snapshots/salesorderdetail.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9.
[0m14:08:28.869510 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m14:08:28.870593 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.006930351257324219s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:28.870981 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderdetail_snapshot, now snapshot.medallion_spark_dbt.salesorderheader_snapshot)
[0m14:08:28.871439 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.007776975631713867s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:08:28.871850 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.008196592330932617s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Acquired connection on thread (14997, 140701008021184), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m14:08:28.872284 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:08:28.876330 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:08:28.883707 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.019991397857666016s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Checking idleness
[0m14:08:28.884236 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.020594358444213867s, acquire-count=1, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Retrieving connection
[0m14:08:28.884595 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderheader_snapshot"
[0m14:08:28.885054 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:08:28.885499 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Created cursor
[0m14:08:29.258131 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, command-id=Unknown) - Closing cursor
[0m14:08:29.259099 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=d017eed8-217b-4bbb-9ba1-30a0c1b17c51
[0m14:08:29.260113 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:29.264567 [debug] [Thread-1 (]: Database Error in snapshot salesorderheader_snapshot (snapshots/salesorderheader.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
[0m14:08:29.265385 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140701009880576, session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(14997, 140701008021184), compute-name=) - Released connection
[0m14:08:29.266010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '417f5cf9-2950-46d3-9b39-ef0ed19b6821', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff780143440>]}
[0m14:08:29.267168 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.40s]
[0m14:08:29.268144 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:08:29.269102 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.salesorderheader_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderheader_snapshot (snapshots/salesorderheader.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9.
[0m14:08:29.271454 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=3.2954964637756348s, acquire-count=0, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Checking idleness
[0m14:08:29.272304 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=3.296393871307373s, acquire-count=0, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Reusing connection previously named master
[0m14:08:29.273017 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=3.2971031665802s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Acquired connection on thread (14997, 140701658042496), using default compute resource
[0m14:08:29.273863 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=3.2979073524475098s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Checking idleness
[0m14:08:29.274578 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=None, name=master, idle-time=3.2986669540405273s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Retrieving connection
[0m14:08:29.275319 [debug] [MainThread]: On master: ROLLBACK
[0m14:08:29.275946 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:08:30.397585 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=51ed4aad-099b-43da-9afb-9802c4a6444b, name=master, idle-time=1.3828277587890625e-05s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Connection created
[0m14:08:30.399548 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:08:30.401292 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=51ed4aad-099b-43da-9afb-9802c4a6444b, name=master, idle-time=0.0038518905639648438s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Checking idleness
[0m14:08:30.403100 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=51ed4aad-099b-43da-9afb-9802c4a6444b, name=master, idle-time=0.005558967590332031s, acquire-count=1, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Retrieving connection
[0m14:08:30.405907 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:30.407983 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:08:30.409486 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140701014268800, session-id=51ed4aad-099b-43da-9afb-9802c4a6444b, name=master, idle-time=7.62939453125e-06s, acquire-count=0, language=None, thread-identifier=(14997, 140701658042496), compute-name=) - Released connection
[0m14:08:30.411353 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:08:30.412408 [debug] [MainThread]: On master: ROLLBACK
[0m14:08:30.413586 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:08:30.414592 [debug] [MainThread]: On master: Close
[0m14:08:30.415663 [debug] [MainThread]: Databricks adapter: Connection(session-id=51ed4aad-099b-43da-9afb-9802c4a6444b) - Closing connection
[0m14:08:30.716773 [debug] [MainThread]: Connection 'snapshot.medallion_spark_dbt.salesorderheader_snapshot' was properly closed.
[0m14:08:30.717206 [debug] [MainThread]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: ROLLBACK
[0m14:08:30.717610 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:08:30.718040 [debug] [MainThread]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: Close
[0m14:08:30.718347 [debug] [MainThread]: Databricks adapter: Connection(session-id=d7f697d1-07e1-49c1-a9b9-c0cd0f600894) - Closing connection
[0m14:08:31.114116 [info ] [MainThread]: 
[0m14:08:31.114722 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 8.10 seconds (8.10s).
[0m14:08:31.116425 [debug] [MainThread]: Command end result
[0m14:08:31.148051 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:08:31.149542 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:08:31.154387 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m14:08:31.154745 [info ] [MainThread]: 
[0m14:08:31.155192 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m14:08:31.155559 [info ] [MainThread]: 
[0m14:08:31.155972 [error] [MainThread]:   Database Error in snapshot address_snapshot (snapshots/address.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:08:31.156373 [info ] [MainThread]: 
[0m14:08:31.156781 [error] [MainThread]:   Database Error in snapshot customer_snapshot (snapshots/customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
[0m14:08:31.157125 [info ] [MainThread]: 
[0m14:08:31.157507 [error] [MainThread]:   Database Error in snapshot customeraddress_snapshot (snapshots/customeraddress.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:08:31.157859 [info ] [MainThread]: 
[0m14:08:31.158223 [error] [MainThread]:   Database Error in snapshot product_snapshot (snapshots/product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
[0m14:08:31.158548 [info ] [MainThread]: 
[0m14:08:31.158913 [error] [MainThread]:   Database Error in snapshot productmodel_snapshot (snapshots/productmodel.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:08:31.159238 [info ] [MainThread]: 
[0m14:08:31.159622 [error] [MainThread]:   Database Error in snapshot salesorderdetail_snapshot (snapshots/salesorderdetail.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:08:31.159995 [info ] [MainThread]: 
[0m14:08:31.160426 [error] [MainThread]:   Database Error in snapshot salesorderheader_snapshot (snapshots/salesorderheader.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
[0m14:08:31.160777 [info ] [MainThread]: 
[0m14:08:31.161129 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=7 SKIP=0 TOTAL=7
[0m14:08:31.161961 [debug] [MainThread]: Resource report: {"command_name": "snapshot", "command_success": false, "command_wall_clock_time": 10.108625, "process_in_blocks": "40", "process_kernel_time": 1.133444, "process_mem_max_rss": "249428", "process_out_blocks": "3336", "process_user_time": 3.668781}
[0m14:08:31.162434 [debug] [MainThread]: Command `dbt snapshot` failed at 14:08:31.162325 after 10.11 seconds
[0m14:08:31.162905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7a84df2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7a4bf9df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff781b64140>]}
[0m14:08:31.163342 [debug] [MainThread]: Flushing usage events
[0m14:08:33.665579 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:09:41.165179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3797f8b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3797f8b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc37a317290>]}


============================== 14:09:41.173958 | 9eb326b3-c92f-45b1-9afb-b500077a6bcf ==============================
[0m14:09:41.173958 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:09:41.174769 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:09:41.208749 [info ] [MainThread]: dbt version: 1.9.2
[0m14:09:41.209320 [info ] [MainThread]: python version: 3.12.3
[0m14:09:41.209775 [info ] [MainThread]: python path: /home/stanley_one/stan/airflow_one/airflow_test_one/bin/python3
[0m14:09:41.210252 [info ] [MainThread]: os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
[0m14:09:42.122991 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:09:42.123592 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:09:42.124024 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:09:43.660900 [info ] [MainThread]: Using profiles dir at /home/stanley_one/.dbt
[0m14:09:43.661499 [info ] [MainThread]: Using profiles.yml file at /home/stanley_one/.dbt/profiles.yml
[0m14:09:43.662158 [info ] [MainThread]: Using dbt_project.yml file at /home/stanley_one/medallion_spark_dbt/dbt_project.yml
[0m14:09:43.662663 [info ] [MainThread]: adapter type: databricks
[0m14:09:43.663095 [info ] [MainThread]: adapter version: 1.9.4
[0m14:09:43.828421 [info ] [MainThread]: Configuration:
[0m14:09:43.829158 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:09:43.829685 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:09:43.830158 [info ] [MainThread]: Required dependencies:
[0m14:09:43.830662 [debug] [MainThread]: Executing "git --help"
[0m14:09:43.851472 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:09:43.852115 [debug] [MainThread]: STDERR: "b''"
[0m14:09:43.852522 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:09:43.853001 [info ] [MainThread]: Connection:
[0m14:09:43.853491 [info ] [MainThread]:   host: adb-1527819694938014.14.azuredatabricks.net
[0m14:09:43.854098 [info ] [MainThread]:   http_path: sql/protocolv1/o/1527819694938014/0206-120341-e6us3p8z
[0m14:09:43.854560 [info ] [MainThread]:   catalog: hive_metastore
[0m14:09:43.855011 [info ] [MainThread]:   schema: saleslt
[0m14:09:43.855627 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m14:09:43.975529 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140476951471904, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15111, 140477595164800), compute-name=) - Creating connection
[0m14:09:43.976078 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:09:43.976533 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140476951471904, session-id=None, name=debug, idle-time=6.198883056640625e-06s, acquire-count=1, language=None, thread-identifier=(15111, 140477595164800), compute-name=) - Acquired connection on thread (15111, 140477595164800), using default compute resource
[0m14:09:43.976997 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140476951471904, session-id=None, name=debug, idle-time=0.0004794597625732422s, acquire-count=1, language=None, thread-identifier=(15111, 140477595164800), compute-name=) - Checking idleness
[0m14:09:43.977465 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140476951471904, session-id=None, name=debug, idle-time=0.0009210109710693359s, acquire-count=1, language=None, thread-identifier=(15111, 140477595164800), compute-name=) - Retrieving connection
[0m14:09:43.978035 [debug] [MainThread]: Using databricks connection "debug"
[0m14:09:43.978650 [debug] [MainThread]: On debug: select 1 as id
[0m14:09:43.979108 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:09:44.763060 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140476951471904, session-id=601151b6-a7d3-4396-8a7c-6a43ea6ee882, name=debug, idle-time=4.5299530029296875e-06s, acquire-count=1, language=None, thread-identifier=(15111, 140477595164800), compute-name=) - Connection created
[0m14:09:44.763550 [debug] [MainThread]: Databricks adapter: Cursor(session-id=601151b6-a7d3-4396-8a7c-6a43ea6ee882, command-id=Unknown) - Created cursor
[0m14:09:45.087677 [debug] [MainThread]: SQL status: OK in 1.110 seconds
[0m14:09:45.088636 [debug] [MainThread]: Databricks adapter: Cursor(session-id=601151b6-a7d3-4396-8a7c-6a43ea6ee882, command-id=bd513cf1-d5ae-4711-a8d2-c963e1eb4891) - Closing cursor
[0m14:09:45.091002 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140476951471904, session-id=601151b6-a7d3-4396-8a7c-6a43ea6ee882, name=debug, idle-time=4.76837158203125e-06s, acquire-count=0, language=None, thread-identifier=(15111, 140477595164800), compute-name=) - Released connection
[0m14:09:45.091344 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:09:45.091747 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:09:45.093560 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.016631, "process_in_blocks": "249400", "process_kernel_time": 1.10953, "process_mem_max_rss": "238468", "process_out_blocks": "24", "process_user_time": 3.506909}
[0m14:09:45.094152 [debug] [MainThread]: Command `dbt debug` succeeded at 14:09:45.093996 after 4.02 seconds
[0m14:09:45.094606 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:09:45.094948 [debug] [MainThread]: On debug: Close
[0m14:09:45.095283 [debug] [MainThread]: Databricks adapter: Connection(session-id=601151b6-a7d3-4396-8a7c-6a43ea6ee882) - Closing connection
[0m14:09:45.342662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc37ac327e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc356caa720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc35d3654c0>]}
[0m14:09:45.343418 [debug] [MainThread]: Flushing usage events
[0m14:09:46.361493 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:11:46.725675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fe207b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fe60be30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96ff09a570>]}


============================== 14:11:46.732517 | ac70d603-6fa6-466e-b6b9-7a65abfa74cc ==============================
[0m14:11:46.732517 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:11:46.733235 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt clean', 'send_anonymous_usage_stats': 'True'}
[0m14:11:46.888788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac70d603-6fa6-466e-b6b9-7a65abfa74cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fe2612b0>]}
[0m14:11:46.928609 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.2659688, "process_in_blocks": "63232", "process_kernel_time": 0.233573, "process_mem_max_rss": "96216", "process_out_blocks": "8", "process_user_time": 1.569611}
[0m14:11:46.929073 [debug] [MainThread]: Command `dbt clean` succeeded at 14:11:46.928975 after 0.27 seconds
[0m14:11:46.929412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9700136b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fe151550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96feb847a0>]}
[0m14:11:46.929755 [debug] [MainThread]: Flushing usage events
[0m14:11:48.066531 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:12:30.469695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986b9fd970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986d3ea6f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986ce147a0>]}


============================== 14:12:30.472501 | d235d004-d5c8-4604-8199-38fffee7ab69 ==============================
[0m14:12:30.472501 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:12:30.473041 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt compile', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:12:31.137453 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:12:31.138052 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:12:31.138468 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:12:32.577520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986bf6b050>]}
[0m14:12:32.629671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f984caafc80>]}
[0m14:12:32.630372 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m14:12:32.728292 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m14:12:32.728862 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:12:32.729238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f984c98b8f0>]}
[0m14:12:34.365676 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models/marts/sales/sales.yml'
[0m14:12:34.520121 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models/marts/customer/dim_customer.yml'
[0m14:12:34.541070 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models/marts/product/dim_product.yml'
[0m14:12:34.687757 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderID.810c5f247c' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.688453 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderID.48ce11e7f3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.689134 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_sales_saleOrderDetailID.343b942405' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.689726 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.690256 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderQty.66af966596' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.690774 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productID.cbf6d34890' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.691269 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_unitPrice.3545b5473a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.691762 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_lineTotal.d55bca27f8' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.692252 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_name.4c7b961f77' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.692768 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_productNumber.3a23a94ddd' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.693259 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_standardCost.d3f58be9a3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.693759 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_listPrice.4ee58b9e3f' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.694256 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_sellStartDate.b44c8ea118' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.694808 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_orderDate.6f6f720ec3' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.695349 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_customerID.60b0993af5' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.695887 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_subTotal.bfeb62a487' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.696422 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_taxAmt.94cff67d6a' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.697080 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_freight.ca13e04131' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.697600 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_sales_totalDue.920571e023' (models/marts/sales/sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m14:12:34.698121 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_customers_customer_sk.22a014df62' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m14:12:34.698760 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customer_sk.8ae5836863' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m14:12:34.699277 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_customerid.209fbdda85' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m14:12:34.699901 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_customers_AddressId.86b771f63e' (models/marts/customer/dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m14:12:34.700714 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.unique_dim_products_product_sk.8f20ac7c5b' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m14:12:34.701549 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_sk.2a2df3e1b9' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m14:12:34.702571 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_product_name.991aec73f3' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m14:12:34.703672 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_spark_dbt.not_null_dim_products_sellstartdate.f97a265a0f' (models/marts/product/dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m14:12:34.837701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f984c602de0>]}
[0m14:12:34.962370 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:12:34.964992 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:12:34.983756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98471f1a60>]}
[0m14:12:34.984358 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m14:12:34.984843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f984c0f6d80>]}
[0m14:12:34.987508 [info ] [MainThread]: 
[0m14:12:34.988115 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:12:34.988550 [info ] [MainThread]: 
[0m14:12:34.989273 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140292087826240, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15285, 140292674138240), compute-name=) - Creating connection
[0m14:12:34.989724 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:12:34.990161 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140292087826240, session-id=None, name=master, idle-time=5.7220458984375e-06s, acquire-count=1, language=None, thread-identifier=(15285, 140292674138240), compute-name=) - Acquired connection on thread (15285, 140292674138240), using default compute resource
[0m14:12:34.998345 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Creating connection
[0m14:12:34.999044 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m14:12:34.999491 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=None, name=list_hive_metastore_saleslt, idle-time=4.291534423828125e-06s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource
[0m14:12:34.999973 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0004963874816894531s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:35.000393 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0009169578552246094s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Retrieving connection
[0m14:12:35.000792 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m14:12:35.001188 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m14:12:35.001569 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:12:35.933420 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_saleslt, idle-time=6.4373016357421875e-06s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Connection created
[0m14:12:35.934153 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=162ae081-db09-40ce-9c69-73100bca9fed, command-id=Unknown) - Created cursor
[0m14:12:36.262593 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m14:12:36.265262 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=162ae081-db09-40ce-9c69-73100bca9fed, command-id=a0d9ff10-b0eb-4674-af51-5feac56eaf6f) - Closing cursor
[0m14:12:36.266068 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_saleslt, idle-time=7.62939453125e-06s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.267150 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_saleslt, idle-time=0.0011010169982910156s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.268068 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m14:12:36.268831 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_snapshots, idle-time=0.0027399063110351562s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m14:12:36.269564 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_snapshots, idle-time=0.0034584999084472656s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource
[0m14:12:36.270194 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_snapshots, idle-time=0.0041675567626953125s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.270725 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_snapshots, idle-time=0.004705667495727539s, acquire-count=1, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Retrieving connection
[0m14:12:36.271263 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m14:12:36.271779 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m14:12:36.272306 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=162ae081-db09-40ce-9c69-73100bca9fed, command-id=Unknown) - Created cursor
[0m14:12:36.560085 [debug] [ThreadPool]: SQL status: OK in 0.290 seconds
[0m14:12:36.561513 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=162ae081-db09-40ce-9c69-73100bca9fed, command-id=a65d795f-b1c4-4e2d-94b1-d7e22304b5e7) - Closing cursor
[0m14:12:36.562024 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_snapshots, idle-time=4.5299530029296875e-06s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.562930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd235d004-d5c8-4604-8199-38fffee7ab69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f984c6cdcd0>]}
[0m14:12:36.563512 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140292087826240, session-id=None, name=master, idle-time=4.5299530029296875e-06s, acquire-count=0, language=None, thread-identifier=(15285, 140292674138240), compute-name=) - Released connection
[0m14:12:36.567378 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:36.568359 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=list_hive_metastore_snapshots, idle-time=0.0061948299407958984s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.569112 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now snapshot.medallion_spark_dbt.address_snapshot)
[0m14:12:36.569800 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.007657051086425781s, acquire-count=0, language=None, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m14:12:36.570364 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.008267402648925781s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m14:12:36.570796 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:36.580061 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:36.580711 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=3.814697265625e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.581302 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.581983 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:36.582488 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:36.583091 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.0017728805541992188s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.583499 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.address_snapshot, now snapshot.medallion_spark_dbt.customer_snapshot)
[0m14:12:36.584142 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.0028154850006103516s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:36.584746 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.003371000289916992s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m14:12:36.585435 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:36.589196 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:36.589836 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.590457 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.591146 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:36.591668 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:36.592416 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.0019576549530029297s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.592778 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customer_snapshot, now snapshot.medallion_spark_dbt.customeraddress_snapshot)
[0m14:12:36.593280 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.002794981002807617s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:36.593784 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.003297090530395508s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m14:12:36.594308 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:36.598950 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:36.599607 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.600235 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=3.5762786865234375e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.601001 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:36.601826 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:36.603026 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.002738475799560547s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.603513 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customeraddress_snapshot, now snapshot.medallion_spark_dbt.product_snapshot)
[0m14:12:36.603948 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.003723621368408203s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:36.604372 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.004158735275268555s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m14:12:36.604759 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:36.608402 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:36.608972 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=3.814697265625e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.609557 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.610155 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:36.610588 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:12:36.611328 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.0017900466918945312s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.611711 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.product_snapshot, now snapshot.medallion_spark_dbt.productmodel_snapshot)
[0m14:12:36.612158 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0026116371154785156s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:36.612573 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0030472278594970703s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m14:12:36.612979 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:12:36.617596 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:12:36.618215 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.618903 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=3.5762786865234375e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.619590 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:12:36.620049 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:12:36.620598 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0017712116241455078s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.621138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.productmodel_snapshot, now snapshot.medallion_spark_dbt.salesorderdetail_snapshot)
[0m14:12:36.621770 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.002930879592895508s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:12:36.622221 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.003394603729248047s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m14:12:36.622616 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:12:36.625981 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:12:36.626494 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=1.9073486328125e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.627113 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=1.9073486328125e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.627720 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:12:36.628183 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:12:36.629003 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.0019025802612304688s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.629373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderdetail_snapshot, now snapshot.medallion_spark_dbt.salesorderheader_snapshot)
[0m14:12:36.629805 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.002712249755859375s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:12:36.630237 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.0031464099884033203s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m14:12:36.630614 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:12:36.633959 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:12:36.634538 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.635251 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=3.814697265625e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.636062 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:12:36.636525 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_customer
[0m14:12:36.637276 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.0021164417266845703s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.637653 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderheader_snapshot, now model.medallion_spark_dbt.dim_customer)
[0m14:12:36.638094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_customer, idle-time=0.002931833267211914s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:12:36.638528 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_customer, idle-time=0.0033752918243408203s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m14:12:36.638959 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_customer
[0m14:12:36.642892 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_customer"
[0m14:12:36.648405 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_customer
[0m14:12:36.649002 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_customer, idle-time=4.76837158203125e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.649566 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_customer, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.650212 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_customer
[0m14:12:36.650684 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.dim_product
[0m14:12:36.651547 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_customer, idle-time=0.0019593238830566406s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.652024 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_customer, now model.medallion_spark_dbt.dim_product)
[0m14:12:36.652701 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_product, idle-time=0.002971172332763672s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_customer
[0m14:12:36.653173 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_product, idle-time=0.003595590591430664s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m14:12:36.653567 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.dim_product
[0m14:12:36.657018 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.dim_product"
[0m14:12:36.657900 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.dim_product
[0m14:12:36.658374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_product, idle-time=3.5762786865234375e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.658898 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_product, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.659479 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.dim_product
[0m14:12:36.659888 [debug] [Thread-1 (]: Began running node model.medallion_spark_dbt.sales
[0m14:12:36.660386 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.dim_product, idle-time=0.0014946460723876953s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Checking idleness
[0m14:12:36.660721 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_spark_dbt.dim_product, now model.medallion_spark_dbt.sales)
[0m14:12:36.661249 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.sales, idle-time=0.0023491382598876953s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Reusing connection previously named model.medallion_spark_dbt.dim_product
[0m14:12:36.661648 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.sales, idle-time=0.0027539730072021484s, acquire-count=1, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Acquired connection on thread (15285, 140292002911936), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m14:12:36.662004 [debug] [Thread-1 (]: Began compiling node model.medallion_spark_dbt.sales
[0m14:12:36.665589 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_spark_dbt.sales"
[0m14:12:36.666315 [debug] [Thread-1 (]: Began executing node model.medallion_spark_dbt.sales
[0m14:12:36.666744 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.sales, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.667234 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140292006417168, session-id=162ae081-db09-40ce-9c69-73100bca9fed, name=model.medallion_spark_dbt.sales, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(15285, 140292002911936), compute-name=) - Released connection
[0m14:12:36.667826 [debug] [Thread-1 (]: Finished running node model.medallion_spark_dbt.sales
[0m14:12:36.669366 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:12:36.669743 [debug] [MainThread]: Connection 'model.medallion_spark_dbt.sales' was properly closed.
[0m14:12:36.670083 [debug] [MainThread]: On model.medallion_spark_dbt.sales: Close
[0m14:12:36.670421 [debug] [MainThread]: Databricks adapter: Connection(session-id=162ae081-db09-40ce-9c69-73100bca9fed) - Closing connection
[0m14:12:36.858715 [debug] [MainThread]: Command end result
[0m14:12:36.889028 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:12:36.890465 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:12:36.896295 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m14:12:36.897224 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 6.481921, "process_in_blocks": "188816", "process_kernel_time": 1.051146, "process_mem_max_rss": "252700", "process_out_blocks": "4648", "process_user_time": 5.045505}
[0m14:12:36.897662 [debug] [MainThread]: Command `dbt compile` succeeded at 14:12:36.897560 after 6.48 seconds
[0m14:12:36.898030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986ef6f320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986c0ddd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f986c0df470>]}
[0m14:12:36.898404 [debug] [MainThread]: Flushing usage events
[0m14:12:37.882979 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:12:54.869575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc812a20da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc812c01ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc812bd6210>]}


============================== 14:12:54.872486 | 9a816e87-38b8-4d2b-acf9-6da14a8c35ab ==============================
[0m14:12:54.872486 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:12:54.873008 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/stanley_one/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/stanley_one/medallion_spark_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m14:12:55.385099 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:12:55.385671 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:12:55.386045 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:12:56.044189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7f462cc80>]}
[0m14:12:56.097551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7efe18fe0>]}
[0m14:12:56.098158 [info ] [MainThread]: Registered adapter: databricks=1.9.4
[0m14:12:56.181197 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m14:12:56.394581 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:12:56.395074 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:12:56.448620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ef154d10>]}
[0m14:12:56.541745 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:12:56.543259 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:12:56.566371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7f615e690>]}
[0m14:12:56.566949 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 605 macros
[0m14:12:56.567380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ef0fbaa0>]}
[0m14:12:56.569450 [info ] [MainThread]: 
[0m14:12:56.569870 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:12:56.570251 [info ] [MainThread]: 
[0m14:12:56.570881 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Creating connection
[0m14:12:56.571253 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:12:56.571614 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=4.76837158203125e-06s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Acquired connection on thread (15386, 140497334018176), using default compute resource
[0m14:12:56.577874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Creating connection
[0m14:12:56.578543 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m14:12:56.579206 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=None, name=list_hive_metastore, idle-time=5.245208740234375e-06s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource
[0m14:12:56.579592 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=None, name=list_hive_metastore, idle-time=0.0004143714904785156s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:56.579907 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=None, name=list_hive_metastore, idle-time=0.0007321834564208984s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:56.580189 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:12:56.580551 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m14:12:56.581000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:12:57.351036 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore, idle-time=1.4543533325195312e-05s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Connection created
[0m14:12:57.352992 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:57.599836 [debug] [ThreadPool]: SQL status: OK in 1.020 seconds
[0m14:12:57.603267 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=c80365c4-af70-44aa-9c51-52a3c83a10a1) - Closing cursor
[0m14:12:57.604383 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore, idle-time=6.198883056640625e-06s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:57.608184 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore, idle-time=0.0038824081420898438s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:57.609131 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_snapshots)
[0m14:12:57.609827 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_snapshots, idle-time=0.005569934844970703s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named list_hive_metastore
[0m14:12:57.610446 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_snapshots, idle-time=0.006193399429321289s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource
[0m14:12:57.611136 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_snapshots, idle-time=0.006891489028930664s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:57.611771 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_snapshots, idle-time=0.00752568244934082s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:57.612297 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m14:12:57.612911 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m14:12:57.613443 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:57.887719 [debug] [ThreadPool]: SQL status: OK in 0.270 seconds
[0m14:12:57.889297 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=9adb2958-fa75-407a-a3d1-6d2c29b77cb7) - Closing cursor
[0m14:12:57.889849 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_snapshots, idle-time=3.5762786865234375e-06s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:57.890753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_snapshots, idle-time=0.0008897781372070312s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:57.891387 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m14:12:57.891866 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_saleslt, idle-time=0.0020084381103515625s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named list_hive_metastore_snapshots
[0m14:12:57.892419 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_saleslt, idle-time=0.0025527477264404297s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource
[0m14:12:57.892924 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_saleslt, idle-time=0.0030820369720458984s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:57.893364 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_saleslt, idle-time=0.003544330596923828s, acquire-count=1, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:57.893699 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m14:12:57.894042 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m14:12:57.894458 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:58.175307 [debug] [ThreadPool]: SQL status: OK in 0.280 seconds
[0m14:12:58.176630 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=6297dab9-63f0-4fed-854b-a611f1a5df81) - Closing cursor
[0m14:12:58.177066 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_saleslt, idle-time=2.6226043701171875e-06s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:58.177891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc813a060f0>]}
[0m14:12:58.178450 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=1.6068432331085205s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Checking idleness
[0m14:12:58.178828 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=1.6072266101837158s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Retrieving connection
[0m14:12:58.179219 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=1.6076035499572754s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Checking idleness
[0m14:12:58.179589 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=1.607987880706787s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Retrieving connection
[0m14:12:58.179953 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:12:58.180369 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:12:58.180740 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=2.86102294921875e-06s, acquire-count=0, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Released connection
[0m14:12:58.183148 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:58.183771 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m14:12:58.184470 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=list_hive_metastore_saleslt, idle-time=0.007355451583862305s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:58.184886 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now snapshot.medallion_spark_dbt.address_snapshot)
[0m14:12:58.185333 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.008212804794311523s, acquire-count=0, language=None, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m14:12:58.185801 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.00867009162902832s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m14:12:58.186215 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:58.196586 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:58.302353 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.12520790100097656s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:58.302965 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.12578821182250977s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:58.303280 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.address_snapshot"
[0m14:12:58.303713 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:12:58.304151 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:58.886597 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:12:58.887732 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=50d522ad-1f36-4ccf-9520-31573b302989
[0m14:12:58.888936 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:58.913866 [debug] [Thread-1 (]: Database Error in snapshot address_snapshot (snapshots/address.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:12:58.914504 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:58.916565 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eef46ba0>]}
[0m14:12:58.917495 [error] [Thread-1 (]: 1 of 7 ERROR snapshotting snapshots.address_snapshot ........................... [[31mERROR[0m in 0.73s]
[0m14:12:58.918318 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:58.918842 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:58.919468 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.address_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot address_snapshot (snapshots/address.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9.
[0m14:12:58.920124 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m14:12:58.921855 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.address_snapshot, idle-time=0.007326841354370117s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:58.922249 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.address_snapshot, now snapshot.medallion_spark_dbt.customer_snapshot)
[0m14:12:58.922702 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.00821828842163086s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.address_snapshot
[0m14:12:58.923153 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.008666276931762695s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m14:12:58.923586 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:58.927825 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:58.932567 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.0180509090423584s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:58.933049 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.018582820892333984s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:58.933373 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customer_snapshot"
[0m14:12:58.933851 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:12:58.934338 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:59.370403 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:12:59.371171 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=c422b4cc-d9f6-474e-b730-1fd9bac20b6f
[0m14:12:59.372032 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:59.375608 [debug] [Thread-1 (]: Database Error in snapshot customer_snapshot (snapshots/customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
[0m14:12:59.376063 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:59.376528 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ece1ba40>]}
[0m14:12:59.377300 [error] [Thread-1 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 0.45s]
[0m14:12:59.377928 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:59.378374 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:59.378955 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.customer_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customer_snapshot (snapshots/customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9.
[0m14:12:59.379490 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m14:12:59.380382 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customer_snapshot, idle-time=0.004315614700317383s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:59.380724 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customer_snapshot, now snapshot.medallion_spark_dbt.customeraddress_snapshot)
[0m14:12:59.381125 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.00506591796875s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customer_snapshot
[0m14:12:59.381541 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.005480527877807617s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m14:12:59.381933 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:59.386096 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:59.390903 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.014802694320678711s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:59.391384 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.015336751937866211s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:59.391665 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.customeraddress_snapshot"
[0m14:12:59.392051 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:12:59.392449 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:59.696801 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:12:59.697559 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=1b0d3a6b-f6a6-4f00-bfdd-e8a7dbb5c992
[0m14:12:59.698359 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:59.701693 [debug] [Thread-1 (]: Database Error in snapshot customeraddress_snapshot (snapshots/customeraddress.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:12:59.702205 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=2.384185791015625e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:12:59.702699 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ece46450>]}
[0m14:12:59.703520 [error] [Thread-1 (]: 3 of 7 ERROR snapshotting snapshots.customeraddress_snapshot ................... [[31mERROR[0m in 0.32s]
[0m14:12:59.704141 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:59.704578 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:59.705098 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.customeraddress_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customeraddress_snapshot (snapshots/customeraddress.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9.
[0m14:12:59.705629 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m14:12:59.706490 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.customeraddress_snapshot, idle-time=0.0042994022369384766s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:59.706818 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.customeraddress_snapshot, now snapshot.medallion_spark_dbt.product_snapshot)
[0m14:12:59.707226 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.005030632019042969s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.customeraddress_snapshot
[0m14:12:59.707609 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.005432605743408203s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m14:12:59.708000 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:59.711009 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.product_snapshot
[0m14:12:59.720284 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.0179290771484375s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:12:59.720838 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.018637895584106445s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:12:59.721191 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.product_snapshot"
[0m14:12:59.721586 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:12:59.721978 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:12:59.997593 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:12:59.998499 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=33d94a56-540d-4e0e-9f25-352f50e8dc8c
[0m14:12:59.999469 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=3.337860107421875e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.003504 [debug] [Thread-1 (]: Database Error in snapshot product_snapshot (snapshots/product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
[0m14:13:00.004177 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.004720 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ece43050>]}
[0m14:13:00.005588 [error] [Thread-1 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.30s]
[0m14:13:00.006349 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.product_snapshot
[0m14:13:00.006872 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:13:00.007485 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.product_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot product_snapshot (snapshots/product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9.
[0m14:13:00.008122 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m14:13:00.009131 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.product_snapshot, idle-time=0.004975318908691406s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:13:00.009540 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.product_snapshot, now snapshot.medallion_spark_dbt.productmodel_snapshot)
[0m14:13:00.010036 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0058956146240234375s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.product_snapshot
[0m14:13:00.010494 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.006357431411743164s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m14:13:00.010954 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:13:00.015478 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:13:00.021082 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.016901493072509766s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:13:00.021611 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.017502784729003906s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:13:00.021905 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.productmodel_snapshot"
[0m14:13:00.022294 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:13:00.022676 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:13:00.309725 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:13:00.310457 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=7cfa93f3-ddf9-4826-b58b-9d4188d95837
[0m14:13:00.311242 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.314810 [debug] [Thread-1 (]: Database Error in snapshot productmodel_snapshot (snapshots/productmodel.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:13:00.315365 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.315963 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eef8cce0>]}
[0m14:13:00.316819 [error] [Thread-1 (]: 5 of 7 ERROR snapshotting snapshots.productmodel_snapshot ...................... [[31mERROR[0m in 0.31s]
[0m14:13:00.317500 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:13:00.317955 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:13:00.318494 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m14:13:00.319131 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.productmodel_snapshot, idle-time=0.0037772655487060547s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:13:00.319522 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.productmodel_snapshot, now snapshot.medallion_spark_dbt.salesorderdetail_snapshot)
[0m14:13:00.320053 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.productmodel_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot productmodel_snapshot (snapshots/productmodel.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9.
[0m14:13:00.320610 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.00524592399597168s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.productmodel_snapshot
[0m14:13:00.321314 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.005952358245849609s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m14:13:00.321720 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:13:00.325091 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:13:00.329795 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.014396190643310547s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:13:00.330297 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.014962911605834961s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:13:00.330595 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"
[0m14:13:00.330977 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:13:00.331378 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:13:00.652771 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:13:00.654954 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=367ebfb6-a04c-4195-8b98-a8e90489e370
[0m14:13:00.657406 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=6.9141387939453125e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.666252 [debug] [Thread-1 (]: Database Error in snapshot salesorderdetail_snapshot (snapshots/salesorderdetail.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:13:00.667249 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=4.76837158203125e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.668023 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ece57770>]}
[0m14:13:00.669502 [error] [Thread-1 (]: 6 of 7 ERROR snapshotting snapshots.salesorderdetail_snapshot .................. [[31mERROR[0m in 0.35s]
[0m14:13:00.671000 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:13:00.671821 [debug] [Thread-1 (]: Began running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:13:00.672832 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.salesorderdetail_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderdetail_snapshot (snapshots/salesorderdetail.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9.
[0m14:13:00.673852 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m14:13:00.675508 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderdetail_snapshot, idle-time=0.008262872695922852s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:13:00.676052 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_spark_dbt.salesorderdetail_snapshot, now snapshot.medallion_spark_dbt.salesorderheader_snapshot)
[0m14:13:00.676681 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.009480953216552734s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Reusing connection previously named snapshot.medallion_spark_dbt.salesorderdetail_snapshot
[0m14:13:00.677489 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.010212182998657227s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Acquired connection on thread (15386, 140496683718336), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m14:13:00.678223 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:13:00.682764 [debug] [Thread-1 (]: Began executing node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:13:00.688844 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.02163410186767578s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Checking idleness
[0m14:13:00.689684 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=0.022408246994018555s, acquire-count=1, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Retrieving connection
[0m14:13:00.690195 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_spark_dbt.salesorderheader_snapshot"
[0m14:13:00.690735 [debug] [Thread-1 (]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m14:13:00.691272 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Created cursor
[0m14:13:00.985479 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, command-id=Unknown) - Closing cursor
[0m14:13:00.986575 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.2", "dbt_databricks_version": "1.9.4", "databricks_sql_connector_version": "3.6.0", "profile_name": "medallion_spark_dbt", "target_name": "dev", "node_id": "snapshot.medallion_spark_dbt.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:805)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:641)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:711)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:486)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:88)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:191)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:449)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:499)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:775)
	... 42 more
, operation-id=e1c05b1f-90b4-4233-89f3-80e2081ceb8c
[0m14:13:00.987495 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.990577 [debug] [Thread-1 (]: Database Error in snapshot salesorderheader_snapshot (snapshots/salesorderheader.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
[0m14:13:00.991097 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(id=140496685737376, session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316, name=snapshot.medallion_spark_dbt.salesorderheader_snapshot, idle-time=2.6226043701171875e-06s, acquire-count=0, language=sql, thread-identifier=(15386, 140496683718336), compute-name=) - Released connection
[0m14:13:00.991510 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a816e87-38b8-4d2b-acf9-6da14a8c35ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ece7f920>]}
[0m14:13:00.992268 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.32s]
[0m14:13:00.992825 [debug] [Thread-1 (]: Finished running node snapshot.medallion_spark_dbt.salesorderheader_snapshot
[0m14:13:00.993367 [debug] [Thread-4 (]: Marking all children of 'snapshot.medallion_spark_dbt.salesorderheader_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderheader_snapshot (snapshots/salesorderheader.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9.
[0m14:13:00.994878 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=2.814087152481079s, acquire-count=0, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Checking idleness
[0m14:13:00.995448 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=2.814674139022827s, acquire-count=0, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Reusing connection previously named master
[0m14:13:00.995922 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=2.81516695022583s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Acquired connection on thread (15386, 140497334018176), using default compute resource
[0m14:13:00.996460 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=2.8156955242156982s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Checking idleness
[0m14:13:00.996951 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=None, name=master, idle-time=2.8161838054656982s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Retrieving connection
[0m14:13:00.997401 [debug] [MainThread]: On master: ROLLBACK
[0m14:13:00.997819 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:13:02.178252 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=d1dccd35-2044-4f30-a5da-570e84e64be2, name=master, idle-time=5.7220458984375e-06s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Connection created
[0m14:13:02.178920 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:13:02.179592 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=d1dccd35-2044-4f30-a5da-570e84e64be2, name=master, idle-time=0.0013649463653564453s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Checking idleness
[0m14:13:02.180154 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=d1dccd35-2044-4f30-a5da-570e84e64be2, name=master, idle-time=0.0019783973693847656s, acquire-count=1, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Retrieving connection
[0m14:13:02.180600 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:13:02.180988 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:13:02.181761 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=140496688869024, session-id=d1dccd35-2044-4f30-a5da-570e84e64be2, name=master, idle-time=3.0994415283203125e-06s, acquire-count=0, language=None, thread-identifier=(15386, 140497334018176), compute-name=) - Released connection
[0m14:13:02.182433 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:13:02.182859 [debug] [MainThread]: On master: ROLLBACK
[0m14:13:02.183434 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:13:02.183985 [debug] [MainThread]: On master: Close
[0m14:13:02.184776 [debug] [MainThread]: Databricks adapter: Connection(session-id=d1dccd35-2044-4f30-a5da-570e84e64be2) - Closing connection
[0m14:13:02.485017 [debug] [MainThread]: Connection 'snapshot.medallion_spark_dbt.salesorderheader_snapshot' was properly closed.
[0m14:13:02.487132 [debug] [MainThread]: On snapshot.medallion_spark_dbt.salesorderheader_snapshot: Close
[0m14:13:02.489182 [debug] [MainThread]: Databricks adapter: Connection(session-id=16f9b54f-61fd-4ab2-8292-d49fc4c60316) - Closing connection
[0m14:13:02.691290 [info ] [MainThread]: 
[0m14:13:02.692092 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 6.12 seconds (6.12s).
[0m14:13:02.694475 [debug] [MainThread]: Command end result
[0m14:13:02.741946 [debug] [MainThread]: Wrote artifact WritableManifest to /home/stanley_one/medallion_spark_dbt/target/manifest.json
[0m14:13:02.743699 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/stanley_one/medallion_spark_dbt/target/semantic_manifest.json
[0m14:13:02.749178 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/stanley_one/medallion_spark_dbt/target/run_results.json
[0m14:13:02.749588 [info ] [MainThread]: 
[0m14:13:02.750011 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m14:13:02.750650 [info ] [MainThread]: 
[0m14:13:02.751409 [error] [MainThread]:   Database Error in snapshot address_snapshot (snapshots/address.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`address` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:13:02.752135 [info ] [MainThread]: 
[0m14:13:02.752577 [error] [MainThread]:   Database Error in snapshot customer_snapshot (snapshots/customer.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customer` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 46 pos 9
[0m14:13:02.752960 [info ] [MainThread]: 
[0m14:13:02.753538 [error] [MainThread]:   Database Error in snapshot customeraddress_snapshot (snapshots/customeraddress.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`customeraddress` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:13:02.754060 [info ] [MainThread]: 
[0m14:13:02.754845 [error] [MainThread]:   Database Error in snapshot product_snapshot (snapshots/product.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 48 pos 9
[0m14:13:02.755502 [info ] [MainThread]: 
[0m14:13:02.756017 [error] [MainThread]:   Database Error in snapshot productmodel_snapshot (snapshots/productmodel.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`productmodel` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 36 pos 9
[0m14:13:02.756498 [info ] [MainThread]: 
[0m14:13:02.757123 [error] [MainThread]:   Database Error in snapshot salesorderdetail_snapshot (snapshots/salesorderdetail.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderdetail` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 40 pos 9
[0m14:13:02.757762 [info ] [MainThread]: 
[0m14:13:02.758609 [error] [MainThread]:   Database Error in snapshot salesorderheader_snapshot (snapshots/salesorderheader.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`salesorderheader` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 53 pos 9
[0m14:13:02.759617 [info ] [MainThread]: 
[0m14:13:02.760728 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=7 SKIP=0 TOTAL=7
[0m14:13:02.761996 [debug] [MainThread]: Resource report: {"command_name": "snapshot", "command_success": false, "command_wall_clock_time": 7.950961, "process_in_blocks": "752", "process_kernel_time": 1.036584, "process_mem_max_rss": "247828", "process_out_blocks": "3320", "process_user_time": 3.378868}
[0m14:13:02.762579 [debug] [MainThread]: Command `dbt snapshot` failed at 14:13:02.762434 after 7.95 seconds
[0m14:13:02.763083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8125727e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8123fe570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eeede780>]}
[0m14:13:02.763551 [debug] [MainThread]: Flushing usage events
[0m14:13:04.056630 [debug] [MainThread]: An error was encountered while trying to flush usage events
